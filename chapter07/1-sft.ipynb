{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:00.278612Z",
     "start_time": "2025-10-10T01:06:51.952024Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "device = \"mps\"\n",
    "model_path = \"/Users/zhangyf/llm/Qwen2-0.5B\"\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:02.986199Z",
     "start_time": "2025-10-10T01:07:02.983161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model.generation_config)\n",
    "\n",
    "model.generation_config.do_sample = True\n",
    "model.generation_config.eos_token_id = [151645, 151643]\n",
    "model.generation_config.pad_token_id = 151643\n",
    "model.generation_config.temperature = 0.7\n",
    "model.generation_config.top_p = 0.8\n",
    "model.generation_config.top_k = 20\n",
    "model.generation_config.repetition_penalty = 1.05\n",
    "\n",
    "print(model.generation_config)"
   ],
   "id": "b0187f5e9ac39c3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:05.647486Z",
     "start_time": "2025-10-10T01:07:05.645453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer.decode(151643))\n",
    "print(tokenizer.decode(151645))"
   ],
   "id": "11b1f6d62a0ba993",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:07.561720Z",
     "start_time": "2025-10-10T01:07:07.559382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class SFTConfig:\n",
    "    max_length:int = 2500\n",
    "    batch_size:int = 2\n",
    "    gradient_accumulation_steps:int = 8\n",
    "    log_iter:int = 400\n",
    "    max_lr:float = 2e-5\n",
    "    min_lr:float = 2e-6\n",
    "    warmup_steps:int = 1000"
   ],
   "id": "48355c9f48b4616d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:10.815314Z",
     "start_time": "2025-10-10T01:07:09.558298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "ultrachat_200k_data = datasets.load_dataset('./ultrachat_200k')"
   ],
   "id": "f116655db57617ef",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:13.039011Z",
     "start_time": "2025-10-10T01:07:13.034111Z"
    }
   },
   "cell_type": "code",
   "source": "ultrachat_200k_data['train_sft'][110]['messages']",
   "id": "ce6a2e4844fddbb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'As Iâ€™ve said again and again â€“ Science is everywhere! Itâ€™s in the air we breathe, the body we move around in, the nature around us, and the food we eat. Itâ€™s even in the sweet treats we enjoy. Yes, Peeps science is a thing. One of our kidsâ€™ (and maybe my) favorite things to do with Peeps candy at Easter is put them in the microwave and watch them expand. If you remove them from the microwave before they explode or catch fire, (note: Remove the Peeps before this happens. This can ruin your microwave.) the puffy Peep collapses into a flattened mass of goo. If you plan on eating it, do it fast before it cools and hardens. So what makes the Peep puff up in the microwave and then collapse as it cools? Peeps are made mostly of sugar, water, and gelatin. This mixture is heated and whipped to add air bubbles. This creates the light, fluffy texture of Peeps. As this mixture cools, the gelatin and air bubbles helps the Peeps hold their shape and not flatten. When we heat a Peep in the microwave, the air molecules in the bubbles inside the Peep start moving faster. As the molecules start moving faster, the pressure inside the bubbles begins to increase and push on the walls of the bubbles. The heat of the microwave is, also, starting to cause the sugar in the Peep to soften. So, when as the air molecules push on the sides of the bubbles inside the peep, the bubbles to expand and grow. This is what causes Peeps to get bigger when heated in the microwave. My kids are always disappointed when we take the puffed up Peep from the microwave only to see it instantly deflate. Once the heating stops, the air molecules slow down and stop pushing against the wall of the bubbles and some bubbles burst letting the air out of the Peep. This, plus the fact that some of the water in the Peep evaporates during heating lead to the collapse of the Peep. One day after looking at our deflated Peeps, we talked about the changes that took place in the Peeps do to heating. We thought about the air escaping and the water evaporating and wondered if this caused a change in mass, or weight. Well, there is only one way to find out. Design an experiment! Weigh your glass bowl. If your scale has a tare feature, use it now. If not, record the weight of the bowl. Next, add a Peep to the bowl and record the weight of the Peep. If you do not have a tare feature on your scale, you will have to subtract the weight of the bowl from the weight of the bowl and the Peep to find the weight of the Peep alone. Place the Peep and the bowl in the microwave. Heat for 30 seconds. Immediately after the microwave shuts off, take the bowl and Peep out of the microwave and weigh it. Record the weight of the Peep. Again, subtract the weight of the bowl if you did not use the tare feature. Continue this procedure with new Peeps each time increasing the time by 10 seconds until you have reached 60 seconds. Each time you repeat the experiment, it is called a trial. Record the before and after weights, along with the time for each trial. When all the trials are completed. Subtract the ending weights of each trial from the beginning weights. This is the mass or weight lost in each trial. Graph the amounts of weight lost against the time each Peep was heated. What does the graph look like? Discuss and explain your results. We always have way more peeps than we need to eat so this experiment will be a fun way to use up the extra peeps! Is it possible to do this in an oven? We donâ€™t have a microwave. That would be a great experiment â€“ just to see if this works! Try it out and let me know how it goes. Can you explain why Peeps expand in the microwave and then collapse as they cool?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Yes, Peeps expand in the microwave as the air molecules in the bubbles inside the Peep start moving faster, causing the pressure inside the bubbles to increase and push on the walls of the bubbles. This, in turn, causes the bubbles to expand and grow. However, when the heating stops, the air molecules slow down, stop pushing against the walls of the bubbles, and some bubbles burst, letting the air out of the Peep. Additionally, some of the water in the Peep evaporates during heating, leading to the collapse of the Peep.',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Can you suggest any other fun experiment that we can do with Peeps candy?',\n",
       "  'role': 'user'},\n",
       " {'content': \"Sure, here's another fun experiment you can do with Peeps candy:\\n\\nPeeps Density Experiment\\nMaterials needed:\\n- Three clear glasses or jars\\n- Water\\n- Salt\\n- Food coloring (optional)\\n- Peeps candy\\n\\nInstructions:\\n1. Fill the first glass with plain water.\\n2. Fill the second glass with water and add a few drops of food coloring (optional).\\n3. Fill the third glass with water and add salt until it is fully dissolved.\\n4. Drop a Peep into each glass and observe what happens.\\n5. Record your observations and note which Peep floats the highest and which sinks the furthest.\\n\\nExplanation:\\nThe Peep in the first glass will float because it is less dense than water. The Peep in the second glass may also float, depending on the amount of food coloring used, as it may increase or decrease the density of the water. The Peep in the third glass will sink because the salt has increased the density of the water, making it more dense than the Peep. The differences in density make for an interesting observation and discussion on buoyancy and density.\",\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Can you provide me with the recipe for making Peeps candy at home?',\n",
       "  'role': 'user'},\n",
       " {'content': \"Yes, here's a recipe for making Peeps candy at home:\\n\\nHomemade Peeps Recipe\\n\\nIngredients:\\n- 3 packages unflavored gelatin\\n- 1 cup water\\n- 1 1/2 cups granulated sugar\\n- 1 cup light corn syrup\\n- 1/4 teaspoon salt\\n- 1 teaspoon pure vanilla extract\\n- Food coloring\\n- Powdered sugar\\n\\nInstructions:\\n1. In a small saucepan, combine the gelatin and 1/2 cup of water. Let it stand for 5 minutes to soften.\\n2. In a large saucepan, combine the sugar, corn syrup, salt, and the remaining 1/2 cup of water. Cook over medium-high heat, stirring until the sugar dissolves.\\n3. Increase the heat to high and bring the mixture to a boil. Allow it to boil, without stirring, until the mixture reaches 240Â°F on a candy thermometer.\\n4. Remove from heat and immediately stir in the softened gelatin mixture until it dissolves.\\n5. Add the vanilla extract and stir well.\\n6. Divide the mixture into separate bowls for each color desired. Using a toothpick, add the desired color of food coloring and stir well.\\n7. Allow the mixture to cool to room temperature, or until it no longer feels warm to the touch.\\n8. Scoop the mixture into a piping bag fitted with a round tip. Lightly dust a baking sheet with powdered sugar.\\n9. Pipe the mixture into chick or bunny shapes on the prepared baking sheet.\\n10. Allow the Peeps to dry at room temperature for at least 6 hours or overnight.\\n11. Once dry, dust with additional powdered sugar to coat.\\n\\nEnjoy your homemade Peeps candy!\",\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Can you explain the science behind why Peeps float in water?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Sure, Peeps float in water because their density is less than that of water. Density is a measure of how much mass is contained in a given volume. When something is less dense than water, it will float on top of the water.\\n\\nPeeps are mostly made up of sugar, which is less dense than water. As a result, when a Peep is placed in water, it displaces an amount of water that weighs more than the Peep itself. This displaced water pushes up on the Peep, causing it to float on the surface of the water.\\n\\nThis is also why objects with a higher density than water, such as a rock, sink to the bottom of the body of water. The displaced water weighs less than the object being submerged, causing the object to sink.\\n\\nSo, in summary, Peeps float in water because they are less dense than water, causing the displaced water to push them up and keep them afloat.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-10T01:07:15.053736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_format(data):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        data,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = False,\n",
    "        truncation = True,\n",
    "        max_length = 2500,\n",
    "    )\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "## ç”Ÿæˆè®­ç»ƒæ•°æ®çš„tokenid\n",
    "chosen_input_ids_list = []\n",
    "i = 0\n",
    "while True:\n",
    "    data = ultrachat_200k_data['train_sft'][i]['messages']\n",
    "    # æ·»åŠ  **ç³»ç»Ÿæç¤ºè¯**\n",
    "    data.insert(\n",
    "        0,\n",
    "        {\"content\": \"You are a helpful assistant\", \"role\": \"system\"}\n",
    "    )\n",
    "    input_ids = tokenize_and_format(data)\n",
    "    chosen_input_ids_list.append(input_ids)\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"å·²å¤„ç†{i}æ¡æ•°æ®\")\n",
    "    if i == 50000:#len(ultrachat_200k_data['train_sft']):\n",
    "        break\n",
    "print('-' * 70)"
   ],
   "id": "2e2fc1023fbb350b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†1000æ¡æ•°æ®\n",
      "å·²å¤„ç†2000æ¡æ•°æ®\n",
      "å·²å¤„ç†3000æ¡æ•°æ®\n",
      "å·²å¤„ç†4000æ¡æ•°æ®\n",
      "å·²å¤„ç†5000æ¡æ•°æ®\n",
      "å·²å¤„ç†6000æ¡æ•°æ®\n",
      "å·²å¤„ç†7000æ¡æ•°æ®\n",
      "å·²å¤„ç†8000æ¡æ•°æ®\n",
      "å·²å¤„ç†9000æ¡æ•°æ®\n",
      "å·²å¤„ç†10000æ¡æ•°æ®\n",
      "å·²å¤„ç†11000æ¡æ•°æ®\n",
      "å·²å¤„ç†12000æ¡æ•°æ®\n",
      "å·²å¤„ç†13000æ¡æ•°æ®\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:43.770005Z",
     "start_time": "2025-10-09T09:56:43.765469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "pprint(tokenizer.decode(chosen_input_ids_list[110]))"
   ],
   "id": "62a6e10466dec9c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|im_start|>system\\n'\n",
      " 'You are a helpful assistant<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'As Iâ€™ve said again and again â€“ Science is everywhere! Itâ€™s in the air we '\n",
      " 'breathe, the body we move around in, the nature around us, and the food we '\n",
      " 'eat. Itâ€™s even in the sweet treats we enjoy. Yes, Peeps science is a thing. '\n",
      " 'One of our kidsâ€™ (and maybe my) favorite things to do with Peeps candy at '\n",
      " 'Easter is put them in the microwave and watch them expand. If you remove '\n",
      " 'them from the microwave before they explode or catch fire, (note: Remove the '\n",
      " 'Peeps before this happens. This can ruin your microwave.) the puffy Peep '\n",
      " 'collapses into a flattened mass of goo. If you plan on eating it, do it fast '\n",
      " 'before it cools and hardens. So what makes the Peep puff up in the microwave '\n",
      " 'and then collapse as it cools? Peeps are made mostly of sugar, water, and '\n",
      " 'gelatin. This mixture is heated and whipped to add air bubbles. This creates '\n",
      " 'the light, fluffy texture of Peeps. As this mixture cools, the gelatin and '\n",
      " 'air bubbles helps the Peeps hold their shape and not flatten. When we heat a '\n",
      " 'Peep in the microwave, the air molecules in the bubbles inside the Peep '\n",
      " 'start moving faster. As the molecules start moving faster, the pressure '\n",
      " 'inside the bubbles begins to increase and push on the walls of the bubbles. '\n",
      " 'The heat of the microwave is, also, starting to cause the sugar in the Peep '\n",
      " 'to soften. So, when as the air molecules push on the sides of the bubbles '\n",
      " 'inside the peep, the bubbles to expand and grow. This is what causes Peeps '\n",
      " 'to get bigger when heated in the microwave. My kids are always disappointed '\n",
      " 'when we take the puffed up Peep from the microwave only to see it instantly '\n",
      " 'deflate. Once the heating stops, the air molecules slow down and stop '\n",
      " 'pushing against the wall of the bubbles and some bubbles burst letting the '\n",
      " 'air out of the Peep. This, plus the fact that some of the water in the Peep '\n",
      " 'evaporates during heating lead to the collapse of the Peep. One day after '\n",
      " 'looking at our deflated Peeps, we talked about the changes that took place '\n",
      " 'in the Peeps do to heating. We thought about the air escaping and the water '\n",
      " 'evaporating and wondered if this caused a change in mass, or weight. Well, '\n",
      " 'there is only one way to find out. Design an experiment! Weigh your glass '\n",
      " 'bowl. If your scale has a tare feature, use it now. If not, record the '\n",
      " 'weight of the bowl. Next, add a Peep to the bowl and record the weight of '\n",
      " 'the Peep. If you do not have a tare feature on your scale, you will have to '\n",
      " 'subtract the weight of the bowl from the weight of the bowl and the Peep to '\n",
      " 'find the weight of the Peep alone. Place the Peep and the bowl in the '\n",
      " 'microwave. Heat for 30 seconds. Immediately after the microwave shuts off, '\n",
      " 'take the bowl and Peep out of the microwave and weigh it. Record the weight '\n",
      " 'of the Peep. Again, subtract the weight of the bowl if you did not use the '\n",
      " 'tare feature. Continue this procedure with new Peeps each time increasing '\n",
      " 'the time by 10 seconds until you have reached 60 seconds. Each time you '\n",
      " 'repeat the experiment, it is called a trial. Record the before and after '\n",
      " 'weights, along with the time for each trial. When all the trials are '\n",
      " 'completed. Subtract the ending weights of each trial from the beginning '\n",
      " 'weights. This is the mass or weight lost in each trial. Graph the amounts of '\n",
      " 'weight lost against the time each Peep was heated. What does the graph look '\n",
      " 'like? Discuss and explain your results. We always have way more peeps than '\n",
      " 'we need to eat so this experiment will be a fun way to use up the extra '\n",
      " 'peeps! Is it possible to do this in an oven? We donâ€™t have a microwave. That '\n",
      " 'would be a great experiment â€“ just to see if this works! Try it out and let '\n",
      " 'me know how it goes. Can you explain why Peeps expand in the microwave and '\n",
      " 'then collapse as they cool?<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'Yes, Peeps expand in the microwave as the air molecules in the bubbles '\n",
      " 'inside the Peep start moving faster, causing the pressure inside the bubbles '\n",
      " 'to increase and push on the walls of the bubbles. This, in turn, causes the '\n",
      " 'bubbles to expand and grow. However, when the heating stops, the air '\n",
      " 'molecules slow down, stop pushing against the walls of the bubbles, and some '\n",
      " 'bubbles burst, letting the air out of the Peep. Additionally, some of the '\n",
      " 'water in the Peep evaporates during heating, leading to the collapse of the '\n",
      " 'Peep.<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'Can you suggest any other fun experiment that we can do with Peeps '\n",
      " 'candy?<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " \"Sure, here's another fun experiment you can do with Peeps candy:\\n\"\n",
      " '\\n'\n",
      " 'Peeps Density Experiment\\n'\n",
      " 'Materials needed:\\n'\n",
      " '- Three clear glasses or jars\\n'\n",
      " '- Water\\n'\n",
      " '- Salt\\n'\n",
      " '- Food coloring (optional)\\n'\n",
      " '- Peeps candy\\n'\n",
      " '\\n'\n",
      " 'Instructions:\\n'\n",
      " '1. Fill the first glass with plain water.\\n'\n",
      " '2. Fill the second glass with water and add a few drops of food coloring '\n",
      " '(optional).\\n'\n",
      " '3. Fill the third glass with water and add salt until it is fully '\n",
      " 'dissolved.\\n'\n",
      " '4. Drop a Peep into each glass and observe what happens.\\n'\n",
      " '5. Record your observations and note which Peep floats the highest and which '\n",
      " 'sinks the furthest.\\n'\n",
      " '\\n'\n",
      " 'Explanation:\\n'\n",
      " 'The Peep in the first glass will float because it is less dense than water. '\n",
      " 'The Peep in the second glass may also float, depending on the amount of food '\n",
      " 'coloring used, as it may increase or decrease the density of the water. The '\n",
      " 'Peep in the third glass will sink because the salt has increased the density '\n",
      " 'of the water, making it more dense than the Peep. The differences in density '\n",
      " 'make for an interesting observation and discussion on buoyancy and '\n",
      " 'density.<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'Can you provide me with the recipe for making Peeps candy at '\n",
      " 'home?<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " \"Yes, here's a recipe for making Peeps candy at home:\\n\"\n",
      " '\\n'\n",
      " 'Homemade Peeps Recipe\\n'\n",
      " '\\n'\n",
      " 'Ingredients:\\n'\n",
      " '- 3 packages unflavored gelatin\\n'\n",
      " '- 1 cup water\\n'\n",
      " '- 1 1/2 cups granulated sugar\\n'\n",
      " '- 1 cup light corn syrup\\n'\n",
      " '- 1/4 teaspoon salt\\n'\n",
      " '- 1 teaspoon pure vanilla extract\\n'\n",
      " '- Food coloring\\n'\n",
      " '- Powdered sugar\\n'\n",
      " '\\n'\n",
      " 'Instructions:\\n'\n",
      " '1. In a small saucepan, combine the gelatin and 1/2 cup of water. Let it '\n",
      " 'stand for 5 minutes to soften.\\n'\n",
      " '2. In a large saucepan, combine the sugar, corn syrup, salt, and the '\n",
      " 'remaining 1/2 cup of water. Cook over medium-high heat, stirring until the '\n",
      " 'sugar dissolves.\\n'\n",
      " '3. Increase the heat to high and bring the mixture to a boil. Allow it to '\n",
      " 'boil, without stirring, until the mixture reaches 240Â°F on a candy '\n",
      " 'thermometer.\\n'\n",
      " '4. Remove from heat and immediately stir in the softened gelatin mixture '\n",
      " 'until it dissolves.\\n'\n",
      " '5. Add the vanilla extract and stir well.\\n'\n",
      " '6. Divide the mixture into separate bowls for each color desired. Using a '\n",
      " 'toothpick, add the desired color of food coloring and stir well.\\n'\n",
      " '7. Allow the mixture to cool to room temperature, or until it no longer '\n",
      " 'feels warm to the touch.\\n'\n",
      " '8. Scoop the mixture into a piping bag fitted with a round tip. Lightly dust '\n",
      " 'a baking sheet with powdered sugar.\\n'\n",
      " '9. Pipe the mixture into chick or bunny shapes on the prepared baking '\n",
      " 'sheet.\\n'\n",
      " '10. Allow the Peeps to dry at room temperature for at least 6 hours or '\n",
      " 'overnight.\\n'\n",
      " '11. Once dry, dust with additional powdered sugar to coat.\\n'\n",
      " '\\n'\n",
      " 'Enjoy your homemade Peeps candy!<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'Can you explain the science behind why Peeps float in water?<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'Sure, Peeps float in water because their density is less than that of water. '\n",
      " 'Density is a measure of how much mass is contained in a given volume. When '\n",
      " 'something is less dense than water, it will float on top of the water.\\n'\n",
      " '\\n'\n",
      " 'Peeps are mostly made up of sugar, which is less dense than water. As a '\n",
      " 'result, when a Peep is placed in water, it displaces an amount of water that '\n",
      " 'weighs more than the Peep itself. This displaced water pushes up on the '\n",
      " 'Peep, causing it to float on the surface of the water.\\n'\n",
      " '\\n'\n",
      " 'This is also why objects with a higher density than water, such as a rock, '\n",
      " 'sink to the bottom of the body of water. The displaced water weighs less '\n",
      " 'than the object being submerged, causing the object to sink.\\n'\n",
      " '\\n'\n",
      " 'So, in summary, Peeps float in water because they are less dense than water, '\n",
      " 'causing the displaced water to push them up and keep them '\n",
      " 'afloat.<|im_end|>\\n')\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:50.483908Z",
     "start_time": "2025-10-09T09:56:50.477182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = SFTConfig.batch_size\n",
    "gradient_accumulation_steps = SFTConfig.gradient_accumulation_steps\n",
    "log_iter = SFTConfig.log_iter\n",
    "max_lr = SFTConfig.max_lr\n",
    "min_lr = SFTConfig.min_lr\n",
    "warmup_steps = SFTConfig.warmup_steps\n",
    "total_steps = len(chosen_input_ids_list) // batch_size\n",
    "optimizer = torch.optim.AdamW(filter(\n",
    "    lambda p: p.requires_grad,\n",
    "    model.parameters()\n",
    "), lr=max_lr)\n",
    "trainable_parameters_num = sum(p.numel() for p in filter(\n",
    "    lambda p: p.requires_grad,\n",
    "    model.parameters())\n",
    ")  ##å…¨å‚å¾®è°ƒ"
   ],
   "id": "22f4d72353fb6cd1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:52.968172Z",
     "start_time": "2025-10-09T09:56:52.961924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##é…ç½®logging\n",
    "import time\n",
    "\n",
    "with open(f\"./Qwen2.5-0.5B-SFT_log.txt\", \"a\") as my_file:\n",
    "    my_file.write(f' \\\n",
    "        time:{time.strftime(\"%Y-%m-%d, %H:%M:%S\")}, \\\n",
    "        batch_size:{batch_size}, \\\n",
    "        trainable_parameters_num:{trainable_parameters_num}, \\\n",
    "        warmup_steps:{warmup_steps}, \\\n",
    "        max_lr:{max_lr}, \\\n",
    "        min_lr:{min_lr}\\n')\n",
    "\n",
    "#å®šä¹‰ä¸€ä¸ªæ—¥å¿—è®°å½•å‡½æ•°\n",
    "def log_call(iters, iters_average_loss):\n",
    "    with open(f\"./Qwen2.5-0.5B-SFT_log.txt\", \"a\") as my_file:\n",
    "        my_file.write(f' \\\n",
    "            time:{time.strftime(\"%Y-%m-%d, %H:%M:%S\")}, \\\n",
    "            iters:{iters+1}, \\\n",
    "            iters_average_Loss:{iters_average_loss:.4f}\\n')"
   ],
   "id": "66ccafae47bfcf5a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:55.213448Z",
     "start_time": "2025-10-09T09:56:55.209835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_warmup(current_step, warmup_steps, max_lr):\n",
    "    if current_step < warmup_steps:\n",
    "        return max_lr * current_step / warmup_steps\n",
    "    else:\n",
    "        return max_lr\n",
    "\n",
    "def cosine_decay(current_step, warmup_steps, total_steps, max_lr, min_lr):\n",
    "    if current_step < warmup_steps:\n",
    "        return linear_warmup(current_step, warmup_steps, max_lr)\n",
    "    else:\n",
    "        progress = (current_step - warmup_steps)          \\\n",
    "                   /                                      \\\n",
    "                   (total_steps - warmup_steps)\n",
    "        decay = 0.5 * (1 + np.cos(np.pi * progress))\n",
    "        return (max_lr - min_lr) * decay + min_lr"
   ],
   "id": "5cc5f9f818cf6efe",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:57.860681Z",
     "start_time": "2025-10-09T09:56:57.852791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_answer_mask(input_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºä»…å¯¹åŠ©æ‰‹å›ç­”éƒ¨åˆ†è®¡ç®—æŸå¤±çš„æ©ç \n",
    "\n",
    "    Args:\n",
    "        input_ids: è¾“å…¥tokenåºåˆ— [batch_size, seq_len]\n",
    "        tokenizer: åˆ†è¯å™¨\n",
    "\n",
    "    Returns:\n",
    "        answer_mask: åŠ©æ‰‹å›ç­”éƒ¨åˆ†ä¸º1ï¼Œå…¶ä»–éƒ¨åˆ†ä¸º0çš„æ©ç \n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    answer_mask = torch.zeros_like(input_ids)\n",
    "\n",
    "    # è·å–ç»“æŸæ ‡è®°çš„token id\n",
    "    eos_token_id = tokenizer.encode('<|im_end|>')[0]\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        # æ‰¾åˆ°æ‰€æœ‰ <|im_end|> çš„ä½ç½®\n",
    "        eos_positions = torch.where(\n",
    "            input_ids[batch_idx] == eos_token_id\n",
    "        )[0].tolist()\n",
    "\n",
    "        if len(eos_positions) < 2:  # è‡³å°‘éœ€è¦userå’Œassistantå„ä¸€ä¸ªç»“æŸæ ‡è®°\n",
    "            continue\n",
    "\n",
    "        # è§£æå¯¹è¯è½®æ¬¡\n",
    "        user_ends, assistant_ends = \\\n",
    "            _parse_conversation_turns(eos_positions)\n",
    "\n",
    "        # ä¸ºæ¯ä¸ªåŠ©æ‰‹å›ç­”è®¾ç½®æ©ç \n",
    "        _set_answer_masks(\n",
    "            answer_mask[batch_idx],\n",
    "            user_ends,\n",
    "            assistant_ends,\n",
    "            seq_len\n",
    "        )\n",
    "\n",
    "    return answer_mask\n",
    "\n",
    "\n",
    "def _parse_conversation_turns(eos_positions):\n",
    "    \"\"\"\n",
    "    è§£æå¯¹è¯è½®æ¬¡ï¼Œåˆ†ç¦»ç”¨æˆ·å’ŒåŠ©æ‰‹çš„ç»“æŸä½ç½®\n",
    "\n",
    "    å¯¹è¯æ ¼å¼ï¼š\n",
    "    <|im_start|>user\\n{user_msg}<|im_end|>\\n<|im_start|>assistant\\n{assistant_msg}<|im_end|>\\n\n",
    "\n",
    "    eos_positions[0]: systemç»“æŸ (å¦‚æœæœ‰)\n",
    "    eos_positions[1]: ç¬¬1è½®userç»“æŸ\n",
    "    eos_positions[2]: ç¬¬1è½®assistantç»“æŸ\n",
    "    eos_positions[3]: ç¬¬2è½®userç»“æŸ\n",
    "    eos_positions[4]: ç¬¬2è½®assistantç»“æŸ\n",
    "    ...\n",
    "    \"\"\"\n",
    "    # è·³è¿‡systemç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼Œä»ç¬¬ä¸€ä¸ªuserå¼€å§‹\n",
    "    conversation_eos = eos_positions[1:]  # å»æ‰systemçš„eos\n",
    "\n",
    "    # å¥‡æ•°ç´¢å¼•ï¼šuserç»“æŸä½ç½®ï¼Œå¶æ•°ç´¢å¼•ï¼šassistantç»“æŸä½ç½®\n",
    "    user_ends = [pos + 1 for pos in conversation_eos[::2]] # æ¯éš”2ä¸ªå–ä¸€ä¸ªï¼Œä»0å¼€å§‹\n",
    "    assistant_ends = [pos + 1 for pos in conversation_eos[1::2]] # æ¯éš”2ä¸ªå–ä¸€ä¸ªï¼Œä»1å¼€å§‹\n",
    "\n",
    "    return user_ends, assistant_ends\n",
    "\n",
    "\n",
    "def _set_answer_masks(mask, user_ends, assistant_ends, seq_len):\n",
    "    \"\"\"\n",
    "    ä¸ºåŠ©æ‰‹å›ç­”éƒ¨åˆ†è®¾ç½®æ©ç \n",
    "\n",
    "    Args:\n",
    "        mask: å½“å‰æ ·æœ¬çš„æ©ç  [seq_len]\n",
    "        user_ends: ç”¨æˆ·æ¶ˆæ¯ç»“æŸä½ç½®åˆ—è¡¨\n",
    "        assistant_ends: åŠ©æ‰‹æ¶ˆæ¯ç»“æŸä½ç½®åˆ—è¡¨\n",
    "        seq_len: åºåˆ—é•¿åº¦\n",
    "    \"\"\"\n",
    "    num_user_turns = len(user_ends)\n",
    "    num_assistant_turns = len(assistant_ends)\n",
    "\n",
    "    if num_user_turns == num_assistant_turns:\n",
    "        # å®Œæ•´å¯¹è¯ï¼šæ¯è½®éƒ½æœ‰ç”¨æˆ·é—®é¢˜å’ŒåŠ©æ‰‹å›ç­”\n",
    "        for user_end, assistant_end in zip(user_ends, assistant_ends):\n",
    "            answer_start = user_end + 3  # è·³è¿‡ <|im_start|>assistant\\n\n",
    "            answer_end = assistant_end - 1  # ä¸åŒ…å« <|im_end|>\n",
    "            mask[answer_start:answer_end] = 1\n",
    "\n",
    "    elif num_user_turns == num_assistant_turns + 1:\n",
    "        # æœªå®Œæˆå¯¹è¯ï¼šæœ€åä¸€è½®åŠ©æ‰‹å›ç­”è¢«æˆªæ–­\n",
    "\n",
    "        # å¤„ç†å®Œæ•´çš„å¯¹è¯è½®æ¬¡\n",
    "        for user_end, assistant_end in zip(user_ends[:-1], assistant_ends):\n",
    "            answer_start = user_end + 3\n",
    "            answer_end = assistant_end - 1\n",
    "            mask[answer_start:answer_end] = 1\n",
    "\n",
    "        # å¤„ç†æœ€åä¸€è½®è¢«æˆªæ–­çš„åŠ©æ‰‹å›ç­”\n",
    "        last_user_end = user_ends[-1]\n",
    "        last_answer_start = last_user_end + 3\n",
    "        mask[last_answer_start:] = 1  # åˆ°åºåˆ—ç»“å°¾"
   ],
   "id": "7ef9a43441698054",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T10:02:52.407375Z",
     "start_time": "2025-10-09T09:57:03.587325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "training_losses = []\n",
    "model.zero_grad()  # è®­ç»ƒå¼€å§‹æ—¶æ¸…ç©ºæ¢¯åº¦\n",
    "skipped_batches_count = 0\n",
    "\n",
    "total_batches = len(chosen_input_ids_list) // batch_size\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    ## ==================== æ•°æ®å‡†å¤‡é˜¶æ®µ ====================\n",
    "\n",
    "    # è·å–å½“å‰æ‰¹æ¬¡çš„åŸå§‹æ•°æ®\n",
    "    current_batch_sequences = chosen_input_ids_list[\n",
    "        batch_idx * batch_size : (batch_idx + 1) * batch_size\n",
    "    ]\n",
    "\n",
    "    # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œç”¨äºpaddingå¯¹é½\n",
    "    max_sequence_length = max([len(sequence) for sequence in current_batch_sequences])\n",
    "\n",
    "    ### å¯¹æ‰¹æ¬¡æ•°æ®è¿›è¡Œå³å¡«å……ï¼Œä½¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´ä»¥ä¾¿å¹¶è¡Œè®¡ç®—\n",
    "    padded_sequences_list = []\n",
    "    pad_token_id = model.generation_config.eos_token_id[-1]\n",
    "\n",
    "    for seq_idx in range(batch_size):\n",
    "        # åŸå§‹çš„ä¸€æ¡è®­ç»ƒæ•°æ®\n",
    "        original_sequence = current_batch_sequences[seq_idx]\n",
    "        # è¦å¡«å……çš„é•¿åº¦\n",
    "        padding_length = max_sequence_length - len(original_sequence)\n",
    "\n",
    "        # ä½¿ç”¨EOS tokenè¿›è¡Œå³å¡«å……\n",
    "        padded_sequence = torch.nn.functional.pad(\n",
    "            torch.tensor(original_sequence),\n",
    "            (0, padding_length),\n",
    "            mode='constant',\n",
    "            value=pad_token_id\n",
    "        ).tolist()\n",
    "\n",
    "        padded_sequences_list.append(padded_sequence)\n",
    "\n",
    "    # è½¬æ¢ä¸ºå¼ é‡ shape[bs,max_len]\n",
    "    batch_input_tensor = torch.tensor(padded_sequences_list)\n",
    "\n",
    "    ## ==================== æ„å»ºè¾“å…¥è¾“å‡ºå¯¹ ====================\n",
    "\n",
    "    # æ„å»ºå› æœè¯­è¨€æ¨¡å‹çš„è¾“å…¥è¾“å‡ºå¯¹ï¼šx->yï¼ˆä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼‰\n",
    "    model_inputs = batch_input_tensor[:, :-1].to(device)    # è¾“å…¥ï¼šå‰n-1ä¸ªtoken\n",
    "    target_labels = batch_input_tensor[:, 1:].to(device)    # æ ‡ç­¾ï¼šån-1ä¸ªtoken\n",
    "\n",
    "    ## ==================== æ„å»ºè®­ç»ƒæ©ç  ====================\n",
    "\n",
    "    # æ„å»ºæ©ç çŸ©é˜µæ¥æ§åˆ¶æŸå¤±è®¡ç®—èŒƒå›´\n",
    "    # 1. padding_maskï¼šæ ‡è¯†å“ªäº›ä½ç½®æ˜¯å¡«å……tokenï¼ˆä¸è®¡ç®—æŸå¤±ï¼‰\n",
    "    # 2. answer_maskï¼šæ ‡è¯†å“ªäº›ä½ç½®æ˜¯åŠ©æ‰‹å›ç­”éƒ¨åˆ†ï¼ˆåªå¯¹å›ç­”è®¡ç®—æŸå¤±ï¼‰\n",
    "\n",
    "    ### ã€å¡«å……æ©ç ã€‘ï¼šéå¡«å……tokenä¸º1ï¼Œå¡«å……tokenä¸º0\n",
    "    padding_mask = torch.where(target_labels == pad_token_id, 0, 1)\n",
    "\n",
    "    ### ã€å›ç­”æ©ç ã€‘ï¼šåªæœ‰åŠ©æ‰‹å›ç­”éƒ¨åˆ†ä¸º1ï¼Œå…¶ä»–éƒ¨åˆ†ä¸º0\n",
    "    assistant_answer_mask = create_answer_mask(model_inputs, tokenizer)\n",
    "\n",
    "    ### ã€ç»„åˆæ©ç ã€‘ï¼šåŒæ—¶æ»¡è¶³\"éå¡«å……\"ä¸”\"æ˜¯å›ç­”éƒ¨åˆ†\"çš„tokenæ‰è®¡ç®—æŸå¤±\n",
    "    final_loss_mask = (assistant_answer_mask & padding_mask)\n",
    "\n",
    "    ## ==================== æ‰¹æ¬¡æœ‰æ•ˆæ€§æ£€æŸ¥ ====================\n",
    "\n",
    "    # æ£€æŸ¥å½“å‰æ‰¹æ¬¡æ˜¯å¦æœ‰æ•ˆï¼šå¦‚æœæŸä¸ªæ ·æœ¬çš„å›ç­”éƒ¨åˆ†å®Œå…¨ä¸ºç©ºï¼Œåˆ™è·³è¿‡è¯¥æ‰¹æ¬¡\n",
    "    # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨é—®é¢˜è¿‡é•¿å¯¼è‡´å›ç­”éƒ¨åˆ†è¢«æˆªæ–­æ—¶\n",
    "    tokens_per_sample = final_loss_mask.sum(dim=-1)  # æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆå›ç­”tokenæ•°\n",
    "    min_answer_tokens = tokens_per_sample.min().item()  # æœ€å°‘çš„æœ‰æ•ˆtokenæ•°\n",
    "\n",
    "    if min_answer_tokens == 0:\n",
    "        print(f'âš ï¸ è·³è¿‡ç¬¬{batch_idx + 1}æ‰¹æ¬¡ï¼šå›ç­”éƒ¨åˆ†æ•°æ®ä¸è¶³')\n",
    "        skipped_batches_count += 1\n",
    "        continue  # è·³è¿‡å½“å‰æ‰¹æ¬¡\n",
    "\n",
    "    ## ==================== æ¨¡å‹å‰å‘ä¼ æ’­ ====================\n",
    "\n",
    "    # æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œè·å–æ¨¡å‹é¢„æµ‹çš„logits\n",
    "    model_logits = model(model_inputs).logits\n",
    "\n",
    "    ## ==================== æŸå¤±è®¡ç®— ====================\n",
    "\n",
    "    # è®¡ç®—å¸¦æ©ç çš„äº¤å‰ç†µæŸå¤±\n",
    "    # æ­¥éª¤ï¼šlogits -> softmax -> log -> gather -> è´Ÿå¯¹æ•°ä¼¼ç„¶ -> æ©ç è¿‡æ»¤ -> å¹³å‡\n",
    "\n",
    "    # 1. è®¡ç®—æ¯ä¸ªtokençš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±\n",
    "    log_probabilities = torch.log(torch.softmax(model_logits, dim=-1))\n",
    "    gathered_log_probs = torch.gather(\n",
    "        log_probabilities,\n",
    "        dim=-1,\n",
    "        index=target_labels.unsqueeze(2)\n",
    "    )\n",
    "    negative_log_likelihood = gathered_log_probs * (-1)  # è´Ÿå¯¹æ•°ä¼¼ç„¶\n",
    "    token_losses = negative_log_likelihood.squeeze(2)\n",
    "\n",
    "    # 2. åº”ç”¨æ©ç å¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡æŸå¤±\n",
    "    masked_token_losses = torch.mul(token_losses, final_loss_mask)\n",
    "    sample_losses = masked_token_losses.sum(dim=-1) \\\n",
    "                  / final_loss_mask.sum(dim=-1)\n",
    "\n",
    "    # 3. è®¡ç®—æ‰¹æ¬¡å¹³å‡æŸå¤±å¹¶åº”ç”¨æ¢¯åº¦ç´¯ç§¯\n",
    "    batch_average_loss = torch.nanmean(sample_losses) \\\n",
    "                       / gradient_accumulation_steps\n",
    "\n",
    "    ## ==================== åå‘ä¼ æ’­å’Œä¼˜åŒ– ====================\n",
    "\n",
    "    # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
    "    batch_average_loss.backward()\n",
    "\n",
    "    # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼ˆä½™å¼¦è¡°å‡ + é¢„çƒ­ï¼‰\n",
    "    current_learning_rate = cosine_decay(\n",
    "        batch_idx,\n",
    "        warmup_steps,\n",
    "        total_steps,\n",
    "        max_lr,\n",
    "        min_lr\n",
    "    )\n",
    "\n",
    "    # æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_learning_rate\n",
    "\n",
    "    # æ¢¯åº¦ç´¯ç§¯ï¼šåªåœ¨ç´¯ç§¯æ­¥æ•°è¾¾åˆ°æˆ–æœ€åä¸€ä¸ªæ‰¹æ¬¡æ—¶æ›´æ–°æƒé‡\n",
    "    is_accumulation_step = (batch_idx + 1) \\\n",
    "                         % gradient_accumulation_steps == 0\n",
    "    is_final_batch = (batch_idx + 1) == total_batches\n",
    "\n",
    "    if is_accumulation_step or is_final_batch:\n",
    "        optimizer.step()        # æ›´æ–°æ¨¡å‹æƒé‡\n",
    "        optimizer.zero_grad()   # æ¸…ç©ºæ¢¯åº¦ç¼“å­˜\n",
    "\n",
    "    ## ==================== è®­ç»ƒæ—¥å¿—è®°å½• ====================\n",
    "\n",
    "    # è®°å½•å½“å‰æ‰¹æ¬¡çš„æŸå¤±ï¼ˆè¿˜åŸæ¢¯åº¦ç´¯ç§¯çš„ç¼©æ”¾ï¼‰\n",
    "    actual_batch_loss =                   \\\n",
    "        batch_average_loss.item()         \\\n",
    "        *                                 \\\n",
    "        gradient_accumulation_steps\n",
    "    training_losses.append(actual_batch_loss)\n",
    "\n",
    "    # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦\n",
    "    should_log = (batch_idx + 1) % log_iter == 0 or is_final_batch\n",
    "\n",
    "    if should_log:\n",
    "        # è®¡ç®—æœ€è¿‘å‡ ä¸ªæ‰¹æ¬¡çš„å¹³å‡æŸå¤±\n",
    "        recent_losses = training_losses[-log_iter:]\n",
    "        recent_average_loss = np.nanmean(recent_losses)\n",
    "\n",
    "        # è¾“å‡ºè®­ç»ƒçŠ¶æ€\n",
    "        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'â° æ—¶é—´: {current_time} | '\n",
    "              f'ğŸ“Š æ‰¹æ¬¡: {batch_idx + 1}/{total_batches} | '\n",
    "              f'ğŸ“ˆ æœ€è¿‘{len(recent_losses)}æ‰¹æ¬¡å¹³å‡æŸå¤±: {recent_average_loss:.4f} | '\n",
    "              f'ğŸ¯ å­¦ä¹ ç‡: {current_learning_rate:.2e}')\n",
    "\n",
    "        # è°ƒç”¨å¤–éƒ¨æ—¥å¿—è®°å½•å‡½æ•°\n",
    "        log_call(batch_idx, recent_average_loss)\n",
    "\n",
    "## ==================== è®­ç»ƒå®Œæˆæ€»ç»“ ====================\n",
    "\n",
    "print(\"ğŸ‰ è®­ç»ƒå®Œæˆ!\")\n",
    "print(f'ğŸ“Š è®­ç»ƒç»Ÿè®¡:')\n",
    "print(f'   - æ€»æ‰¹æ¬¡æ•°: {total_batches}')\n",
    "print(f'   - è·³è¿‡æ‰¹æ¬¡æ•°: {skipped_batches_count}')\n",
    "print(f'   - æœ‰æ•ˆæ‰¹æ¬¡æ•°: {total_batches - skipped_batches_count}')\n",
    "print(f'   - æœ€ç»ˆå¹³å‡æŸå¤±: {np.nanmean(training_losses[-100:]):.4f}')\n",
    "\n",
    "if skipped_batches_count > 0:\n",
    "    skip_ratio = skipped_batches_count / total_batches * 100\n",
    "    print(f'âš ï¸ è·³è¿‡æ‰¹æ¬¡å æ¯”: {skip_ratio:.2f}%')\n",
    "    if skip_ratio > 10:\n",
    "        print('ğŸ’¡ å»ºè®®: è·³è¿‡æ‰¹æ¬¡è¿‡å¤šï¼Œè€ƒè™‘å¢åŠ æœ€å¤§åºåˆ—é•¿åº¦æˆ–ä¼˜åŒ–æ•°æ®é¢„å¤„ç†')"
   ],
   "id": "ef896bef4e76290b",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 37.96 GiB, other allocations: 11.74 GiB, max allowed: 47.74 GiB). Tried to allocate 1.14 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 107\u001B[0m\n\u001B[1;32m    101\u001B[0m batch_average_loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnanmean(sample_losses) \\\n\u001B[1;32m    102\u001B[0m                    \u001B[38;5;241m/\u001B[39m gradient_accumulation_steps\n\u001B[1;32m    104\u001B[0m \u001B[38;5;66;03m## ==================== åå‘ä¼ æ’­å’Œä¼˜åŒ– ====================\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \n\u001B[1;32m    106\u001B[0m \u001B[38;5;66;03m# åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\u001B[39;00m\n\u001B[0;32m--> 107\u001B[0m \u001B[43mbatch_average_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;66;03m# åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼ˆä½™å¼¦è¡°å‡ + é¢„çƒ­ï¼‰\u001B[39;00m\n\u001B[1;32m    110\u001B[0m current_learning_rate \u001B[38;5;241m=\u001B[39m cosine_decay(\n\u001B[1;32m    111\u001B[0m     batch_idx,\n\u001B[1;32m    112\u001B[0m     warmup_steps,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    115\u001B[0m     min_lr\n\u001B[1;32m    116\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/_tensor.py:647\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    639\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    640\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    645\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    646\u001B[0m     )\n\u001B[0;32m--> 647\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    648\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    349\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 354\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    827\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 829\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 37.96 GiB, other allocations: 11.74 GiB, max allowed: 47.74 GiB). Tried to allocate 1.14 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
