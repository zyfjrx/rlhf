{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:00.278612Z",
     "start_time": "2025-10-10T01:06:51.952024Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/conda/envs/rlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "device = \"cuda\"\n",
    "model_path = \"/root/sft/pretrained/Qwen2.5-0.5B-SFT/\"\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# 冻结的参考模型\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0187f5e9ac39c3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:02.986199Z",
     "start_time": "2025-10-10T01:07:02.983161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generation_config)\n",
    "\n",
    "model.generation_config.do_sample = True\n",
    "model.generation_config.eos_token_id = [151645, 151643]\n",
    "model.generation_config.pad_token_id = 151643\n",
    "model.generation_config.temperature = 0.7\n",
    "model.generation_config.top_p = 0.8\n",
    "model.generation_config.top_k = 20\n",
    "model.generation_config.repetition_penalty = 1.05\n",
    "\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b1f6d62a0ba993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:05.647486Z",
     "start_time": "2025-10-10T01:07:05.645453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(151643))\n",
    "print(tokenizer.decode(151645))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48355c9f48b4616d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:07.561720Z",
     "start_time": "2025-10-10T01:07:07.559382Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DPOConfig:\n",
    "    max_length:int = 1700 #根据自身具备的算力条件进行自适应更改\n",
    "    batch_size:int = 2\n",
    "    gradient_accumulation_steps:int = 8\n",
    "    beta:float = 0.5 # β是dpo公式中的超参数\n",
    "    log_iter:int = 200\n",
    "    max_lr:float = 1e-6\n",
    "    min_lr:float = 1e-7\n",
    "    warmup_steps:int = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f116655db57617ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:10.815314Z",
     "start_time": "2025-10-10T01:07:09.558298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train_prefs split: 100%|██████████| 61135/61135 [00:00<00:00, 128875.74 examples/s]\n",
      "Generating train_sft split: 100%|██████████| 61135/61135 [00:00<00:00, 144920.78 examples/s]\n",
      "Generating test_prefs split: 100%|██████████| 2000/2000 [00:00<00:00, 93684.55 examples/s]\n",
      "Generating test_sft split: 100%|██████████| 1000/1000 [00:00<00:00, 79842.84 examples/s]\n",
      "Generating train_gen split: 100%|██████████| 61135/61135 [00:00<00:00, 154098.93 examples/s]\n",
      "Generating test_gen split: 100%|██████████| 1000/1000 [00:00<00:00, 82143.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "binarized_data = datasets.load_dataset('/root/sft/dataset/ultrafeedback_binarized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce6a2e4844fddbb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:13.039011Z",
     "start_time": "2025-10-10T01:07:13.034111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'how can i develop a habit of drawing daily',\n",
       " 'prompt_id': '086b3e24f29b8956a01059f79c56db35d118a06fb6b844b095737d042795cd43',\n",
       " 'chosen': [{'content': 'how can i develop a habit of drawing daily',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\",\n",
       "   'role': 'assistant'}],\n",
       " 'rejected': [{'content': 'how can i develop a habit of drawing daily',\n",
       "   'role': 'user'},\n",
       "  {'content': \"As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\",\n",
       "   'role': 'assistant'}],\n",
       " 'messages': [{'content': 'how can i develop a habit of drawing daily',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\",\n",
       "   'role': 'assistant'}],\n",
       " 'score_chosen': 8.5,\n",
       " 'score_rejected': 8.5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarized_data['train_prefs'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2fc1023fbb350b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-10-10T01:07:15.053736Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "偏好数据已处理10000条数据\n",
      "偏好数据已处理20000条数据\n",
      "偏好数据已处理30000条数据\n",
      "----------------------------------------------------------------------\n",
      "非偏好数据已处理10000条数据\n",
      "非偏好数据已处理20000条数据\n",
      "非偏好数据已处理30000条数据\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_format(data):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        data,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = False,\n",
    "        truncation = True,\n",
    "        max_length = DPOConfig.max_length,\n",
    "    )\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "## 生成偏好数据的input_ids\n",
    "chosen_input_ids_list = []\n",
    "i = 0\n",
    "while True:\n",
    "    data = binarized_data['train_sft'][i]['chosen']\n",
    "    data.insert(\n",
    "        0,\n",
    "        {\"content\": \"You are a helpful assistant\", \"role\": \"system\"}\n",
    "    )\n",
    "    input_ids = tokenize_and_format(data)\n",
    "    chosen_input_ids_list.append(input_ids)\n",
    "    i += 1\n",
    "    if i % 10000 == 0 or i == len(binarized_data['train_sft']):\n",
    "        print(f\"偏好数据已处理{i}条数据\")\n",
    "    if i == 30000:\n",
    "        break\n",
    "print('-' * 70)\n",
    "\n",
    "#############################################################################\n",
    "## 生成不偏好数据的input_ids\n",
    "rejected_input_ids_list = []\n",
    "i = 0\n",
    "while True:\n",
    "    data = binarized_data['train_sft'][i]['rejected']\n",
    "    data.insert(\n",
    "        0,\n",
    "        {\"content\": \"You are a helpful assistant\", \"role\": \"system\"}\n",
    "    )\n",
    "    input_ids = tokenize_and_format(data)\n",
    "    rejected_input_ids_list.append(input_ids)\n",
    "    i += 1\n",
    "    if i % 10000 == 0 or i == len(binarized_data['train_sft']):\n",
    "        print(f\"非偏好数据已处理{i}条数据\")\n",
    "    if i == 30000:\n",
    "        break\n",
    "\n",
    "## 确保数据条数一致\n",
    "assert len(chosen_input_ids_list) == len(rejected_input_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6e10466dec9c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:43.770005Z",
     "start_time": "2025-10-09T09:56:43.765469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|im_start|>system\\n'\n",
      " 'You are a helpful assistant<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'how can i develop a habit of drawing daily<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'Developing a daily habit of drawing can be challenging but with consistent '\n",
      " 'practice and a few tips, it can become an enjoyable and rewarding part of '\n",
      " 'your daily routine. Here are some strategies to help you develop the habit '\n",
      " 'of drawing daily:\\n'\n",
      " '\\n'\n",
      " '1. Set a specific time: Allocate a specific time of the day to draw. It '\n",
      " 'could be in the morning, afternoon, or evening. Make drawing a part of your '\n",
      " 'daily routine.\\n'\n",
      " '2. Set a specific duration: Determine the amount of time you want to spend '\n",
      " 'on drawing each day. It can be as little as 10 minutes or as long as an '\n",
      " 'hour. Be consistent with the duration to help build the habit.\\n'\n",
      " \"3. Start small and simple: Don't try to create a masterpiece every day, \"\n",
      " 'start with simple and easy-to-do sketches. Focus on improving your skills '\n",
      " 'gradually.\\n'\n",
      " '4. Use a variety of tools and mediums: Experiment with different tools like '\n",
      " 'pencils, pens, markers, and different mediums like paper, canvas, or digital '\n",
      " 'apps to keep your drawing practice interesting and engaging.\\n'\n",
      " '5. Take breaks and rest: Taking breaks and resting after some time of '\n",
      " 'drawing can help you avoid burnout and stay motivated.\\n'\n",
      " '6. Challenge yourself: Set challenges like drawing objects from memory or a '\n",
      " 'specific subject to improve your skills and keep your drawing practice '\n",
      " 'interesting.\\n'\n",
      " '7. Track your progress: Keep a record of your daily drawing practice and '\n",
      " 'track your progress. This can be a source of motivation and help you see how '\n",
      " \"far you've come.\\n\"\n",
      " '\\n'\n",
      " 'Remember, developing a habit takes time and patience. Stay consistent with '\n",
      " 'your drawing practice, be flexible and open to trying new things, and with '\n",
      " \"time, you'll develop a habit of daily drawing that brings you joy and \"\n",
      " 'satisfaction.<|im_end|>\\n')\n",
      "----------------------------------------------------------------------\n",
      "('<|im_start|>system\\n'\n",
      " 'You are a helpful assistant<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'how can i develop a habit of drawing daily<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'As an AI language model, I cannot personally develop habits for you. But, '\n",
      " 'here are some tips for developing a habit of drawing daily:\\n'\n",
      " '\\n'\n",
      " '1. Start small: Start with simple drawings or doodles and gradually increase '\n",
      " 'the complexity of your drawings.\\n'\n",
      " '\\n'\n",
      " '2. Set a schedule: Set a specific time every day to draw, and try to stick '\n",
      " 'to it as much as possible.\\n'\n",
      " '\\n'\n",
      " \"3. Make it fun: Don't pressure yourself to create masterpieces every time \"\n",
      " 'you draw. Make it a relaxing and enjoyable experience.\\n'\n",
      " '\\n'\n",
      " '4. Use resources: There are many drawing tutorials available online. Use '\n",
      " 'resources like YouTube or online drawing courses to help you improve your '\n",
      " 'skills.\\n'\n",
      " '\\n'\n",
      " '5. Surround yourself with inspiration: Expose yourself to a variety of art '\n",
      " 'forms, such as paintings, illustrations, and photographs, to inspire and '\n",
      " 'motivate you.\\n'\n",
      " '\\n'\n",
      " 'Remember, everyone has their own creative style and pace. Just keep '\n",
      " 'practicing and enjoying the process of drawing.<|im_end|>\\n')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# pprint(tokenizer.decode(chosen_input_ids_list[0]))\n",
    "print('-' * 70)\n",
    "pprint(tokenizer.decode(rejected_input_ids_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22f4d72353fb6cd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:50.483908Z",
     "start_time": "2025-10-09T09:56:50.477182Z"
    }
   },
   "outputs": [],
   "source": [
    "beta = DPOConfig.beta # β超参数\n",
    "batch_size = DPOConfig.batch_size\n",
    "gradient_accumulation_steps = DPOConfig.gradient_accumulation_steps\n",
    "log_iter = DPOConfig.log_iter\n",
    "max_lr = DPOConfig.max_lr\n",
    "min_lr = DPOConfig.min_lr\n",
    "warmup_steps = DPOConfig.warmup_steps\n",
    "total_steps = len(chosen_input_ids_list) // batch_size\n",
    "optimizer = torch.optim.AdamW(filter(lambda p:p.requires_grad, model.parameters()), lr=max_lr)\n",
    "trainable_parameters_num = sum(p.numel() for p in filter(lambda p:p.requires_grad, model.parameters()))  ##全参微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66ccafae47bfcf5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:52.968172Z",
     "start_time": "2025-10-09T09:56:52.961924Z"
    }
   },
   "outputs": [],
   "source": [
    "##配置logging\n",
    "import time\n",
    "\n",
    "with open(f\"./Qwen2.5-0.5B-DPO_log.txt\", \"a\") as my_file:\n",
    "    my_file.write(f' \\\n",
    "        time:{time.strftime(\"%Y-%m-%d, %H:%M:%S\")}, \\\n",
    "        batch_size:{batch_size}, \\\n",
    "        trainable_parameters_num:{trainable_parameters_num}, \\\n",
    "        warmup_steps:{warmup_steps}, \\\n",
    "        max_lr:{max_lr}, \\\n",
    "        min_lr:{min_lr}\\n')\n",
    "\n",
    "#定义一个日志记录函数\n",
    "def log_call(iters, iters_average_loss):\n",
    "    with open(f\"./Qwen2.5-0.5B-DPO_log.txt\", \"a\") as my_file:\n",
    "        my_file.write(f' \\\n",
    "            time:{time.strftime(\"%Y-%m-%d, %H:%M:%S\")}, \\\n",
    "            iters:{iters+1}, \\\n",
    "            iters_average_Loss:{iters_average_loss:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cc5f9f818cf6efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:55.213448Z",
     "start_time": "2025-10-09T09:56:55.209835Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_warmup(current_step, warmup_steps, max_lr):\n",
    "    if current_step < warmup_steps:\n",
    "        return max_lr * current_step / warmup_steps\n",
    "    else:\n",
    "        return max_lr\n",
    "\n",
    "def cosine_decay(current_step, warmup_steps, total_steps, max_lr, min_lr):\n",
    "    if current_step < warmup_steps:\n",
    "        return linear_warmup(current_step, warmup_steps, max_lr)\n",
    "    else:\n",
    "        progress = (current_step - warmup_steps) \\\n",
    "                 / (total_steps - warmup_steps)\n",
    "        decay = 0.5 * (1 + np.cos(np.pi * progress))\n",
    "        return (max_lr - min_lr) * decay + min_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ef9a43441698054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:57.860681Z",
     "start_time": "2025-10-09T09:56:57.852791Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_answer_mask(input_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    创建仅对助手回答部分计算损失的掩码\n",
    "    \n",
    "    Args:\n",
    "        input_ids: 输入token序列 [batch_size, seq_len]\n",
    "        tokenizer: 分词器\n",
    "    \n",
    "    Returns:\n",
    "        answer_mask: 助手回答部分为1，其他部分为0的掩码\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    answer_mask = torch.zeros_like(input_ids)\n",
    "    \n",
    "    # 获取<im_end>标记的token_id\n",
    "    eos_token_id = tokenizer.encode('<|im_end|>')[0]\n",
    "    \n",
    "    for batch_idx in range(batch_size):\n",
    "        # 找到所有 <|im_end|> 的位置\n",
    "        eos_positions = torch.where(\n",
    "            input_ids[batch_idx] == eos_token_id\n",
    "        )[0].tolist()\n",
    "        \n",
    "        if len(eos_positions) < 2:  # 至少需要user和assistant各一个结束标记\n",
    "            continue\n",
    "            \n",
    "        # 解析对话轮次\n",
    "        user_ends, assistant_ends = _parse_conversation_turns(eos_positions)\n",
    "        \n",
    "        # 为每个助手回答设置掩码\n",
    "        _set_answer_masks(\n",
    "            answer_mask[batch_idx],\n",
    "            user_ends,\n",
    "            assistant_ends,\n",
    "            seq_len\n",
    "        )\n",
    "    \n",
    "    return answer_mask\n",
    "\n",
    "\n",
    "def _parse_conversation_turns(eos_positions):\n",
    "    \"\"\"\n",
    "    解析对话轮次，分离用户和助手的结束位置\n",
    "    \n",
    "    对话格式：\n",
    "    <|im_start|>user\\n{user_msg}<|im_end|>\\n<|im_start|>assistant\\n{assistant_msg}<|im_end|>\\n\n",
    "    \n",
    "    eos_positions[0]: system结束 (如果有)\n",
    "    eos_positions[1]: 第1轮user结束  \n",
    "    eos_positions[2]: 第1轮assistant结束\n",
    "    eos_positions[3]: 第2轮user结束\n",
    "    eos_positions[4]: 第2轮assistant结束\n",
    "    ...\n",
    "    \"\"\"\n",
    "    # 跳过system部分，从第一个user开始\n",
    "    conversation_eos = eos_positions[1:]  # 去掉system的<im_end>\n",
    "    \n",
    "    # 偶数索引：user结束位置，奇数索引：assistant结束位置\n",
    "    user_ends = [pos + 1 for pos in conversation_eos[::2]] # 每隔2个取一个，从0开始\n",
    "    assistant_ends = [pos + 1 for pos in conversation_eos[1::2]] # 每隔2个取一个，从1开始\n",
    "    \n",
    "    return user_ends, assistant_ends\n",
    "\n",
    "\n",
    "def _set_answer_masks(mask, user_ends, assistant_ends, seq_len):\n",
    "    \"\"\"\n",
    "    为助手回答部分设置掩码\n",
    "    \n",
    "    Args:\n",
    "        mask: 当前样本的掩码 [seq_len]\n",
    "        user_ends: 用户消息结束位置列表\n",
    "        assistant_ends: 助手消息结束位置列表  \n",
    "        seq_len: 序列长度\n",
    "    \"\"\"\n",
    "    num_user_turns = len(user_ends)\n",
    "    num_assistant_turns = len(assistant_ends)\n",
    "    \n",
    "    if num_user_turns == num_assistant_turns:\n",
    "        # 完整对话：每轮都有用户问题和助手回答\n",
    "        for user_end, assistant_end in zip(user_ends, assistant_ends):\n",
    "            answer_start = user_end + 3  # 跳过 <|im_start|>assistant\\n\n",
    "            answer_end = assistant_end - 1  # 不包含 <|im_end|>\n",
    "            mask[answer_start:answer_end] = 1\n",
    "            \n",
    "    elif num_user_turns == num_assistant_turns + 1:\n",
    "        # 未完成对话：最后一轮助手回答被截断\n",
    "        \n",
    "        # 处理完整的对话轮次\n",
    "        for user_end, assistant_end in zip(user_ends[:-1], assistant_ends):\n",
    "            answer_start = user_end + 3\n",
    "            answer_end = assistant_end - 1\n",
    "            mask[answer_start:answer_end] = 1\n",
    "        \n",
    "        # 处理最后一轮被截断的助手回答\n",
    "        last_user_end = user_ends[-1]\n",
    "        last_answer_start = last_user_end + 3\n",
    "        mask[last_answer_start:] = 1  # 到序列结尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eab96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_average_log_probability(logits, target_labels, mask):\n",
    "    \"\"\"\n",
    "    计算带掩码的平均对数概率\n",
    "    \n",
    "    Args:\n",
    "        logits: 模型输出 [batch_size, seq_len, vocab_size]\n",
    "        target_labels: 目标标签 [batch_size, seq_len]\n",
    "        mask: 计算掩码 [batch_size, seq_len]\n",
    "    \n",
    "    Returns:\n",
    "        average_log_prob: 每个样本的平均对数概率 [batch_size]\n",
    "    \"\"\"\n",
    "    # 计算softmax概率分布\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # 计算对数概率\n",
    "    log_probabilities = torch.log(probabilities)\n",
    "    \n",
    "    # 获取目标token的对数概率\n",
    "    gathered_log_probs = torch.gather(\n",
    "        log_probabilities, \n",
    "        dim=-1, \n",
    "        index=target_labels.unsqueeze(2)\n",
    "    ).squeeze(2)\n",
    "    \n",
    "    # 应用掩码并计算平均值\n",
    "    masked_log_probs = torch.mul(gathered_log_probs, mask)\n",
    "    average_log_prob = masked_log_probs.sum(dim=-1) / mask.sum(dim=-1)\n",
    "    \n",
    "    return average_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8068a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "# ==================== 训练指标记录列表 ====================\n",
    "training_losses = []\n",
    "# 偏好的回答的概率\n",
    "preferred_log_probabilities = []\n",
    "# 讨厌的回答的概率\n",
    "rejected_log_probabilities = []\n",
    "# 偏好的回答的奖励\n",
    "preferred_rewards = []\n",
    "# 讨厌的回答的奖励\n",
    "rejected_rewards = []\n",
    "reward_margins = []\n",
    "\n",
    "model.zero_grad()  # 训练开始时清空梯度\n",
    "skipped_batches_count = 0\n",
    "total_batches = len(chosen_input_ids_list) // batch_size\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    ## ==================== 获取批次数据 ====================\n",
    "    \n",
    "    # 获取当前批次的偏好对数据\n",
    "    preferred_batch_sequences = chosen_input_ids_list[\n",
    "        batch_idx * batch_size:(batch_idx + 1) * batch_size\n",
    "    ]\n",
    "    rejected_batch_sequences = rejected_input_ids_list[\n",
    "        batch_idx * batch_size:(batch_idx + 1) * batch_size\n",
    "    ]\n",
    "\n",
    "    ## ==================== 数据填充对齐 ====================\n",
    "    \n",
    "    # 计算各自批次的最大序列长度\n",
    "    preferred_max_length = max([len(sequence) for sequence in preferred_batch_sequences])\n",
    "    rejected_max_length = max([len(sequence) for sequence in rejected_batch_sequences])\n",
    "    # 使用eos token作为pad token\n",
    "    pad_token_id = model.generation_config.eos_token_id[-1]\n",
    "    \n",
    "    ### 偏好数据填充处理\n",
    "    preferred_padded_sequences = []\n",
    "    for seq_idx in range(batch_size):\n",
    "        original_sequence = preferred_batch_sequences[seq_idx]\n",
    "        # 计算要填充多少个pad\n",
    "        padding_length = preferred_max_length - len(original_sequence)\n",
    "        # 在训练数据的末尾填充pad\n",
    "        padded_sequence = torch.nn.functional.pad(\n",
    "            torch.tensor(original_sequence), \n",
    "            (0, padding_length), \n",
    "            mode='constant', \n",
    "            value=pad_token_id\n",
    "        ).tolist()\n",
    "        # 将填充过的数据放入列表\n",
    "        preferred_padded_sequences.append(padded_sequence)\n",
    "    \n",
    "    preferred_batch_tensor = torch.tensor(preferred_padded_sequences)\n",
    "    \n",
    "    ### 拒绝数据填充处理\n",
    "    rejected_padded_sequences = []\n",
    "    for seq_idx in range(batch_size):\n",
    "        original_sequence = rejected_batch_sequences[seq_idx]\n",
    "        padding_length = rejected_max_length - len(original_sequence)\n",
    "        \n",
    "        padded_sequence = torch.nn.functional.pad(\n",
    "            torch.tensor(original_sequence), \n",
    "            (0, padding_length), \n",
    "            mode='constant', \n",
    "            value=pad_token_id\n",
    "        ).tolist()\n",
    "        \n",
    "        rejected_padded_sequences.append(padded_sequence)\n",
    "    \n",
    "    rejected_batch_tensor = torch.tensor(rejected_padded_sequences)\n",
    "\n",
    "    ## ==================== 构建输入输出对 ====================\n",
    "    \n",
    "    # 构建因果语言模型的输入输出对：x->y（下一个词预测）\n",
    "    # 模型的输入：偏好的回答\n",
    "    preferred_model_inputs = preferred_batch_tensor[:, :-1].to(device)\n",
    "    # 真实的标签\n",
    "    preferred_target_labels = preferred_batch_tensor[:, 1:].to(device)\n",
    "    \n",
    "    rejected_model_inputs = rejected_batch_tensor[:, :-1].to(device)\n",
    "    rejected_target_labels = rejected_batch_tensor[:, 1:].to(device)\n",
    "\n",
    "    ## ==================== 构建训练掩码 ====================\n",
    "    \n",
    "    # 构建掩码矩阵：padding_mask（忽略填充token）+ answer_mask（只关注回答部分）\n",
    "    \n",
    "    # pad_token_id 对应的置为 0 ，其它置为 1 。\n",
    "    preferred_padding_mask = torch.where(\n",
    "        preferred_target_labels == pad_token_id,\n",
    "        0,\n",
    "        1\n",
    "    )\n",
    "    rejected_padding_mask = torch.where(\n",
    "        rejected_target_labels == pad_token_id,\n",
    "        0,\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    # 助手回答的掩码：将助手回答的部分掩码为 1 。其它都是 0 。\n",
    "    preferred_answer_mask = create_answer_mask(\n",
    "        preferred_model_inputs,\n",
    "        tokenizer\n",
    "    )\n",
    "    rejected_answer_mask = create_answer_mask(\n",
    "        rejected_model_inputs,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    # 最终掩码：取交集\n",
    "    preferred_final_mask = (preferred_answer_mask & preferred_padding_mask)\n",
    "    rejected_final_mask = (rejected_answer_mask & rejected_padding_mask)\n",
    "\n",
    "    ## ==================== 批次有效性检查 ====================\n",
    "    \n",
    "    # 检查偏好对数据是否都有有效的回答部分\n",
    "    preferred_min_tokens = preferred_final_mask.sum(dim=-1).min().item()\n",
    "    rejected_min_tokens = rejected_final_mask.sum(dim=-1).min().item()\n",
    "    \n",
    "    if preferred_min_tokens == 0 or rejected_min_tokens == 0:\n",
    "        print(f'⚠️ 跳过第{batch_idx + 1}批次：偏好对数据回答部分不足')\n",
    "        skipped_batches_count += 1\n",
    "        continue  # 跳过当前批次\n",
    "\n",
    "    ## ==================== 模型前向传播 ====================\n",
    "    \n",
    "    # 训练模型对偏好数据的前向传播\n",
    "    preferred_logits = model(preferred_model_inputs).logits\n",
    "    torch.cuda.empty_cache()  # 清理GPU显存\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    # 训练模型对拒绝数据的前向传播\n",
    "    rejected_logits = model(rejected_model_inputs).logits\n",
    "    torch.cuda.empty_cache()  # 清理GPU显存\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    # 参考模型的前向传播（不计算梯度）\n",
    "    with torch.no_grad():\n",
    "        reference_preferred_logits = ref_model(preferred_model_inputs) \\\n",
    "            .logits                                                    \\\n",
    "            .detach()\n",
    "        reference_rejected_logits = ref_model(rejected_model_inputs)   \\\n",
    "            .logits                                                    \\\n",
    "            .detach()\n",
    "\n",
    "    ## ==================== DPO损失计算 ====================\n",
    "    \"\"\"\n",
    "    DPO (Direct Preference Optimization) 论文: https://arxiv.org/pdf/2305.18290.pdf\n",
    "    核心思想：通过偏好对比学习，无需显式奖励模型\n",
    "    \"\"\"\n",
    "    \n",
    "    # 计算平均对数概率 (average_log_prob = True)\n",
    "    # 参考: https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L924\n",
    "    \n",
    "    ### 训练模型的对数概率\n",
    "    ### 正在微调的模型，接收到正例的logits，计算对数概率\n",
    "    preferred_log_prob = _compute_average_log_probability(\n",
    "        preferred_logits,\n",
    "        preferred_target_labels,\n",
    "        preferred_final_mask\n",
    "    )\n",
    "    rejected_log_prob = _compute_average_log_probability(\n",
    "        rejected_logits,\n",
    "        rejected_target_labels,\n",
    "        rejected_final_mask\n",
    "    )\n",
    "    \n",
    "    ### 参考模型的对数概率\n",
    "    reference_preferred_log_prob = _compute_average_log_probability(\n",
    "        reference_preferred_logits,\n",
    "        preferred_target_labels,\n",
    "        preferred_final_mask\n",
    "    )\n",
    "    reference_rejected_log_prob = _compute_average_log_probability(\n",
    "        reference_rejected_logits,\n",
    "        rejected_target_labels,\n",
    "        rejected_final_mask\n",
    "    )\n",
    "\n",
    "    ## ==================== 奖励和边际计算 ====================\n",
    "    \n",
    "    # 计算隐式奖励 (基于KL散度)\n",
    "    preferred_implicit_reward =                              \\\n",
    "        beta *                                               \\\n",
    "        (preferred_log_prob - reference_preferred_log_prob)\n",
    "    rejected_implicit_reward =                               \\\n",
    "        beta *                                               \\\n",
    "        (rejected_log_prob - reference_rejected_log_prob)\n",
    "    \n",
    "    # 计算奖励边际 (偏好数据应该有更高的奖励)\n",
    "    reward_margin = preferred_implicit_reward - rejected_implicit_reward\n",
    "    \n",
    "    # DPO损失：-log(sigmoid(margin))\n",
    "    preference_probability = torch.nn.functional.sigmoid(reward_margin)\n",
    "    sample_losses = -torch.log(preference_probability)\n",
    "    \n",
    "    # 批次平均损失 + 梯度累积\n",
    "    batch_average_loss =                          \\\n",
    "        torch.nanmean(sample_losses) /            \\\n",
    "        gradient_accumulation_steps\n",
    "\n",
    "    ## ==================== 反向传播和优化 ====================\n",
    "    \n",
    "    batch_average_loss.backward()\n",
    "\n",
    "    # 动态学习率调整\n",
    "    current_learning_rate = cosine_decay(\n",
    "        batch_idx,\n",
    "        warmup_steps,\n",
    "        total_steps,\n",
    "        max_lr,\n",
    "        min_lr\n",
    "    )\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_learning_rate\n",
    "\n",
    "    # 梯度累积和权重更新\n",
    "    is_accumulation_step = (batch_idx + 1) % gradient_accumulation_steps == 0\n",
    "    is_final_batch = (batch_idx + 1) == total_batches\n",
    "    \n",
    "    if is_accumulation_step or is_final_batch:\n",
    "        optimizer.step()        # 更新权重\n",
    "        optimizer.zero_grad()   # 清空梯度\n",
    "\n",
    "    ## ==================== 训练指标记录 ====================\n",
    "    \n",
    "    # 记录各项训练指标（detach避免梯度追踪）\n",
    "    training_losses.append(\n",
    "        batch_average_loss.detach().item() * gradient_accumulation_steps)\n",
    "    preferred_log_probabilities.append(\n",
    "        torch.nanmean(preferred_log_prob.detach()).item())\n",
    "    rejected_log_probabilities.append(\n",
    "        torch.nanmean(rejected_log_prob.detach()).item())\n",
    "    preferred_rewards.append(\n",
    "        torch.nanmean(preferred_implicit_reward.detach()).item())\n",
    "    rejected_rewards.append(torch.nanmean(\n",
    "        rejected_implicit_reward.detach()).item())\n",
    "    reward_margins.append(\n",
    "        torch.nanmean(reward_margin.detach()).item())\n",
    "\n",
    "    ## ==================== 训练日志输出 ====================\n",
    "    \n",
    "    should_log = (batch_idx + 1) % log_iter == 0 or is_final_batch\n",
    "    \n",
    "    if should_log:\n",
    "        # 计算最近批次的平均指标\n",
    "        recent_loss = np.nanmean(training_losses[-log_iter:])\n",
    "        recent_preferred_logprob = np.nanmean(\n",
    "            preferred_log_probabilities[-log_iter:])\n",
    "        recent_rejected_logprob = np.nanmean(\n",
    "            rejected_log_probabilities[-log_iter:])\n",
    "        recent_preferred_reward = np.nanmean(preferred_rewards[-log_iter:])\n",
    "        recent_rejected_reward = np.nanmean(rejected_rewards[-log_iter:])\n",
    "        recent_margin = np.nanmean(reward_margins[-log_iter:])\n",
    "        \n",
    "        # 格式化输出训练状态\n",
    "        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'⏰ 时间: {current_time}')\n",
    "        print(f'📊 批次: {batch_idx + 1}/{total_batches}')\n",
    "        print(f'📈 最近{log_iter}批次指标:')\n",
    "        print(f'   - 平均损失: {recent_loss:.4f}')\n",
    "        print(f'   - 偏好对数概率: {recent_preferred_logprob:.4f}')\n",
    "        print(f'   - 拒绝对数概率: {recent_rejected_logprob:.4f}')\n",
    "        print(f'   - 偏好奖励: {recent_preferred_reward:.4f}')\n",
    "        print(f'   - 拒绝奖励: {recent_rejected_reward:.4f}')\n",
    "        print(f'   - 奖励边际: {recent_margin:.4f}')\n",
    "        print(f'🎯 学习率: {current_learning_rate:.2e}')\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # 调用外部日志记录\n",
    "        log_call(batch_idx, recent_loss)\n",
    "\n",
    "## ==================== 训练完成总结 ====================\n",
    "\n",
    "print(\"🎉 DPO训练完成!\")\n",
    "print(f'📊 训练统计:')\n",
    "print(f'   - 总批次数: {total_batches}')\n",
    "print(f'   - 跳过批次数: {skipped_batches_count}')\n",
    "print(f'   - 有效批次数: {total_batches - skipped_batches_count}')\n",
    "\n",
    "# 输出最终训练指标\n",
    "if training_losses:\n",
    "    final_metrics = {\n",
    "        'loss': np.nanmean(training_losses[-100:]),\n",
    "        'preferred_logprob': np.nanmean(preferred_log_probabilities[-100:]),\n",
    "        'rejected_logprob': np.nanmean(rejected_log_probabilities[-100:]),\n",
    "        'preferred_reward': np.nanmean(preferred_rewards[-100:]),\n",
    "        'rejected_reward': np.nanmean(rejected_rewards[-100:]),\n",
    "        'margin': np.nanmean(reward_margins[-100:])\n",
    "    }\n",
    "    \n",
    "    print(f'🎯 最终指标 (最近100批次平均):')\n",
    "    for metric_name, metric_value in final_metrics.items():\n",
    "        print(f'   - {metric_name}: {metric_value:.4f}')\n",
    "\n",
    "if skipped_batches_count > 0:\n",
    "    skip_ratio = skipped_batches_count / total_batches * 100\n",
    "    print(f'⚠️ 跳过批次占比: {skip_ratio:.2f}%')\n",
    "    if skip_ratio > 10:\n",
    "        print('💡 建议: 跳过批次过多，考虑增加最大序列长度或优化数据预处理')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./Qwen2.5-0.5B-DPO')\n",
    "tokenizer.save_pretrained('./Qwen2.5-0.5B-DPO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
