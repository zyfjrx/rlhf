{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:00.278612Z",
     "start_time": "2025-10-10T01:06:51.952024Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/conda/envs/rlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "device = \"cuda\"\n",
    "model_path = \"/root/sft/pretrained/Qwen2.5-0.5B-SFT/\"\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# å†»ç»“çš„å‚è€ƒæ¨¡å‹\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0187f5e9ac39c3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:02.986199Z",
     "start_time": "2025-10-10T01:07:02.983161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generation_config)\n",
    "\n",
    "model.generation_config.do_sample = True\n",
    "model.generation_config.eos_token_id = [151645, 151643]\n",
    "model.generation_config.pad_token_id = 151643\n",
    "model.generation_config.temperature = 0.7\n",
    "model.generation_config.top_p = 0.8\n",
    "model.generation_config.top_k = 20\n",
    "model.generation_config.repetition_penalty = 1.05\n",
    "\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b1f6d62a0ba993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:05.647486Z",
     "start_time": "2025-10-10T01:07:05.645453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(151643))\n",
    "print(tokenizer.decode(151645))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48355c9f48b4616d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:07.561720Z",
     "start_time": "2025-10-10T01:07:07.559382Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DPOConfig:\n",
    "    max_length:int = 1700 #æ ¹æ®è‡ªèº«å…·å¤‡çš„ç®—åŠ›æ¡ä»¶è¿›è¡Œè‡ªé€‚åº”æ›´æ”¹\n",
    "    batch_size:int = 2\n",
    "    gradient_accumulation_steps:int = 8\n",
    "    beta:float = 0.5 # Î²æ˜¯dpoå…¬å¼ä¸­çš„è¶…å‚æ•°\n",
    "    log_iter:int = 200\n",
    "    max_lr:float = 1e-6\n",
    "    min_lr:float = 1e-7\n",
    "    warmup_steps:int = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f116655db57617ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:10.815314Z",
     "start_time": "2025-10-10T01:07:09.558298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train_prefs split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61135/61135 [00:00<00:00, 128875.74 examples/s]\n",
      "Generating train_sft split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61135/61135 [00:00<00:00, 144920.78 examples/s]\n",
      "Generating test_prefs split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 93684.55 examples/s]\n",
      "Generating test_sft split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 79842.84 examples/s]\n",
      "Generating train_gen split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61135/61135 [00:00<00:00, 154098.93 examples/s]\n",
      "Generating test_gen split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 82143.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "binarized_data = datasets.load_dataset('/root/sft/dataset/ultrafeedback_binarized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce6a2e4844fddbb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T01:07:13.039011Z",
     "start_time": "2025-10-10T01:07:13.034111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'how can i develop a habit of drawing daily',\n",
       " 'prompt_id': '086b3e24f29b8956a01059f79c56db35d118a06fb6b844b095737d042795cd43',\n",
       " 'chosen': [{'content': 'how can i develop a habit of drawing daily',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\",\n",
       "   'role': 'assistant'}],\n",
       " 'rejected': [{'content': 'how can i develop a habit of drawing daily',\n",
       "   'role': 'user'},\n",
       "  {'content': \"As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\",\n",
       "   'role': 'assistant'}],\n",
       " 'messages': [{'content': 'how can i develop a habit of drawing daily',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\",\n",
       "   'role': 'assistant'}],\n",
       " 'score_chosen': 8.5,\n",
       " 'score_rejected': 8.5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarized_data['train_prefs'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2fc1023fbb350b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-10-10T01:07:15.053736Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åå¥½æ•°æ®å·²å¤„ç†10000æ¡æ•°æ®\n",
      "åå¥½æ•°æ®å·²å¤„ç†20000æ¡æ•°æ®\n",
      "åå¥½æ•°æ®å·²å¤„ç†30000æ¡æ•°æ®\n",
      "----------------------------------------------------------------------\n",
      "éåå¥½æ•°æ®å·²å¤„ç†10000æ¡æ•°æ®\n",
      "éåå¥½æ•°æ®å·²å¤„ç†20000æ¡æ•°æ®\n",
      "éåå¥½æ•°æ®å·²å¤„ç†30000æ¡æ•°æ®\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_format(data):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        data,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = False,\n",
    "        truncation = True,\n",
    "        max_length = DPOConfig.max_length,\n",
    "    )\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "## ç”Ÿæˆåå¥½æ•°æ®çš„input_ids\n",
    "chosen_input_ids_list = []\n",
    "i = 0\n",
    "while True:\n",
    "    data = binarized_data['train_sft'][i]['chosen']\n",
    "    data.insert(\n",
    "        0,\n",
    "        {\"content\": \"You are a helpful assistant\", \"role\": \"system\"}\n",
    "    )\n",
    "    input_ids = tokenize_and_format(data)\n",
    "    chosen_input_ids_list.append(input_ids)\n",
    "    i += 1\n",
    "    if i % 10000 == 0 or i == len(binarized_data['train_sft']):\n",
    "        print(f\"åå¥½æ•°æ®å·²å¤„ç†{i}æ¡æ•°æ®\")\n",
    "    if i == 30000:\n",
    "        break\n",
    "print('-' * 70)\n",
    "\n",
    "#############################################################################\n",
    "## ç”Ÿæˆä¸åå¥½æ•°æ®çš„input_ids\n",
    "rejected_input_ids_list = []\n",
    "i = 0\n",
    "while True:\n",
    "    data = binarized_data['train_sft'][i]['rejected']\n",
    "    data.insert(\n",
    "        0,\n",
    "        {\"content\": \"You are a helpful assistant\", \"role\": \"system\"}\n",
    "    )\n",
    "    input_ids = tokenize_and_format(data)\n",
    "    rejected_input_ids_list.append(input_ids)\n",
    "    i += 1\n",
    "    if i % 10000 == 0 or i == len(binarized_data['train_sft']):\n",
    "        print(f\"éåå¥½æ•°æ®å·²å¤„ç†{i}æ¡æ•°æ®\")\n",
    "    if i == 30000:\n",
    "        break\n",
    "\n",
    "## ç¡®ä¿æ•°æ®æ¡æ•°ä¸€è‡´\n",
    "assert len(chosen_input_ids_list) == len(rejected_input_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6e10466dec9c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:43.770005Z",
     "start_time": "2025-10-09T09:56:43.765469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|im_start|>system\\n'\n",
      " 'You are a helpful assistant<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'how can i develop a habit of drawing daily<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'Developing a daily habit of drawing can be challenging but with consistent '\n",
      " 'practice and a few tips, it can become an enjoyable and rewarding part of '\n",
      " 'your daily routine. Here are some strategies to help you develop the habit '\n",
      " 'of drawing daily:\\n'\n",
      " '\\n'\n",
      " '1. Set a specific time: Allocate a specific time of the day to draw. It '\n",
      " 'could be in the morning, afternoon, or evening. Make drawing a part of your '\n",
      " 'daily routine.\\n'\n",
      " '2. Set a specific duration: Determine the amount of time you want to spend '\n",
      " 'on drawing each day. It can be as little as 10 minutes or as long as an '\n",
      " 'hour. Be consistent with the duration to help build the habit.\\n'\n",
      " \"3. Start small and simple: Don't try to create a masterpiece every day, \"\n",
      " 'start with simple and easy-to-do sketches. Focus on improving your skills '\n",
      " 'gradually.\\n'\n",
      " '4. Use a variety of tools and mediums: Experiment with different tools like '\n",
      " 'pencils, pens, markers, and different mediums like paper, canvas, or digital '\n",
      " 'apps to keep your drawing practice interesting and engaging.\\n'\n",
      " '5. Take breaks and rest: Taking breaks and resting after some time of '\n",
      " 'drawing can help you avoid burnout and stay motivated.\\n'\n",
      " '6. Challenge yourself: Set challenges like drawing objects from memory or a '\n",
      " 'specific subject to improve your skills and keep your drawing practice '\n",
      " 'interesting.\\n'\n",
      " '7. Track your progress: Keep a record of your daily drawing practice and '\n",
      " 'track your progress. This can be a source of motivation and help you see how '\n",
      " \"far you've come.\\n\"\n",
      " '\\n'\n",
      " 'Remember, developing a habit takes time and patience. Stay consistent with '\n",
      " 'your drawing practice, be flexible and open to trying new things, and with '\n",
      " \"time, you'll develop a habit of daily drawing that brings you joy and \"\n",
      " 'satisfaction.<|im_end|>\\n')\n",
      "----------------------------------------------------------------------\n",
      "('<|im_start|>system\\n'\n",
      " 'You are a helpful assistant<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'how can i develop a habit of drawing daily<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n'\n",
      " 'As an AI language model, I cannot personally develop habits for you. But, '\n",
      " 'here are some tips for developing a habit of drawing daily:\\n'\n",
      " '\\n'\n",
      " '1. Start small: Start with simple drawings or doodles and gradually increase '\n",
      " 'the complexity of your drawings.\\n'\n",
      " '\\n'\n",
      " '2. Set a schedule: Set a specific time every day to draw, and try to stick '\n",
      " 'to it as much as possible.\\n'\n",
      " '\\n'\n",
      " \"3. Make it fun: Don't pressure yourself to create masterpieces every time \"\n",
      " 'you draw. Make it a relaxing and enjoyable experience.\\n'\n",
      " '\\n'\n",
      " '4. Use resources: There are many drawing tutorials available online. Use '\n",
      " 'resources like YouTube or online drawing courses to help you improve your '\n",
      " 'skills.\\n'\n",
      " '\\n'\n",
      " '5. Surround yourself with inspiration: Expose yourself to a variety of art '\n",
      " 'forms, such as paintings, illustrations, and photographs, to inspire and '\n",
      " 'motivate you.\\n'\n",
      " '\\n'\n",
      " 'Remember, everyone has their own creative style and pace. Just keep '\n",
      " 'practicing and enjoying the process of drawing.<|im_end|>\\n')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# pprint(tokenizer.decode(chosen_input_ids_list[0]))\n",
    "print('-' * 70)\n",
    "pprint(tokenizer.decode(rejected_input_ids_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22f4d72353fb6cd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:50.483908Z",
     "start_time": "2025-10-09T09:56:50.477182Z"
    }
   },
   "outputs": [],
   "source": [
    "beta = DPOConfig.beta # Î²è¶…å‚æ•°\n",
    "batch_size = DPOConfig.batch_size\n",
    "gradient_accumulation_steps = DPOConfig.gradient_accumulation_steps\n",
    "log_iter = DPOConfig.log_iter\n",
    "max_lr = DPOConfig.max_lr\n",
    "min_lr = DPOConfig.min_lr\n",
    "warmup_steps = DPOConfig.warmup_steps\n",
    "total_steps = len(chosen_input_ids_list) // batch_size\n",
    "optimizer = torch.optim.AdamW(filter(lambda p:p.requires_grad, model.parameters()), lr=max_lr)\n",
    "trainable_parameters_num = sum(p.numel() for p in filter(lambda p:p.requires_grad, model.parameters()))  ##å…¨å‚å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66ccafae47bfcf5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:52.968172Z",
     "start_time": "2025-10-09T09:56:52.961924Z"
    }
   },
   "outputs": [],
   "source": [
    "##é…ç½®logging\n",
    "import time\n",
    "\n",
    "with open(f\"./Qwen2.5-0.5B-DPO_log.txt\", \"a\") as my_file:\n",
    "    my_file.write(f' \\\n",
    "        time:{time.strftime(\"%Y-%m-%d, %H:%M:%S\")}, \\\n",
    "        batch_size:{batch_size}, \\\n",
    "        trainable_parameters_num:{trainable_parameters_num}, \\\n",
    "        warmup_steps:{warmup_steps}, \\\n",
    "        max_lr:{max_lr}, \\\n",
    "        min_lr:{min_lr}\\n')\n",
    "\n",
    "#å®šä¹‰ä¸€ä¸ªæ—¥å¿—è®°å½•å‡½æ•°\n",
    "def log_call(iters, iters_average_loss):\n",
    "    with open(f\"./Qwen2.5-0.5B-DPO_log.txt\", \"a\") as my_file:\n",
    "        my_file.write(f' \\\n",
    "            time:{time.strftime(\"%Y-%m-%d, %H:%M:%S\")}, \\\n",
    "            iters:{iters+1}, \\\n",
    "            iters_average_Loss:{iters_average_loss:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cc5f9f818cf6efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:55.213448Z",
     "start_time": "2025-10-09T09:56:55.209835Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_warmup(current_step, warmup_steps, max_lr):\n",
    "    if current_step < warmup_steps:\n",
    "        return max_lr * current_step / warmup_steps\n",
    "    else:\n",
    "        return max_lr\n",
    "\n",
    "def cosine_decay(current_step, warmup_steps, total_steps, max_lr, min_lr):\n",
    "    if current_step < warmup_steps:\n",
    "        return linear_warmup(current_step, warmup_steps, max_lr)\n",
    "    else:\n",
    "        progress = (current_step - warmup_steps) \\\n",
    "                 / (total_steps - warmup_steps)\n",
    "        decay = 0.5 * (1 + np.cos(np.pi * progress))\n",
    "        return (max_lr - min_lr) * decay + min_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ef9a43441698054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T09:56:57.860681Z",
     "start_time": "2025-10-09T09:56:57.852791Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_answer_mask(input_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºä»…å¯¹åŠ©æ‰‹å›ç­”éƒ¨åˆ†è®¡ç®—æŸå¤±çš„æ©ç \n",
    "    \n",
    "    Args:\n",
    "        input_ids: è¾“å…¥tokenåºåˆ— [batch_size, seq_len]\n",
    "        tokenizer: åˆ†è¯å™¨\n",
    "    \n",
    "    Returns:\n",
    "        answer_mask: åŠ©æ‰‹å›ç­”éƒ¨åˆ†ä¸º1ï¼Œå…¶ä»–éƒ¨åˆ†ä¸º0çš„æ©ç \n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    answer_mask = torch.zeros_like(input_ids)\n",
    "    \n",
    "    # è·å–<im_end>æ ‡è®°çš„token_id\n",
    "    eos_token_id = tokenizer.encode('<|im_end|>')[0]\n",
    "    \n",
    "    for batch_idx in range(batch_size):\n",
    "        # æ‰¾åˆ°æ‰€æœ‰ <|im_end|> çš„ä½ç½®\n",
    "        eos_positions = torch.where(\n",
    "            input_ids[batch_idx] == eos_token_id\n",
    "        )[0].tolist()\n",
    "        \n",
    "        if len(eos_positions) < 2:  # è‡³å°‘éœ€è¦userå’Œassistantå„ä¸€ä¸ªç»“æŸæ ‡è®°\n",
    "            continue\n",
    "            \n",
    "        # è§£æå¯¹è¯è½®æ¬¡\n",
    "        user_ends, assistant_ends = _parse_conversation_turns(eos_positions)\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªåŠ©æ‰‹å›ç­”è®¾ç½®æ©ç \n",
    "        _set_answer_masks(\n",
    "            answer_mask[batch_idx],\n",
    "            user_ends,\n",
    "            assistant_ends,\n",
    "            seq_len\n",
    "        )\n",
    "    \n",
    "    return answer_mask\n",
    "\n",
    "\n",
    "def _parse_conversation_turns(eos_positions):\n",
    "    \"\"\"\n",
    "    è§£æå¯¹è¯è½®æ¬¡ï¼Œåˆ†ç¦»ç”¨æˆ·å’ŒåŠ©æ‰‹çš„ç»“æŸä½ç½®\n",
    "    \n",
    "    å¯¹è¯æ ¼å¼ï¼š\n",
    "    <|im_start|>user\\n{user_msg}<|im_end|>\\n<|im_start|>assistant\\n{assistant_msg}<|im_end|>\\n\n",
    "    \n",
    "    eos_positions[0]: systemç»“æŸ (å¦‚æœæœ‰)\n",
    "    eos_positions[1]: ç¬¬1è½®userç»“æŸ  \n",
    "    eos_positions[2]: ç¬¬1è½®assistantç»“æŸ\n",
    "    eos_positions[3]: ç¬¬2è½®userç»“æŸ\n",
    "    eos_positions[4]: ç¬¬2è½®assistantç»“æŸ\n",
    "    ...\n",
    "    \"\"\"\n",
    "    # è·³è¿‡systeméƒ¨åˆ†ï¼Œä»ç¬¬ä¸€ä¸ªuserå¼€å§‹\n",
    "    conversation_eos = eos_positions[1:]  # å»æ‰systemçš„<im_end>\n",
    "    \n",
    "    # å¶æ•°ç´¢å¼•ï¼šuserç»“æŸä½ç½®ï¼Œå¥‡æ•°ç´¢å¼•ï¼šassistantç»“æŸä½ç½®\n",
    "    user_ends = [pos + 1 for pos in conversation_eos[::2]] # æ¯éš”2ä¸ªå–ä¸€ä¸ªï¼Œä»0å¼€å§‹\n",
    "    assistant_ends = [pos + 1 for pos in conversation_eos[1::2]] # æ¯éš”2ä¸ªå–ä¸€ä¸ªï¼Œä»1å¼€å§‹\n",
    "    \n",
    "    return user_ends, assistant_ends\n",
    "\n",
    "\n",
    "def _set_answer_masks(mask, user_ends, assistant_ends, seq_len):\n",
    "    \"\"\"\n",
    "    ä¸ºåŠ©æ‰‹å›ç­”éƒ¨åˆ†è®¾ç½®æ©ç \n",
    "    \n",
    "    Args:\n",
    "        mask: å½“å‰æ ·æœ¬çš„æ©ç  [seq_len]\n",
    "        user_ends: ç”¨æˆ·æ¶ˆæ¯ç»“æŸä½ç½®åˆ—è¡¨\n",
    "        assistant_ends: åŠ©æ‰‹æ¶ˆæ¯ç»“æŸä½ç½®åˆ—è¡¨  \n",
    "        seq_len: åºåˆ—é•¿åº¦\n",
    "    \"\"\"\n",
    "    num_user_turns = len(user_ends)\n",
    "    num_assistant_turns = len(assistant_ends)\n",
    "    \n",
    "    if num_user_turns == num_assistant_turns:\n",
    "        # å®Œæ•´å¯¹è¯ï¼šæ¯è½®éƒ½æœ‰ç”¨æˆ·é—®é¢˜å’ŒåŠ©æ‰‹å›ç­”\n",
    "        for user_end, assistant_end in zip(user_ends, assistant_ends):\n",
    "            answer_start = user_end + 3  # è·³è¿‡ <|im_start|>assistant\\n\n",
    "            answer_end = assistant_end - 1  # ä¸åŒ…å« <|im_end|>\n",
    "            mask[answer_start:answer_end] = 1\n",
    "            \n",
    "    elif num_user_turns == num_assistant_turns + 1:\n",
    "        # æœªå®Œæˆå¯¹è¯ï¼šæœ€åä¸€è½®åŠ©æ‰‹å›ç­”è¢«æˆªæ–­\n",
    "        \n",
    "        # å¤„ç†å®Œæ•´çš„å¯¹è¯è½®æ¬¡\n",
    "        for user_end, assistant_end in zip(user_ends[:-1], assistant_ends):\n",
    "            answer_start = user_end + 3\n",
    "            answer_end = assistant_end - 1\n",
    "            mask[answer_start:answer_end] = 1\n",
    "        \n",
    "        # å¤„ç†æœ€åä¸€è½®è¢«æˆªæ–­çš„åŠ©æ‰‹å›ç­”\n",
    "        last_user_end = user_ends[-1]\n",
    "        last_answer_start = last_user_end + 3\n",
    "        mask[last_answer_start:] = 1  # åˆ°åºåˆ—ç»“å°¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eab96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_average_log_probability(logits, target_labels, mask):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å¸¦æ©ç çš„å¹³å‡å¯¹æ•°æ¦‚ç‡\n",
    "    \n",
    "    Args:\n",
    "        logits: æ¨¡å‹è¾“å‡º [batch_size, seq_len, vocab_size]\n",
    "        target_labels: ç›®æ ‡æ ‡ç­¾ [batch_size, seq_len]\n",
    "        mask: è®¡ç®—æ©ç  [batch_size, seq_len]\n",
    "    \n",
    "    Returns:\n",
    "        average_log_prob: æ¯ä¸ªæ ·æœ¬çš„å¹³å‡å¯¹æ•°æ¦‚ç‡ [batch_size]\n",
    "    \"\"\"\n",
    "    # è®¡ç®—softmaxæ¦‚ç‡åˆ†å¸ƒ\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # è®¡ç®—å¯¹æ•°æ¦‚ç‡\n",
    "    log_probabilities = torch.log(probabilities)\n",
    "    \n",
    "    # è·å–ç›®æ ‡tokençš„å¯¹æ•°æ¦‚ç‡\n",
    "    gathered_log_probs = torch.gather(\n",
    "        log_probabilities, \n",
    "        dim=-1, \n",
    "        index=target_labels.unsqueeze(2)\n",
    "    ).squeeze(2)\n",
    "    \n",
    "    # åº”ç”¨æ©ç å¹¶è®¡ç®—å¹³å‡å€¼\n",
    "    masked_log_probs = torch.mul(gathered_log_probs, mask)\n",
    "    average_log_prob = masked_log_probs.sum(dim=-1) / mask.sum(dim=-1)\n",
    "    \n",
    "    return average_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8068a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "# ==================== è®­ç»ƒæŒ‡æ ‡è®°å½•åˆ—è¡¨ ====================\n",
    "training_losses = []\n",
    "# åå¥½çš„å›ç­”çš„æ¦‚ç‡\n",
    "preferred_log_probabilities = []\n",
    "# è®¨åŒçš„å›ç­”çš„æ¦‚ç‡\n",
    "rejected_log_probabilities = []\n",
    "# åå¥½çš„å›ç­”çš„å¥–åŠ±\n",
    "preferred_rewards = []\n",
    "# è®¨åŒçš„å›ç­”çš„å¥–åŠ±\n",
    "rejected_rewards = []\n",
    "reward_margins = []\n",
    "\n",
    "model.zero_grad()  # è®­ç»ƒå¼€å§‹æ—¶æ¸…ç©ºæ¢¯åº¦\n",
    "skipped_batches_count = 0\n",
    "total_batches = len(chosen_input_ids_list) // batch_size\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    ## ==================== è·å–æ‰¹æ¬¡æ•°æ® ====================\n",
    "    \n",
    "    # è·å–å½“å‰æ‰¹æ¬¡çš„åå¥½å¯¹æ•°æ®\n",
    "    preferred_batch_sequences = chosen_input_ids_list[\n",
    "        batch_idx * batch_size:(batch_idx + 1) * batch_size\n",
    "    ]\n",
    "    rejected_batch_sequences = rejected_input_ids_list[\n",
    "        batch_idx * batch_size:(batch_idx + 1) * batch_size\n",
    "    ]\n",
    "\n",
    "    ## ==================== æ•°æ®å¡«å……å¯¹é½ ====================\n",
    "    \n",
    "    # è®¡ç®—å„è‡ªæ‰¹æ¬¡çš„æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    preferred_max_length = max([len(sequence) for sequence in preferred_batch_sequences])\n",
    "    rejected_max_length = max([len(sequence) for sequence in rejected_batch_sequences])\n",
    "    # ä½¿ç”¨eos tokenä½œä¸ºpad token\n",
    "    pad_token_id = model.generation_config.eos_token_id[-1]\n",
    "    \n",
    "    ### åå¥½æ•°æ®å¡«å……å¤„ç†\n",
    "    preferred_padded_sequences = []\n",
    "    for seq_idx in range(batch_size):\n",
    "        original_sequence = preferred_batch_sequences[seq_idx]\n",
    "        # è®¡ç®—è¦å¡«å……å¤šå°‘ä¸ªpad\n",
    "        padding_length = preferred_max_length - len(original_sequence)\n",
    "        # åœ¨è®­ç»ƒæ•°æ®çš„æœ«å°¾å¡«å……pad\n",
    "        padded_sequence = torch.nn.functional.pad(\n",
    "            torch.tensor(original_sequence), \n",
    "            (0, padding_length), \n",
    "            mode='constant', \n",
    "            value=pad_token_id\n",
    "        ).tolist()\n",
    "        # å°†å¡«å……è¿‡çš„æ•°æ®æ”¾å…¥åˆ—è¡¨\n",
    "        preferred_padded_sequences.append(padded_sequence)\n",
    "    \n",
    "    preferred_batch_tensor = torch.tensor(preferred_padded_sequences)\n",
    "    \n",
    "    ### æ‹’ç»æ•°æ®å¡«å……å¤„ç†\n",
    "    rejected_padded_sequences = []\n",
    "    for seq_idx in range(batch_size):\n",
    "        original_sequence = rejected_batch_sequences[seq_idx]\n",
    "        padding_length = rejected_max_length - len(original_sequence)\n",
    "        \n",
    "        padded_sequence = torch.nn.functional.pad(\n",
    "            torch.tensor(original_sequence), \n",
    "            (0, padding_length), \n",
    "            mode='constant', \n",
    "            value=pad_token_id\n",
    "        ).tolist()\n",
    "        \n",
    "        rejected_padded_sequences.append(padded_sequence)\n",
    "    \n",
    "    rejected_batch_tensor = torch.tensor(rejected_padded_sequences)\n",
    "\n",
    "    ## ==================== æ„å»ºè¾“å…¥è¾“å‡ºå¯¹ ====================\n",
    "    \n",
    "    # æ„å»ºå› æœè¯­è¨€æ¨¡å‹çš„è¾“å…¥è¾“å‡ºå¯¹ï¼šx->yï¼ˆä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼‰\n",
    "    # æ¨¡å‹çš„è¾“å…¥ï¼šåå¥½çš„å›ç­”\n",
    "    preferred_model_inputs = preferred_batch_tensor[:, :-1].to(device)\n",
    "    # çœŸå®çš„æ ‡ç­¾\n",
    "    preferred_target_labels = preferred_batch_tensor[:, 1:].to(device)\n",
    "    \n",
    "    rejected_model_inputs = rejected_batch_tensor[:, :-1].to(device)\n",
    "    rejected_target_labels = rejected_batch_tensor[:, 1:].to(device)\n",
    "\n",
    "    ## ==================== æ„å»ºè®­ç»ƒæ©ç  ====================\n",
    "    \n",
    "    # æ„å»ºæ©ç çŸ©é˜µï¼špadding_maskï¼ˆå¿½ç•¥å¡«å……tokenï¼‰+ answer_maskï¼ˆåªå…³æ³¨å›ç­”éƒ¨åˆ†ï¼‰\n",
    "    \n",
    "    # pad_token_id å¯¹åº”çš„ç½®ä¸º 0 ï¼Œå…¶å®ƒç½®ä¸º 1 ã€‚\n",
    "    preferred_padding_mask = torch.where(\n",
    "        preferred_target_labels == pad_token_id,\n",
    "        0,\n",
    "        1\n",
    "    )\n",
    "    rejected_padding_mask = torch.where(\n",
    "        rejected_target_labels == pad_token_id,\n",
    "        0,\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    # åŠ©æ‰‹å›ç­”çš„æ©ç ï¼šå°†åŠ©æ‰‹å›ç­”çš„éƒ¨åˆ†æ©ç ä¸º 1 ã€‚å…¶å®ƒéƒ½æ˜¯ 0 ã€‚\n",
    "    preferred_answer_mask = create_answer_mask(\n",
    "        preferred_model_inputs,\n",
    "        tokenizer\n",
    "    )\n",
    "    rejected_answer_mask = create_answer_mask(\n",
    "        rejected_model_inputs,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    # æœ€ç»ˆæ©ç ï¼šå–äº¤é›†\n",
    "    preferred_final_mask = (preferred_answer_mask & preferred_padding_mask)\n",
    "    rejected_final_mask = (rejected_answer_mask & rejected_padding_mask)\n",
    "\n",
    "    ## ==================== æ‰¹æ¬¡æœ‰æ•ˆæ€§æ£€æŸ¥ ====================\n",
    "    \n",
    "    # æ£€æŸ¥åå¥½å¯¹æ•°æ®æ˜¯å¦éƒ½æœ‰æœ‰æ•ˆçš„å›ç­”éƒ¨åˆ†\n",
    "    preferred_min_tokens = preferred_final_mask.sum(dim=-1).min().item()\n",
    "    rejected_min_tokens = rejected_final_mask.sum(dim=-1).min().item()\n",
    "    \n",
    "    if preferred_min_tokens == 0 or rejected_min_tokens == 0:\n",
    "        print(f'âš ï¸ è·³è¿‡ç¬¬{batch_idx + 1}æ‰¹æ¬¡ï¼šåå¥½å¯¹æ•°æ®å›ç­”éƒ¨åˆ†ä¸è¶³')\n",
    "        skipped_batches_count += 1\n",
    "        continue  # è·³è¿‡å½“å‰æ‰¹æ¬¡\n",
    "\n",
    "    ## ==================== æ¨¡å‹å‰å‘ä¼ æ’­ ====================\n",
    "    \n",
    "    # è®­ç»ƒæ¨¡å‹å¯¹åå¥½æ•°æ®çš„å‰å‘ä¼ æ’­\n",
    "    preferred_logits = model(preferred_model_inputs).logits\n",
    "    torch.cuda.empty_cache()  # æ¸…ç†GPUæ˜¾å­˜\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    # è®­ç»ƒæ¨¡å‹å¯¹æ‹’ç»æ•°æ®çš„å‰å‘ä¼ æ’­\n",
    "    rejected_logits = model(rejected_model_inputs).logits\n",
    "    torch.cuda.empty_cache()  # æ¸…ç†GPUæ˜¾å­˜\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    # å‚è€ƒæ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰\n",
    "    with torch.no_grad():\n",
    "        reference_preferred_logits = ref_model(preferred_model_inputs) \\\n",
    "            .logits                                                    \\\n",
    "            .detach()\n",
    "        reference_rejected_logits = ref_model(rejected_model_inputs)   \\\n",
    "            .logits                                                    \\\n",
    "            .detach()\n",
    "\n",
    "    ## ==================== DPOæŸå¤±è®¡ç®— ====================\n",
    "    \"\"\"\n",
    "    DPO (Direct Preference Optimization) è®ºæ–‡: https://arxiv.org/pdf/2305.18290.pdf\n",
    "    æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡åå¥½å¯¹æ¯”å­¦ä¹ ï¼Œæ— éœ€æ˜¾å¼å¥–åŠ±æ¨¡å‹\n",
    "    \"\"\"\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡å¯¹æ•°æ¦‚ç‡ (average_log_prob = True)\n",
    "    # å‚è€ƒ: https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L924\n",
    "    \n",
    "    ### è®­ç»ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡\n",
    "    ### æ­£åœ¨å¾®è°ƒçš„æ¨¡å‹ï¼Œæ¥æ”¶åˆ°æ­£ä¾‹çš„logitsï¼Œè®¡ç®—å¯¹æ•°æ¦‚ç‡\n",
    "    preferred_log_prob = _compute_average_log_probability(\n",
    "        preferred_logits,\n",
    "        preferred_target_labels,\n",
    "        preferred_final_mask\n",
    "    )\n",
    "    rejected_log_prob = _compute_average_log_probability(\n",
    "        rejected_logits,\n",
    "        rejected_target_labels,\n",
    "        rejected_final_mask\n",
    "    )\n",
    "    \n",
    "    ### å‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡\n",
    "    reference_preferred_log_prob = _compute_average_log_probability(\n",
    "        reference_preferred_logits,\n",
    "        preferred_target_labels,\n",
    "        preferred_final_mask\n",
    "    )\n",
    "    reference_rejected_log_prob = _compute_average_log_probability(\n",
    "        reference_rejected_logits,\n",
    "        rejected_target_labels,\n",
    "        rejected_final_mask\n",
    "    )\n",
    "\n",
    "    ## ==================== å¥–åŠ±å’Œè¾¹é™…è®¡ç®— ====================\n",
    "    \n",
    "    # è®¡ç®—éšå¼å¥–åŠ± (åŸºäºKLæ•£åº¦)\n",
    "    preferred_implicit_reward =                              \\\n",
    "        beta *                                               \\\n",
    "        (preferred_log_prob - reference_preferred_log_prob)\n",
    "    rejected_implicit_reward =                               \\\n",
    "        beta *                                               \\\n",
    "        (rejected_log_prob - reference_rejected_log_prob)\n",
    "    \n",
    "    # è®¡ç®—å¥–åŠ±è¾¹é™… (åå¥½æ•°æ®åº”è¯¥æœ‰æ›´é«˜çš„å¥–åŠ±)\n",
    "    reward_margin = preferred_implicit_reward - rejected_implicit_reward\n",
    "    \n",
    "    # DPOæŸå¤±ï¼š-log(sigmoid(margin))\n",
    "    preference_probability = torch.nn.functional.sigmoid(reward_margin)\n",
    "    sample_losses = -torch.log(preference_probability)\n",
    "    \n",
    "    # æ‰¹æ¬¡å¹³å‡æŸå¤± + æ¢¯åº¦ç´¯ç§¯\n",
    "    batch_average_loss =                          \\\n",
    "        torch.nanmean(sample_losses) /            \\\n",
    "        gradient_accumulation_steps\n",
    "\n",
    "    ## ==================== åå‘ä¼ æ’­å’Œä¼˜åŒ– ====================\n",
    "    \n",
    "    batch_average_loss.backward()\n",
    "\n",
    "    # åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´\n",
    "    current_learning_rate = cosine_decay(\n",
    "        batch_idx,\n",
    "        warmup_steps,\n",
    "        total_steps,\n",
    "        max_lr,\n",
    "        min_lr\n",
    "    )\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_learning_rate\n",
    "\n",
    "    # æ¢¯åº¦ç´¯ç§¯å’Œæƒé‡æ›´æ–°\n",
    "    is_accumulation_step = (batch_idx + 1) % gradient_accumulation_steps == 0\n",
    "    is_final_batch = (batch_idx + 1) == total_batches\n",
    "    \n",
    "    if is_accumulation_step or is_final_batch:\n",
    "        optimizer.step()        # æ›´æ–°æƒé‡\n",
    "        optimizer.zero_grad()   # æ¸…ç©ºæ¢¯åº¦\n",
    "\n",
    "    ## ==================== è®­ç»ƒæŒ‡æ ‡è®°å½• ====================\n",
    "    \n",
    "    # è®°å½•å„é¡¹è®­ç»ƒæŒ‡æ ‡ï¼ˆdetaché¿å…æ¢¯åº¦è¿½è¸ªï¼‰\n",
    "    training_losses.append(\n",
    "        batch_average_loss.detach().item() * gradient_accumulation_steps)\n",
    "    preferred_log_probabilities.append(\n",
    "        torch.nanmean(preferred_log_prob.detach()).item())\n",
    "    rejected_log_probabilities.append(\n",
    "        torch.nanmean(rejected_log_prob.detach()).item())\n",
    "    preferred_rewards.append(\n",
    "        torch.nanmean(preferred_implicit_reward.detach()).item())\n",
    "    rejected_rewards.append(torch.nanmean(\n",
    "        rejected_implicit_reward.detach()).item())\n",
    "    reward_margins.append(\n",
    "        torch.nanmean(reward_margin.detach()).item())\n",
    "\n",
    "    ## ==================== è®­ç»ƒæ—¥å¿—è¾“å‡º ====================\n",
    "    \n",
    "    should_log = (batch_idx + 1) % log_iter == 0 or is_final_batch\n",
    "    \n",
    "    if should_log:\n",
    "        # è®¡ç®—æœ€è¿‘æ‰¹æ¬¡çš„å¹³å‡æŒ‡æ ‡\n",
    "        recent_loss = np.nanmean(training_losses[-log_iter:])\n",
    "        recent_preferred_logprob = np.nanmean(\n",
    "            preferred_log_probabilities[-log_iter:])\n",
    "        recent_rejected_logprob = np.nanmean(\n",
    "            rejected_log_probabilities[-log_iter:])\n",
    "        recent_preferred_reward = np.nanmean(preferred_rewards[-log_iter:])\n",
    "        recent_rejected_reward = np.nanmean(rejected_rewards[-log_iter:])\n",
    "        recent_margin = np.nanmean(reward_margins[-log_iter:])\n",
    "        \n",
    "        # æ ¼å¼åŒ–è¾“å‡ºè®­ç»ƒçŠ¶æ€\n",
    "        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f'â° æ—¶é—´: {current_time}')\n",
    "        print(f'ğŸ“Š æ‰¹æ¬¡: {batch_idx + 1}/{total_batches}')\n",
    "        print(f'ğŸ“ˆ æœ€è¿‘{log_iter}æ‰¹æ¬¡æŒ‡æ ‡:')\n",
    "        print(f'   - å¹³å‡æŸå¤±: {recent_loss:.4f}')\n",
    "        print(f'   - åå¥½å¯¹æ•°æ¦‚ç‡: {recent_preferred_logprob:.4f}')\n",
    "        print(f'   - æ‹’ç»å¯¹æ•°æ¦‚ç‡: {recent_rejected_logprob:.4f}')\n",
    "        print(f'   - åå¥½å¥–åŠ±: {recent_preferred_reward:.4f}')\n",
    "        print(f'   - æ‹’ç»å¥–åŠ±: {recent_rejected_reward:.4f}')\n",
    "        print(f'   - å¥–åŠ±è¾¹é™…: {recent_margin:.4f}')\n",
    "        print(f'ğŸ¯ å­¦ä¹ ç‡: {current_learning_rate:.2e}')\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # è°ƒç”¨å¤–éƒ¨æ—¥å¿—è®°å½•\n",
    "        log_call(batch_idx, recent_loss)\n",
    "\n",
    "## ==================== è®­ç»ƒå®Œæˆæ€»ç»“ ====================\n",
    "\n",
    "print(\"ğŸ‰ DPOè®­ç»ƒå®Œæˆ!\")\n",
    "print(f'ğŸ“Š è®­ç»ƒç»Ÿè®¡:')\n",
    "print(f'   - æ€»æ‰¹æ¬¡æ•°: {total_batches}')\n",
    "print(f'   - è·³è¿‡æ‰¹æ¬¡æ•°: {skipped_batches_count}')\n",
    "print(f'   - æœ‰æ•ˆæ‰¹æ¬¡æ•°: {total_batches - skipped_batches_count}')\n",
    "\n",
    "# è¾“å‡ºæœ€ç»ˆè®­ç»ƒæŒ‡æ ‡\n",
    "if training_losses:\n",
    "    final_metrics = {\n",
    "        'loss': np.nanmean(training_losses[-100:]),\n",
    "        'preferred_logprob': np.nanmean(preferred_log_probabilities[-100:]),\n",
    "        'rejected_logprob': np.nanmean(rejected_log_probabilities[-100:]),\n",
    "        'preferred_reward': np.nanmean(preferred_rewards[-100:]),\n",
    "        'rejected_reward': np.nanmean(rejected_rewards[-100:]),\n",
    "        'margin': np.nanmean(reward_margins[-100:])\n",
    "    }\n",
    "    \n",
    "    print(f'ğŸ¯ æœ€ç»ˆæŒ‡æ ‡ (æœ€è¿‘100æ‰¹æ¬¡å¹³å‡):')\n",
    "    for metric_name, metric_value in final_metrics.items():\n",
    "        print(f'   - {metric_name}: {metric_value:.4f}')\n",
    "\n",
    "if skipped_batches_count > 0:\n",
    "    skip_ratio = skipped_batches_count / total_batches * 100\n",
    "    print(f'âš ï¸ è·³è¿‡æ‰¹æ¬¡å æ¯”: {skip_ratio:.2f}%')\n",
    "    if skip_ratio > 10:\n",
    "        print('ğŸ’¡ å»ºè®®: è·³è¿‡æ‰¹æ¬¡è¿‡å¤šï¼Œè€ƒè™‘å¢åŠ æœ€å¤§åºåˆ—é•¿åº¦æˆ–ä¼˜åŒ–æ•°æ®é¢„å¤„ç†')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./Qwen2.5-0.5B-DPO')\n",
    "tokenizer.save_pretrained('./Qwen2.5-0.5B-DPO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
