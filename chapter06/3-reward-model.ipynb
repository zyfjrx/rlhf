{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-28T06:23:53.987293Z",
     "start_time": "2025-09-28T06:23:53.919255Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_path = '/Users/zhangyf/llm/gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:24:31.000201Z",
     "start_time": "2025-09-28T06:24:30.983561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path = './sst2'\n",
    "dataset = load_dataset(dataset_path)\n",
    "ds_train, ds_val = dataset['train'], dataset['validation']\n",
    "print(ds_train[:3])"
   ],
   "id": "86d5ee06699a6025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': [0, 1, 2], 'sentence': ['hide new secretions from the parental units ', 'contains no wit , only labored gags ', 'that loves its characters and communicates something rather beautiful about human nature '], 'label': [0, 0, 1]}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:26:16.645660Z",
     "start_time": "2025-09-28T06:26:16.601391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义一个新的token，奖励token eos\n",
    "REWARD_TOKEN_ID = tokenizer.eos_token_id\n",
    "print(REWARD_TOKEN_ID)\n",
    "\n",
    "def tokenize(batch):\n",
    "    # 提取出文本内容\n",
    "    outputs = tokenizer(batch['sentence'])\n",
    "    # 每条数据一个评分，初始化为 0 。\n",
    "    outputs['score'] = [0] * len(outputs['input_ids'])\n",
    "    # 对每条数据的最后的reward token进行评分\n",
    "    outputs['score_index'] = [0] * len(outputs['input_ids'])\n",
    "    for i in range(len(outputs['input_ids'])):\n",
    "        # 第 i 条数据的末尾添加一个 eos token，作为reward token\n",
    "        outputs['input_ids'][i].append(REWARD_TOKEN_ID)\n",
    "        # reward token的掩码设置为 1 。\n",
    "        outputs['attention_mask'][i].append(1)\n",
    "        # 正向情感的文本评分为 1 。负向情感的评分为 0 。\n",
    "        outputs['score'][i] = float(batch['label'][i])\n",
    "        # 对 reward token 进行评分，也就是评分的索引为 reward token 的索引。\n",
    "        outputs['score_index'][i] = len(outputs['input_ids'][i]) - 1\n",
    "    return outputs\n",
    "\n",
    "map_kwargs = {\n",
    "    \"batched\": True,\n",
    "    \"batch_size\": 512,\n",
    "    \"remove_columns\": ['idx', 'sentence', 'label']\n",
    "}\n",
    "\n",
    "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
    "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)\n",
    "\n",
    "print(tokenized_dataset_train[4])"
   ],
   "id": "c8b978b05ae64d62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n",
      "{'input_ids': [261, 262, 5290, 15827, 12, 1659, 12, 1169, 12, 1008, 9310, 35478, 20954, 262, 28303, 714, 47478, 469, 510, 220, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'score': 0.0, 'score_index': 20}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:26:39.387056Z",
     "start_time": "2025-09-28T06:26:39.383449Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenized_dataset_train[8])",
   "id": "a0676df8691845fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [64, 19095, 17280, 12, 1941, 12, 727, 705, 82, 26781, 19518, 220, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'score': 0.0, 'score_index': 12}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:28:35.836211Z",
     "start_time": "2025-09-28T06:28:35.829451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_dataset_train.set_format(type='torch')\n",
    "tokenized_dataset_val.set_format(type='torch')\n",
    "\n",
    "print(tokenized_dataset_train[8])"
   ],
   "id": "5fdfcf4653319a24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([   64, 19095, 17280,    12,  1941,    12,   727,   705,    82, 26781,\n",
      "        19518,   220, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'score': tensor(0.), 'score_index': tensor(12)}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:29:12.958471Z",
     "start_time": "2025-09-28T06:29:12.946947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 删除长度小于6的样本\n",
    "tokenized_dataset_train = tokenized_dataset_train.filter(lambda x: len(x['input_ids']) > 6)\n",
    "tokenized_dataset_val = tokenized_dataset_val.filter(lambda x: len(x['input_ids']) > 6)\n",
    "\n",
    "print(len(tokenized_dataset_train))"
   ],
   "id": "34a95705b3b29369",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49401\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:34:57.217709Z",
     "start_time": "2025-09-28T06:34:57.202346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "class RewardHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # llm最后输出的隐藏层的维度\n",
    "        self.hidden_size = config.hidden_size\n",
    "        # 线性层用来对llm最后输出的隐藏层给奖励\n",
    "        self.reward = nn.Linear(self.hidden_size, 1)\n",
    "        self._post_init()\n",
    "\n",
    "    def _post_init(self):\n",
    "        # 使用正态分布初始化权重\n",
    "        nn.init.normal_(\n",
    "            self.reward.weight,\n",
    "            std=(1.0 / np.sqrt(self.hidden_size + 1))\n",
    "        )\n",
    "        # 将偏置初始化为0\n",
    "        nn.init.zeros_(self.reward.bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 给出奖励\n",
    "        return self.reward(hidden_states)\n",
    "\n",
    "class GPT2RewardHead(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.reward_head = RewardHead(self.llm.config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_outputs = self.llm.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        last_hidden_state = transformer_outputs.hidden_states[-1]\n",
    "        # 给出奖励\n",
    "        reward = self.reward_head(last_hidden_state).squeeze(-1)\n",
    "        # sigmoid用来将奖励搞到(0,1)范围内\n",
    "        return torch.sigmoid(reward)"
   ],
   "id": "c702c1d97ce8fe67",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:35:00.694995Z",
     "start_time": "2025-09-28T06:35:00.600893Z"
    }
   },
   "cell_type": "code",
   "source": "model = GPT2RewardHead(model_path)",
   "id": "ce237f43f042dd5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:35:03.688681Z",
     "start_time": "2025-09-28T06:35:03.678653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "# 还是将 eos token 作为 pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "dataloader_params = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    'collate_fn': data_collator\n",
    "}\n",
    "train_dataloader = DataLoader(tokenized_dataset_train, **dataloader_params)\n",
    "val_dataloader = DataLoader(tokenized_dataset_val, **dataloader_params)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "# print(batch.keys())\n",
    "\n",
    "print(batch['input_ids'][1])\n",
    "print(batch['attention_mask'][1])\n",
    "print(batch['score'][1])\n",
    "print(batch['score_index'][1])\n",
    "print(tokenizer.decode(batch['input_ids'][1]))\n",
    "print(batch['attention_mask'][1].nonzero()[-1])"
   ],
   "id": "1622b444d656d1d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  361,  1801,   261,   290,  1527, 40667,  2230,  1194,  1628,  4077,\n",
      "         2971,   837,  1306,   640,   503,   484,  1244,  1949,  5989,  1342,\n",
      "         3241,   284,   262,   949,  5847,   444,   290,   517,  3241,   284,\n",
      "          262,  2646,   340,   318,   546,   764,   220, 50256])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor(0.)\n",
      "tensor(37)\n",
      "if damon and affleck attempt another project greenlight , next time out they might try paying less attention to the miniseries and more attention to the film it is about . <|endoftext|>\n",
      "tensor([37])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:35:07.910838Z",
     "start_time": "2025-09-28T06:35:06.882509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "print(outputs.shape)"
   ],
   "id": "93126bcfd2d2403e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 38])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:35:16.585831Z",
     "start_time": "2025-09-28T06:35:16.564101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('mps') if torch.mps.is_available() else torch.device('cpu')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "# 二分类交叉熵损失\n",
    "criterion = nn.BCELoss()\n",
    "num_epochs = 1 # N+ Implementation Detail paper"
   ],
   "id": "1eb7823ba9afbea1",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:36:00.632753Z",
     "start_time": "2025-09-28T06:36:00.628705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        inputs = batch.to(device)\n",
    "        model_inputs = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask']\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            # 对输出进行评分\n",
    "            scores = model(**model_inputs)\n",
    "            # 批次中每条数据的索引\n",
    "            batch_indices = torch.arange(scores.shape[0])\n",
    "            # 根据索引拿出评分，也就是reward token的评分\n",
    "            score = scores[batch_indices, inputs['score_index']]\n",
    "            # 目标评分，0 或者 1 。\n",
    "            target = inputs['score']\n",
    "            # 计算误差\n",
    "            loss = criterion(score, target)\n",
    "        total_loss += loss.item()\n",
    "    print('validation loss:', total_loss / len(val_dataloader))"
   ],
   "id": "4b6e415b93cab182",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:40:47.703809Z",
     "start_time": "2025-09-28T06:36:51.361664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "\n",
    "validate()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        inputs = batch.to(device)\n",
    "        model_inputs = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask']\n",
    "        }\n",
    "        scores = model(**model_inputs)\n",
    "        batch_indices = torch.arange(scores.shape[0])\n",
    "        score = scores[batch_indices, inputs['score_index']]\n",
    "        target = inputs['score']\n",
    "        loss = criterion(score, target)\n",
    "        # 三部曲：清空梯度 ⟶ 反向传播计算梯度 ⟶ 更新参数\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"损失:\",loss.item())\n",
    "    validate()"
   ],
   "id": "e33c0299eda0ae3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.7581602888447896\n",
      "3.108236789703369\n",
      "0.8091529607772827\n",
      "1.4204561710357666\n",
      "1.0424749851226807\n",
      "0.8312817811965942\n",
      "0.7022022604942322\n",
      "0.8008676767349243\n",
      "0.9361941814422607\n",
      "1.125940203666687\n",
      "0.9442117214202881\n",
      "0.7852380275726318\n",
      "0.6829521059989929\n",
      "0.598456621170044\n",
      "0.7150830030441284\n",
      "0.6870534420013428\n",
      "0.7926227450370789\n",
      "0.9951375126838684\n",
      "0.8767225742340088\n",
      "0.9890028834342957\n",
      "0.7926579713821411\n",
      "0.7238250970840454\n",
      "0.7270512580871582\n",
      "0.6529476642608643\n",
      "0.7587059736251831\n",
      "0.759275496006012\n",
      "0.7556922435760498\n",
      "0.7033265829086304\n",
      "0.6928670406341553\n",
      "0.7024922370910645\n",
      "0.8076363205909729\n",
      "0.7934281826019287\n",
      "0.766585111618042\n",
      "0.7283234596252441\n",
      "0.7111377716064453\n",
      "0.6854632496833801\n",
      "0.701343297958374\n",
      "0.6488975286483765\n",
      "0.6917641162872314\n",
      "0.7289260625839233\n",
      "0.7580981254577637\n",
      "0.7184112071990967\n",
      "0.6983562707901001\n",
      "0.7334611415863037\n",
      "0.6928342580795288\n",
      "0.7089872360229492\n",
      "0.6673755049705505\n",
      "0.6703251600265503\n",
      "0.6563602685928345\n",
      "0.6812747716903687\n",
      "0.6613168716430664\n",
      "0.6867802143096924\n",
      "0.6867527365684509\n",
      "0.6452300548553467\n",
      "0.6725062131881714\n",
      "0.6526024341583252\n",
      "0.6717891097068787\n",
      "0.6567402482032776\n",
      "0.6134257316589355\n",
      "0.6600704193115234\n",
      "0.6315048336982727\n",
      "0.6254419684410095\n",
      "0.6409745216369629\n",
      "0.6187453269958496\n",
      "0.5573406219482422\n",
      "0.5929728150367737\n",
      "0.5486646890640259\n",
      "0.5959082841873169\n",
      "0.5177934765815735\n",
      "0.5227640867233276\n",
      "0.5647643208503723\n",
      "0.5947468280792236\n",
      "0.4061093330383301\n",
      "0.5796020030975342\n",
      "0.49806252121925354\n",
      "0.4749894142150879\n",
      "0.4656188488006592\n",
      "0.3986417055130005\n",
      "0.2813354432582855\n",
      "0.46652913093566895\n",
      "0.5822405815124512\n",
      "0.44686365127563477\n",
      "0.655552089214325\n",
      "0.47031542658805847\n",
      "0.6602475047111511\n",
      "0.3997955918312073\n",
      "0.3746967613697052\n",
      "0.429728239774704\n",
      "0.4278686046600342\n",
      "0.4658830165863037\n",
      "0.42192724347114563\n",
      "0.6135045289993286\n",
      "0.522773027420044\n",
      "0.31089895963668823\n",
      "0.3241247832775116\n",
      "0.5210421085357666\n",
      "0.2769077718257904\n",
      "0.48309504985809326\n",
      "0.3504457473754883\n",
      "0.3656670153141022\n",
      "0.2959407567977905\n",
      "0.27558737993240356\n",
      "0.24610012769699097\n",
      "0.41744786500930786\n",
      "0.3715578317642212\n",
      "0.23483648896217346\n",
      "0.5960434675216675\n",
      "0.6440054178237915\n",
      "0.38961276412010193\n",
      "0.37822455167770386\n",
      "0.5045726299285889\n",
      "0.5397007465362549\n",
      "0.2514422535896301\n",
      "0.41349437832832336\n",
      "0.6795041561126709\n",
      "0.5471453666687012\n",
      "0.48007631301879883\n",
      "0.3814713954925537\n",
      "0.3534432053565979\n",
      "0.4042700231075287\n",
      "0.41277003288269043\n",
      "0.44795966148376465\n",
      "0.6572693586349487\n",
      "0.5380103588104248\n",
      "0.41345536708831787\n",
      "0.4113689661026001\n",
      "0.5400834083557129\n",
      "0.40783363580703735\n",
      "0.47795581817626953\n",
      "0.2855437994003296\n",
      "0.3695898652076721\n",
      "0.6468811631202698\n",
      "0.35779035091400146\n",
      "0.22003313899040222\n",
      "0.38364696502685547\n",
      "0.3825983703136444\n",
      "0.25963979959487915\n",
      "0.3198291063308716\n",
      "0.38300204277038574\n",
      "0.19288715720176697\n",
      "0.3752676844596863\n",
      "0.22238725423812866\n",
      "0.5225936770439148\n",
      "0.2889716625213623\n",
      "0.5840152502059937\n",
      "0.4204069972038269\n",
      "0.25036951899528503\n",
      "0.11707158386707306\n",
      "0.4206582009792328\n",
      "0.37197476625442505\n",
      "0.2817431092262268\n",
      "0.24836783111095428\n",
      "0.44877681136131287\n",
      "0.2556050717830658\n",
      "0.3140636682510376\n",
      "0.4669516086578369\n",
      "0.4637940526008606\n",
      "0.10802842676639557\n",
      "0.2772458791732788\n",
      "0.5520627498626709\n",
      "0.5019562840461731\n",
      "0.3994579315185547\n",
      "0.6051722168922424\n",
      "0.5257253646850586\n",
      "0.4372192621231079\n",
      "0.3860033452510834\n",
      "0.5187420845031738\n",
      "0.5978010892868042\n",
      "0.461281418800354\n",
      "0.3827367126941681\n",
      "0.5017458200454712\n",
      "0.5321444272994995\n",
      "0.34798315167427063\n",
      "0.391764760017395\n",
      "0.5689279437065125\n",
      "0.3543311357498169\n",
      "0.5251829028129578\n",
      "0.2552330493927002\n",
      "0.42116910219192505\n",
      "0.24182094633579254\n",
      "0.2366994023323059\n",
      "0.6091840863227844\n",
      "0.38035306334495544\n",
      "0.4022308588027954\n",
      "0.44042012095451355\n",
      "0.3005046844482422\n",
      "0.3151327073574066\n",
      "0.446432888507843\n",
      "0.18591606616973877\n",
      "0.2728877663612366\n",
      "0.3704743981361389\n",
      "0.26985400915145874\n",
      "0.29706770181655884\n",
      "0.30181652307510376\n",
      "0.3874700367450714\n",
      "0.4708039164543152\n",
      "0.20822295546531677\n",
      "0.29823312163352966\n",
      "0.4408659338951111\n",
      "0.4904330372810364\n",
      "0.23809757828712463\n",
      "0.5158818960189819\n",
      "0.6225166320800781\n",
      "0.25634974241256714\n",
      "0.4055562913417816\n",
      "0.49340784549713135\n",
      "0.40331771969795227\n",
      "0.4582865834236145\n",
      "0.24995560944080353\n",
      "0.2962522804737091\n",
      "0.3821485638618469\n",
      "0.3708657920360565\n",
      "0.3372724652290344\n",
      "0.49986010789871216\n",
      "0.4133915901184082\n",
      "0.2816590368747711\n",
      "0.41156280040740967\n",
      "0.36281895637512207\n",
      "0.23521077632904053\n",
      "0.2809327244758606\n",
      "0.3507729768753052\n",
      "0.5634139776229858\n",
      "0.41594332456588745\n",
      "0.35733383893966675\n",
      "0.41846293210983276\n",
      "0.31219351291656494\n",
      "0.3011225461959839\n",
      "0.38725745677948\n",
      "0.31020277738571167\n",
      "0.2920449376106262\n",
      "0.31704795360565186\n",
      "0.2809451222419739\n",
      "0.4622862637042999\n",
      "0.33497729897499084\n",
      "0.31321313977241516\n",
      "0.3058309257030487\n",
      "0.15793418884277344\n",
      "0.27273431420326233\n",
      "0.36241570115089417\n",
      "0.28910183906555176\n",
      "0.2453235685825348\n",
      "0.43372899293899536\n",
      "0.3062504529953003\n",
      "0.5349738597869873\n",
      "0.1844380497932434\n",
      "0.27540093660354614\n",
      "0.22474826872348785\n",
      "0.2237241566181183\n",
      "0.3045307993888855\n",
      "0.3216264247894287\n",
      "0.4639790654182434\n",
      "0.335579514503479\n",
      "0.3400065302848816\n",
      "0.35407277941703796\n",
      "0.24253802001476288\n",
      "0.2697286307811737\n",
      "0.2180352360010147\n",
      "0.2614305019378662\n",
      "0.3516084551811218\n",
      "0.25380945205688477\n",
      "0.2117612659931183\n",
      "0.2863386273384094\n",
      "0.1793096661567688\n",
      "0.2615658640861511\n",
      "0.3973471522331238\n",
      "0.6014024615287781\n",
      "0.3528449237346649\n",
      "0.2980670928955078\n",
      "0.29886379837989807\n",
      "0.4113021194934845\n",
      "0.41362589597702026\n",
      "0.3510088324546814\n",
      "0.32747727632522583\n",
      "0.2820098400115967\n",
      "0.2472693920135498\n",
      "0.2769390344619751\n",
      "0.31587666273117065\n",
      "0.20255175232887268\n",
      "0.25218257308006287\n",
      "0.2511630356311798\n",
      "0.25838249921798706\n",
      "0.5283972024917603\n",
      "0.35708796977996826\n",
      "0.2022843062877655\n",
      "0.30732160806655884\n",
      "0.41351914405822754\n",
      "0.30920517444610596\n",
      "0.22713135182857513\n",
      "0.2331545352935791\n",
      "0.18223074078559875\n",
      "0.2380901277065277\n",
      "0.18582963943481445\n",
      "0.24537888169288635\n",
      "0.3054279386997223\n",
      "0.4489556849002838\n",
      "0.37962889671325684\n",
      "0.3278394341468811\n",
      "0.12359195202589035\n",
      "0.2447752058506012\n",
      "0.15592651069164276\n",
      "0.3813263773918152\n",
      "0.4790305495262146\n",
      "0.3851742744445801\n",
      "0.2703772783279419\n",
      "0.20149987936019897\n",
      "0.35840094089508057\n",
      "0.47534802556037903\n",
      "0.5827425122261047\n",
      "0.27211785316467285\n",
      "0.3630480170249939\n",
      "0.27081215381622314\n",
      "0.3760163187980652\n",
      "0.2759312391281128\n",
      "0.2925873100757599\n",
      "0.3736892342567444\n",
      "0.14987514913082123\n",
      "0.2759433686733246\n",
      "0.5314960479736328\n",
      "0.4490432143211365\n",
      "0.3176030218601227\n",
      "0.3504076898097992\n",
      "0.26543623208999634\n",
      "0.3236827850341797\n",
      "0.2869214713573456\n",
      "0.2551333010196686\n",
      "0.42078185081481934\n",
      "0.35438093543052673\n",
      "0.25188320875167847\n",
      "0.38410377502441406\n",
      "0.15896162390708923\n",
      "0.18054185807704926\n",
      "0.2630894184112549\n",
      "0.19097435474395752\n",
      "0.515727698802948\n",
      "0.33521348237991333\n",
      "0.21041296422481537\n",
      "0.5770528316497803\n",
      "0.2321375161409378\n",
      "0.22511577606201172\n",
      "0.23910018801689148\n",
      "0.2204534411430359\n",
      "0.21775148808956146\n",
      "0.16900119185447693\n",
      "0.1865243762731552\n",
      "0.40352779626846313\n",
      "0.3129541873931885\n",
      "0.2990444004535675\n",
      "0.2389325350522995\n",
      "0.2828650176525116\n",
      "0.44657593965530396\n",
      "0.37225472927093506\n",
      "0.39507365226745605\n",
      "0.25511297583580017\n",
      "0.2604454457759857\n",
      "0.21327364444732666\n",
      "0.22170120477676392\n",
      "0.45443961024284363\n",
      "0.6432090997695923\n",
      "0.2375027984380722\n",
      "0.49550217390060425\n",
      "0.29392510652542114\n",
      "0.21957087516784668\n",
      "0.37820249795913696\n",
      "0.19613638520240784\n",
      "0.39258134365081787\n",
      "0.26422029733657837\n",
      "0.2676493227481842\n",
      "0.36631909012794495\n",
      "0.2590336799621582\n",
      "0.34780630469322205\n",
      "0.2723599672317505\n",
      "0.21451789140701294\n",
      "0.3043820858001709\n",
      "0.3017258048057556\n",
      "0.25996133685112\n",
      "0.22245268523693085\n",
      "0.11984962224960327\n",
      "0.21251149475574493\n",
      "0.1341688334941864\n",
      "0.2101779282093048\n",
      "0.05561259388923645\n",
      "0.34463441371917725\n",
      "0.38896697759628296\n",
      "0.4775804579257965\n",
      "0.3273394703865051\n",
      "0.1662610024213791\n",
      "0.3545191287994385\n",
      "0.26519134640693665\n",
      "0.3138323426246643\n",
      "0.18740704655647278\n",
      "0.16864675283432007\n",
      "0.2481430470943451\n",
      "0.1639472246170044\n",
      "0.2547389268875122\n",
      "0.31720590591430664\n",
      "0.35926517844200134\n",
      "0.19012440741062164\n",
      "0.44796133041381836\n",
      "0.42873966693878174\n",
      "0.1651880145072937\n",
      "0.18182840943336487\n",
      "0.1677950620651245\n",
      "0.1525786817073822\n",
      "0.22098669409751892\n",
      "0.4954872727394104\n",
      "0.4571515917778015\n",
      "0.6011847853660583\n",
      "0.18170398473739624\n",
      "0.238606259226799\n",
      "0.2710345983505249\n",
      "0.29964709281921387\n",
      "0.24999703466892242\n",
      "0.3862997889518738\n",
      "0.33879947662353516\n",
      "0.2950597405433655\n",
      "0.39120718836784363\n",
      "0.342019259929657\n",
      "0.30821463465690613\n",
      "0.3881932199001312\n",
      "0.21323338150978088\n",
      "0.10383382439613342\n",
      "0.25643646717071533\n",
      "0.3596801161766052\n",
      "0.19511425495147705\n",
      "0.3102415204048157\n",
      "0.636106550693512\n",
      "0.27554887533187866\n",
      "0.385503351688385\n",
      "0.24695761501789093\n",
      "0.4072518050670624\n",
      "0.2503819763660431\n",
      "0.19716235995292664\n",
      "0.4020729660987854\n",
      "0.17295396327972412\n",
      "0.370414674282074\n",
      "0.19286614656448364\n",
      "0.24624720215797424\n",
      "0.3186437487602234\n",
      "0.3444962501525879\n",
      "0.31649982929229736\n",
      "0.3320774734020233\n",
      "0.30613020062446594\n",
      "0.27375784516334534\n",
      "0.16730493307113647\n",
      "0.2959209382534027\n",
      "0.4354274272918701\n",
      "0.34505128860473633\n",
      "0.26092755794525146\n",
      "0.3794613182544708\n",
      "0.21521197259426117\n",
      "0.3157252371311188\n",
      "0.18755502998828888\n",
      "0.23366688191890717\n",
      "0.26559877395629883\n",
      "0.39720720052719116\n",
      "0.1781158447265625\n",
      "0.23316150903701782\n",
      "0.19159752130508423\n",
      "0.16225138306617737\n",
      "0.24494150280952454\n",
      "0.3032582998275757\n",
      "0.20870286226272583\n",
      "0.19139230251312256\n",
      "0.2536828815937042\n",
      "0.44329720735549927\n",
      "0.48394709825515747\n",
      "0.4071289896965027\n",
      "0.17319950461387634\n",
      "0.21320033073425293\n",
      "0.3002787232398987\n",
      "0.33758074045181274\n",
      "0.30540695786476135\n",
      "0.4262933135032654\n",
      "0.40898120403289795\n",
      "0.4695282578468323\n",
      "0.12268289923667908\n",
      "0.3594970107078552\n",
      "0.19368909299373627\n",
      "0.3195574879646301\n",
      "0.37489616870880127\n",
      "0.2945442795753479\n",
      "0.22813528776168823\n",
      "0.2267863005399704\n",
      "0.2586493492126465\n",
      "0.300051748752594\n",
      "0.41852623224258423\n",
      "0.19766436517238617\n",
      "0.23699510097503662\n",
      "0.6495364904403687\n",
      "0.34843209385871887\n",
      "0.28694963455200195\n",
      "0.22254621982574463\n",
      "0.21391062438488007\n",
      "0.24550789594650269\n",
      "0.19867941737174988\n",
      "0.25617510080337524\n",
      "0.15010333061218262\n",
      "0.2853987216949463\n",
      "0.3210153579711914\n",
      "0.17015954852104187\n",
      "0.37185585498809814\n",
      "0.42471277713775635\n",
      "0.3029636740684509\n",
      "0.2635675072669983\n",
      "0.22032862901687622\n",
      "0.199429452419281\n",
      "0.16315120458602905\n",
      "0.5574342012405396\n",
      "0.19912846386432648\n",
      "0.2994825839996338\n",
      "0.39230090379714966\n",
      "0.23135003447532654\n",
      "0.19577370584011078\n",
      "0.40495219826698303\n",
      "0.22700080275535583\n",
      "0.2627139389514923\n",
      "0.24491414427757263\n",
      "0.3133343458175659\n",
      "0.19419974088668823\n",
      "0.2784825563430786\n",
      "0.18464434146881104\n",
      "0.20255833864212036\n",
      "0.21787029504776\n",
      "0.21024560928344727\n",
      "0.18208245933055878\n",
      "0.11273648589849472\n",
      "0.2079755663871765\n",
      "0.2694251537322998\n",
      "0.40860262513160706\n",
      "0.39444366097450256\n",
      "0.18258126080036163\n",
      "0.3366713523864746\n",
      "0.35943201184272766\n",
      "0.59174644947052\n",
      "0.4257759153842926\n",
      "0.329511433839798\n",
      "0.14247608184814453\n",
      "0.2763027250766754\n",
      "0.1610437035560608\n",
      "0.2254161387681961\n",
      "0.3158387839794159\n",
      "0.3303951025009155\n",
      "0.17639267444610596\n",
      "0.40598395466804504\n",
      "0.1879730522632599\n",
      "0.23553906381130219\n",
      "0.22015956044197083\n",
      "0.13519883155822754\n",
      "0.1682857871055603\n",
      "0.32725852727890015\n",
      "0.33956801891326904\n",
      "0.2809733748435974\n",
      "0.23267489671707153\n",
      "0.351318895816803\n",
      "0.2909889817237854\n",
      "0.12399940937757492\n",
      "0.21683283150196075\n",
      "0.13701879978179932\n",
      "0.2801399230957031\n",
      "0.23950356245040894\n",
      "0.22192582488059998\n",
      "0.3790889382362366\n",
      "0.32366886734962463\n",
      "0.16140905022621155\n",
      "0.3227493166923523\n",
      "0.1909254491329193\n",
      "0.14975784718990326\n",
      "0.2188941240310669\n",
      "0.09749732911586761\n",
      "0.31066128611564636\n",
      "0.3424313962459564\n",
      "0.3314182758331299\n",
      "0.3535413146018982\n",
      "0.38755369186401367\n",
      "0.3300648331642151\n",
      "0.30930784344673157\n",
      "0.30549493432044983\n",
      "0.29507875442504883\n",
      "0.2537916302680969\n",
      "0.21236379444599152\n",
      "0.16784104704856873\n",
      "0.3526008427143097\n",
      "0.13836351037025452\n",
      "0.29762306809425354\n",
      "0.12187980860471725\n",
      "0.1805875450372696\n",
      "0.2514917552471161\n",
      "0.2901167869567871\n",
      "0.37920257449150085\n",
      "0.125592440366745\n",
      "0.2826078534126282\n",
      "0.4168224632740021\n",
      "0.38412052392959595\n",
      "0.07349969446659088\n",
      "0.2882081866264343\n",
      "0.15386731922626495\n",
      "0.1913902461528778\n",
      "0.3573508560657501\n",
      "0.26664090156555176\n",
      "0.3673819303512573\n",
      "0.11312876641750336\n",
      "0.14134664833545685\n",
      "0.14199334383010864\n",
      "0.1496199369430542\n",
      "0.32818809151649475\n",
      "0.4463261067867279\n",
      "0.19665998220443726\n",
      "0.17475542426109314\n",
      "0.14892932772636414\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     19\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 20\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28mprint\u001B[39m(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     22\u001B[0m validate()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/optim/optimizer.py:516\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    511\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    512\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    513\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    514\u001B[0m             )\n\u001B[0;32m--> 516\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    519\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/optim/optimizer.py:81\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     79\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     80\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 81\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     83\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/optim/adam.py:247\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    235\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    237\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    238\u001B[0m         group,\n\u001B[1;32m    239\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    244\u001B[0m         state_steps,\n\u001B[1;32m    245\u001B[0m     )\n\u001B[0;32m--> 247\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdecoupled_weight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/optim/optimizer.py:149\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/optim/adam.py:949\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    946\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    947\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 949\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    950\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    951\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    952\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    953\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    954\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    955\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    956\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    957\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    958\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    959\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    967\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    968\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    969\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/optim/adam.py:464\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001B[0m\n\u001B[1;32m    462\u001B[0m         exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 464\u001B[0m     \u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddcmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n\u001B[1;32m    467\u001B[0m     step \u001B[38;5;241m=\u001B[39m step_t\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T05:28:15.930157Z",
     "start_time": "2025-09-25T05:28:14.922579Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'reward_model.pt')",
   "id": "2905cbd5291fa703",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T05:28:35.735102Z",
     "start_time": "2025-09-25T05:28:29.716222Z"
    }
   },
   "cell_type": "code",
   "source": "validate()",
   "id": "8753698c85af8bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.3069670636136185\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:41:36.992358Z",
     "start_time": "2025-09-28T06:41:33.270414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for i, batch in enumerate(val_dataloader):\n",
    "    inputs = batch.to(device)\n",
    "    model_inputs = {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask']\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        scores = model(**model_inputs)\n",
    "        batch_indices = torch.arange(scores.shape[0])\n",
    "        score = scores[batch_indices, inputs['score_index']]\n",
    "        target = inputs['score']\n",
    "    predictions = (score > 0.5).int()\n",
    "\n",
    "    all_predictions.extend(predictions.cpu().numpy())\n",
    "    all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "confusion_matrix(all_labels, all_predictions)"
   ],
   "id": "13fa3d9c4f85bb71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[364,  60],\n",
       "       [ 43, 400]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "395b2eb7e199cb2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
