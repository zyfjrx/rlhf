{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:24.191176Z",
     "start_time": "2025-09-25T02:14:24.102376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = '/Users/zhangyf/llm/gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "84dc0e1435b2c54f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:25.953375Z",
     "start_time": "2025-09-25T02:14:25.932888Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path = './sst2'\n",
    "ds = load_dataset(dataset_path)\n",
    "ds_train, ds_val = ds['train'], ds['validation']\n",
    "\n",
    "print(ds)\n",
    "print(ds_train)\n",
    "print(ds_train[6])\n",
    "print(ds_train[:10])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['idx', 'sentence', 'label'],\n",
      "    num_rows: 67349\n",
      "})\n",
      "{'idx': 6, 'sentence': 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ', 'label': 1}\n",
      "{'idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'sentence': ['hide new secretions from the parental units ', 'contains no wit , only labored gags ', 'that loves its characters and communicates something rather beautiful about human nature ', 'remains utterly satisfied to remain the same throughout ', 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', \"that 's far too tragic to merit such superficial treatment \", 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ', 'of saucy ', \"a depressed fifteen-year-old 's suicidal poetry \", \"are more deeply thought through than in most ` right-thinking ' films \"], 'label': [0, 0, 1, 0, 0, 0, 1, 1, 0, 1]}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:28.364541Z",
     "start_time": "2025-09-25T02:14:28.333984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 只使用文本内容sentence，不使用情感标签\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'])\n",
    "\n",
    "map_kwargs = {\n",
    "    'batched': True,\n",
    "    'batch_size': 512,\n",
    "    'remove_columns': ['idx', 'sentence', 'label']\n",
    "}\n",
    "\n",
    "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
    "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)\n",
    "\n",
    "print(tokenized_dataset_train[0])\n",
    "print(tokenized_dataset_train[5:10])"
   ],
   "id": "9cfdc96209a0b1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [24717, 649, 3200, 507, 422, 262, 21694, 4991, 220], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [[5562, 705, 82, 1290, 1165, 15444, 284, 17004, 884, 31194, 3513, 220], [26567, 2536, 689, 326, 262, 3437, 286, 884, 289, 31777, 2512, 30181, 355, 29408, 1830, 460, 991, 1210, 503, 257, 1402, 837, 2614, 2646, 351, 281, 7016, 3355, 404, 764, 220], [1659, 473, 84, 948, 220], [64, 19095, 17280, 12, 1941, 12, 727, 705, 82, 26781, 19518, 220], [533, 517, 7744, 1807, 832, 621, 287, 749, 4600, 826, 12, 28973, 705, 7328, 220]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:30.649100Z",
     "start_time": "2025-09-25T02:14:30.645551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, seq in enumerate(tokenized_dataset_train[5:10]['input_ids']):\n",
    "    print(f'{i+1}: {tokenizer.decode(seq)}')"
   ],
   "id": "e6d6b1d70d73f716",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: that 's far too tragic to merit such superficial treatment \n",
      "2: demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \n",
      "3: of saucy \n",
      "4: a depressed fifteen-year-old 's suicidal poetry \n",
      "5: are more deeply thought through than in most ` right-thinking ' films \n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:32.821537Z",
     "start_time": "2025-09-25T02:14:32.813409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 去掉少于 6 个 token 的文本\n",
    "print(len(tokenized_dataset_train), len(tokenized_dataset_val))\n",
    "\n",
    "tokenized_dataset_train = tokenized_dataset_train.filter(lambda x: len(x['input_ids']) > 5)\n",
    "tokenized_dataset_val = tokenized_dataset_val.filter(lambda x: len(x['input_ids']) > 5)\n",
    "\n",
    "print(len(tokenized_dataset_train), len(tokenized_dataset_val))"
   ],
   "id": "9440e3ecc4be9bff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349 872\n",
      "49401 867\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:35.060947Z",
     "start_time": "2025-09-25T02:14:35.054442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 准备 dataloader 数据加载器\n",
    "# 设置为 PyTorch 的数据格式\n",
    "tokenized_dataset_train.set_format(type='torch')\n",
    "tokenized_dataset_val.set_format(type='torch')\n",
    "\n",
    "print(tokenized_dataset_train[0])\n",
    "print(tokenized_dataset_train[:5])"
   ],
   "id": "36f88255d4f85740",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([24717,   649,  3200,   507,   422,   262, 21694,  4991,   220]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': [tensor([24717,   649,  3200,   507,   422,   262, 21694,  4991,   220]), tensor([ 3642,  1299,   645, 20868,   837,   691,  2248,  1850,   308,  3775,\n",
      "          220]), tensor([ 5562, 10408,   663,  3435,   290, 48556,  1223,  2138,  4950,   546,\n",
      "         1692,  3450,   220]), tensor([ 2787,  1299, 15950, 11378,   284,  3520,   262,   976,  3690,   220]), tensor([  261,   262,  5290, 15827,    12,  1659,    12,  1169,    12,  1008,\n",
      "         9310, 35478, 20954,   262, 28303,   714, 47478,   469,   510,   220])], 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:37.853764Z",
     "start_time": "2025-09-25T02:14:37.851252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 检查pad token的设置（应该为空）\n",
    "print(tokenizer.pad_token)\n",
    "# # 检查eos token的设置\n",
    "print(tokenizer.eos_token)\n",
    "# N+ Implementation论文（第5页）说法不同\n",
    "# 但我们会使用attention_mask来移除用于填充的额外eos_token\n",
    "# 通过attention_mask来区分真正的结束token和用于填充的token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "9583425060969cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:39.544021Z",
     "start_time": "2025-09-25T02:14:39.541927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 示例说明\n",
    "text1 = \"Hello world\"           # 短文本\n",
    "text2 = \"Hello world how are you today\"  # 长文本\n",
    "\n",
    "# 填充后：\n",
    "# text1: \"Hello world<|endoftext|><|endoftext|><|endoftext|>\"  # 后面的是填充\n",
    "# text2: \"Hello world how are you today<|endoftext|>\"         # 最后的是真正结束\n",
    "\n",
    "# attention_mask区分：\n",
    "# text1: [1, 1, 1, 0, 0, 0]  # 1表示真实token，0表示填充token\n",
    "# text2: [1, 1, 1, 1, 1, 1, 1, 1]  # 全部都是真实token"
   ],
   "id": "cca1d3be3b9428a6",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:43.463354Z",
     "start_time": "2025-09-25T02:14:43.448820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "# mlm=False，将数据整理成“因果语言建模”需要的数据格式\n",
    "# “因果语言建模”就是“预测下一个token”类型的任务，也就是gpt风格的自回归模型\n",
    "# 如果mlm=True，那么数据整理成bert风格的任务所需的数据格式\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) # labels\n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 16, # 6G显存正好够用\n",
    "    'collate_fn': data_collator\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset_train, **dataloader_params)\n",
    "val_dataloader = DataLoader(tokenized_dataset_val, **dataloader_params)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['input_ids'][0])\n",
    "print(batch['labels'][0])\n",
    "print(batch['attention_mask'][0])"
   ],
   "id": "d172c196e6eac21a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3088\n",
      "KeysView({'input_ids': tensor([[24717,   649,  3200,   507,   422,   262, 21694,  4991,   220, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [ 3642,  1299,   645, 20868,   837,   691,  2248,  1850,   308,  3775,\n",
      "           220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [ 5562, 10408,   663,  3435,   290, 48556,  1223,  2138,  4950,   546,\n",
      "          1692,  3450,   220, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [ 2787,  1299, 15950, 11378,   284,  3520,   262,   976,  3690,   220,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [  261,   262,  5290, 15827,    12,  1659,    12,  1169,    12,  1008,\n",
      "          9310, 35478, 20954,   262, 28303,   714, 47478,   469,   510,   220,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [ 5562,   705,    82,  1290,  1165, 15444,   284, 17004,   884, 31194,\n",
      "          3513,   220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [26567,  2536,   689,   326,   262,  3437,   286,   884,   289, 31777,\n",
      "          2512, 30181,   355, 29408,  1830,   460,   991,  1210,   503,   257,\n",
      "          1402,   837,  2614,  2646,   351,   281,  7016,  3355,   404,   764,\n",
      "           220, 50256, 50256, 50256],\n",
      "        [   64, 19095, 17280,    12,  1941,    12,   727,   705,    82, 26781,\n",
      "         19518,   220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [  533,   517,  7744,  1807,   832,   621,   287,   749,  4600,   826,\n",
      "            12, 28973,   705,  7328,   220, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [ 2188,   274,   284, 12986, 20428,   220, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [ 1640,   883,  3807, 31006,   508, 13121,   326,  4600,   484,   466,\n",
      "           299,   470,   787,  6918,   588,   484,   973,   284,  7471,   220,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [ 1169,   636,   810,  2147,   705,    82,  5836,   837,   220, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [43439,   703,  2089,   428,  3807,   373,   220, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [   75,   437,   617, 16247,   284,   257, 13526,  1621,   220, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [ 4480,   465,  6678,  4430,   290, 11800,   774,   220, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256],\n",
      "        [ 2032, 27428,   318,  2029,   477,   546,   257,  1862,  2415,   705,\n",
      "            82,  1986,   837,   290,   416, 13092,   281, 14549,  3025,  1986,\n",
      "          4493,   326,  2415,   705,    82, 17188,   290,   614, 23400,   837,\n",
      "           340, 31137,   764,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[24717,   649,  3200,   507,   422,   262, 21694,  4991,   220,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 3642,  1299,   645, 20868,   837,   691,  2248,  1850,   308,  3775,\n",
      "           220,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 5562, 10408,   663,  3435,   290, 48556,  1223,  2138,  4950,   546,\n",
      "          1692,  3450,   220,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 2787,  1299, 15950, 11378,   284,  3520,   262,   976,  3690,   220,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [  261,   262,  5290, 15827,    12,  1659,    12,  1169,    12,  1008,\n",
      "          9310, 35478, 20954,   262, 28303,   714, 47478,   469,   510,   220,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 5562,   705,    82,  1290,  1165, 15444,   284, 17004,   884, 31194,\n",
      "          3513,   220,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [26567,  2536,   689,   326,   262,  3437,   286,   884,   289, 31777,\n",
      "          2512, 30181,   355, 29408,  1830,   460,   991,  1210,   503,   257,\n",
      "          1402,   837,  2614,  2646,   351,   281,  7016,  3355,   404,   764,\n",
      "           220,  -100,  -100,  -100],\n",
      "        [   64, 19095, 17280,    12,  1941,    12,   727,   705,    82, 26781,\n",
      "         19518,   220,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [  533,   517,  7744,  1807,   832,   621,   287,   749,  4600,   826,\n",
      "            12, 28973,   705,  7328,   220,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 2188,   274,   284, 12986, 20428,   220,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1640,   883,  3807, 31006,   508, 13121,   326,  4600,   484,   466,\n",
      "           299,   470,   787,  6918,   588,   484,   973,   284,  7471,   220,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 1169,   636,   810,  2147,   705,    82,  5836,   837,   220,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [43439,   703,  2089,   428,  3807,   373,   220,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [   75,   437,   617, 16247,   284,   257, 13526,  1621,   220,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 4480,   465,  6678,  4430,   290, 11800,   774,   220,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [ 2032, 27428,   318,  2029,   477,   546,   257,  1862,  2415,   705,\n",
      "            82,  1986,   837,   290,   416, 13092,   281, 14549,  3025,  1986,\n",
      "          4493,   326,  2415,   705,    82, 17188,   290,   614, 23400,   837,\n",
      "           340, 31137,   764,   220]])})\n",
      "torch.Size([16, 34])\n",
      "tensor([24717,   649,  3200,   507,   422,   262, 21694,  4991,   220, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256])\n",
      "tensor([24717,   649,  3200,   507,   422,   262, 21694,  4991,   220,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:278: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:278: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:14:46.794810Z",
     "start_time": "2025-09-25T02:14:46.791628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# 要更新的是model的参数\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# 一般sft会训练1个epoch\n",
    "num_epochs = 1"
   ],
   "id": "c18a85a79f603c99",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:17:12.558379Z",
     "start_time": "2025-09-25T02:17:12.555119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate(epoch):\n",
    "    model.eval() # 评估模式，禁用模型的随机性，例如dropout等特性\n",
    "    total_loss = 0.0\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss # 损失\n",
    "            total_loss += loss.item()\n",
    "    print(f'val_loss at {epoch} epoch:', total_loss / len(val_dataloader))"
   ],
   "id": "35df11eb98942dfb",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:34:00.832413Z",
     "start_time": "2025-09-25T02:18:38.360343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 续写文章的sft\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "validate(0)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        print(f'Loss: {loss.item()}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    validate(epoch+1)"
   ],
   "id": "f8cf353b50496f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:278: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:278: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss at 0 epoch: 5.174781010367654\n",
      "Loss: 6.016317367553711\n",
      "Loss: 5.630809307098389\n",
      "Loss: 5.695191383361816\n",
      "Loss: 5.2630615234375\n",
      "Loss: 5.174826145172119\n",
      "Loss: 4.916262626647949\n",
      "Loss: 4.7063069343566895\n",
      "Loss: 5.204032897949219\n",
      "Loss: 5.0653557777404785\n",
      "Loss: 5.249309539794922\n",
      "Loss: 5.162892818450928\n",
      "Loss: 5.058197498321533\n",
      "Loss: 5.423579692840576\n",
      "Loss: 4.559014320373535\n",
      "Loss: 4.7218546867370605\n",
      "Loss: 4.710030555725098\n",
      "Loss: 4.994248867034912\n",
      "Loss: 4.532787322998047\n",
      "Loss: 4.631570816040039\n",
      "Loss: 5.145158767700195\n",
      "Loss: 5.122682094573975\n",
      "Loss: 4.3845720291137695\n",
      "Loss: 4.83773136138916\n",
      "Loss: 4.355783939361572\n",
      "Loss: 4.824408054351807\n",
      "Loss: 4.490377426147461\n",
      "Loss: 5.0886335372924805\n",
      "Loss: 4.741083145141602\n",
      "Loss: 4.1639485359191895\n",
      "Loss: 4.6926164627075195\n",
      "Loss: 4.283303260803223\n",
      "Loss: 4.578400611877441\n",
      "Loss: 4.436331272125244\n",
      "Loss: 4.757376670837402\n",
      "Loss: 4.3918633460998535\n",
      "Loss: 4.905493259429932\n",
      "Loss: 4.725922107696533\n",
      "Loss: 4.398403644561768\n",
      "Loss: 4.382633686065674\n",
      "Loss: 4.69187068939209\n",
      "Loss: 4.47551965713501\n",
      "Loss: 4.492678642272949\n",
      "Loss: 4.187011241912842\n",
      "Loss: 4.616045951843262\n",
      "Loss: 4.391874313354492\n",
      "Loss: 4.330548286437988\n",
      "Loss: 4.419893741607666\n",
      "Loss: 4.649269104003906\n",
      "Loss: 4.275990009307861\n",
      "Loss: 4.5584564208984375\n",
      "Loss: 4.227543354034424\n",
      "Loss: 4.558674335479736\n",
      "Loss: 4.503846168518066\n",
      "Loss: 4.499718189239502\n",
      "Loss: 4.1630940437316895\n",
      "Loss: 4.649342060089111\n",
      "Loss: 4.0930891036987305\n",
      "Loss: 4.620162010192871\n",
      "Loss: 4.726046085357666\n",
      "Loss: 4.5106730461120605\n",
      "Loss: 4.578683376312256\n",
      "Loss: 4.601531028747559\n",
      "Loss: 4.405278205871582\n",
      "Loss: 4.370319843292236\n",
      "Loss: 4.365302562713623\n",
      "Loss: 4.083185195922852\n",
      "Loss: 4.438045501708984\n",
      "Loss: 4.578304767608643\n",
      "Loss: 4.604438304901123\n",
      "Loss: 4.292018413543701\n",
      "Loss: 4.229341983795166\n",
      "Loss: 4.4873270988464355\n",
      "Loss: 4.307310104370117\n",
      "Loss: 4.561347007751465\n",
      "Loss: 4.438329219818115\n",
      "Loss: 4.467156887054443\n",
      "Loss: 4.193238258361816\n",
      "Loss: 4.288096904754639\n",
      "Loss: 4.298508167266846\n",
      "Loss: 4.423980712890625\n",
      "Loss: 4.689471244812012\n",
      "Loss: 4.507273197174072\n",
      "Loss: 4.580079555511475\n",
      "Loss: 4.1724395751953125\n",
      "Loss: 4.168583869934082\n",
      "Loss: 4.652506351470947\n",
      "Loss: 4.3587164878845215\n",
      "Loss: 4.85780143737793\n",
      "Loss: 4.102158069610596\n",
      "Loss: 5.01383113861084\n",
      "Loss: 4.4141693115234375\n",
      "Loss: 4.250039100646973\n",
      "Loss: 4.682113170623779\n",
      "Loss: 4.2516770362854\n",
      "Loss: 4.789836406707764\n",
      "Loss: 4.703357696533203\n",
      "Loss: 4.602112293243408\n",
      "Loss: 4.969548225402832\n",
      "Loss: 4.073075294494629\n",
      "Loss: 4.3031768798828125\n",
      "Loss: 4.520621299743652\n",
      "Loss: 4.032846927642822\n",
      "Loss: 4.115233421325684\n",
      "Loss: 4.320247173309326\n",
      "Loss: 4.573140621185303\n",
      "Loss: 4.593332767486572\n",
      "Loss: 4.522912979125977\n",
      "Loss: 4.226236343383789\n",
      "Loss: 4.502971649169922\n",
      "Loss: 4.216058254241943\n",
      "Loss: 4.663785457611084\n",
      "Loss: 4.462911605834961\n",
      "Loss: 4.258549690246582\n",
      "Loss: 4.237849712371826\n",
      "Loss: 4.606082916259766\n",
      "Loss: 4.011232376098633\n",
      "Loss: 4.801716327667236\n",
      "Loss: 4.279217720031738\n",
      "Loss: 4.395501136779785\n",
      "Loss: 4.3954973220825195\n",
      "Loss: 4.201939582824707\n",
      "Loss: 4.416398525238037\n",
      "Loss: 4.548923492431641\n",
      "Loss: 4.5424113273620605\n",
      "Loss: 4.622832775115967\n",
      "Loss: 4.726797580718994\n",
      "Loss: 4.200582504272461\n",
      "Loss: 4.164078712463379\n",
      "Loss: 4.563019275665283\n",
      "Loss: 4.283178806304932\n",
      "Loss: 4.318348407745361\n",
      "Loss: 4.134642601013184\n",
      "Loss: 4.738563060760498\n",
      "Loss: 4.166014194488525\n",
      "Loss: 4.5759758949279785\n",
      "Loss: 3.805941581726074\n",
      "Loss: 4.032881259918213\n",
      "Loss: 4.199475288391113\n",
      "Loss: 4.839562892913818\n",
      "Loss: 4.396844387054443\n",
      "Loss: 4.184055805206299\n",
      "Loss: 4.381204128265381\n",
      "Loss: 4.531424522399902\n",
      "Loss: 4.334283351898193\n",
      "Loss: 4.5132904052734375\n",
      "Loss: 4.596274375915527\n",
      "Loss: 3.9722611904144287\n",
      "Loss: 4.542881965637207\n",
      "Loss: 4.0980143547058105\n",
      "Loss: 4.502303123474121\n",
      "Loss: 4.437014102935791\n",
      "Loss: 4.302060604095459\n",
      "Loss: 4.18265438079834\n",
      "Loss: 3.7790188789367676\n",
      "Loss: 4.214375019073486\n",
      "Loss: 4.440948486328125\n",
      "Loss: 4.3497748374938965\n",
      "Loss: 4.520668029785156\n",
      "Loss: 4.686395168304443\n",
      "Loss: 4.1868510246276855\n",
      "Loss: 4.2107439041137695\n",
      "Loss: 4.193894386291504\n",
      "Loss: 4.712660312652588\n",
      "Loss: 4.4684834480285645\n",
      "Loss: 4.337882995605469\n",
      "Loss: 4.438868045806885\n",
      "Loss: 4.539848804473877\n",
      "Loss: 4.316144943237305\n",
      "Loss: 4.57751989364624\n",
      "Loss: 4.084978103637695\n",
      "Loss: 4.011472225189209\n",
      "Loss: 4.283272743225098\n",
      "Loss: 4.5095014572143555\n",
      "Loss: 4.116228103637695\n",
      "Loss: 4.192533016204834\n",
      "Loss: 3.9468133449554443\n",
      "Loss: 4.594254970550537\n",
      "Loss: 4.258766174316406\n",
      "Loss: 4.518592357635498\n",
      "Loss: 4.462275505065918\n",
      "Loss: 4.2918524742126465\n",
      "Loss: 4.262263774871826\n",
      "Loss: 4.383360385894775\n",
      "Loss: 4.066412448883057\n",
      "Loss: 4.088435649871826\n",
      "Loss: 4.368358135223389\n",
      "Loss: 4.578958034515381\n",
      "Loss: 4.227497577667236\n",
      "Loss: 3.8293182849884033\n",
      "Loss: 4.305295467376709\n",
      "Loss: 4.421891689300537\n",
      "Loss: 4.540189266204834\n",
      "Loss: 4.38625431060791\n",
      "Loss: 4.242122173309326\n",
      "Loss: 4.692454814910889\n",
      "Loss: 4.427092552185059\n",
      "Loss: 4.003024578094482\n",
      "Loss: 4.165920257568359\n",
      "Loss: 3.932058811187744\n",
      "Loss: 4.019649028778076\n",
      "Loss: 4.390252113342285\n",
      "Loss: 4.6549153327941895\n",
      "Loss: 4.329635143280029\n",
      "Loss: 4.507875442504883\n",
      "Loss: 4.084389686584473\n",
      "Loss: 4.033252239227295\n",
      "Loss: 4.651866912841797\n",
      "Loss: 4.549755096435547\n",
      "Loss: 4.177940368652344\n",
      "Loss: 4.103423595428467\n",
      "Loss: 4.116428375244141\n",
      "Loss: 4.228857517242432\n",
      "Loss: 4.130797386169434\n",
      "Loss: 4.513852596282959\n",
      "Loss: 4.164422512054443\n",
      "Loss: 4.08790922164917\n",
      "Loss: 4.134433746337891\n",
      "Loss: 4.036871910095215\n",
      "Loss: 4.177538871765137\n",
      "Loss: 4.515110015869141\n",
      "Loss: 4.210949420928955\n",
      "Loss: 4.347339630126953\n",
      "Loss: 4.244302272796631\n",
      "Loss: 4.060436248779297\n",
      "Loss: 4.2117791175842285\n",
      "Loss: 4.146782398223877\n",
      "Loss: 4.122902870178223\n",
      "Loss: 4.507917881011963\n",
      "Loss: 4.296329975128174\n",
      "Loss: 4.214462757110596\n",
      "Loss: 4.1785688400268555\n",
      "Loss: 4.3520379066467285\n",
      "Loss: 4.263264179229736\n",
      "Loss: 4.65310525894165\n",
      "Loss: 4.159587860107422\n",
      "Loss: 4.301678657531738\n",
      "Loss: 3.7552497386932373\n",
      "Loss: 3.871973752975464\n",
      "Loss: 4.315263748168945\n",
      "Loss: 3.900402307510376\n",
      "Loss: 4.097960948944092\n",
      "Loss: 4.300371170043945\n",
      "Loss: 4.70064640045166\n",
      "Loss: 3.8754799365997314\n",
      "Loss: 4.100443363189697\n",
      "Loss: 4.224032878875732\n",
      "Loss: 4.087001800537109\n",
      "Loss: 4.264551162719727\n",
      "Loss: 4.416207790374756\n",
      "Loss: 4.229248046875\n",
      "Loss: 4.260126113891602\n",
      "Loss: 4.2708611488342285\n",
      "Loss: 3.808335542678833\n",
      "Loss: 3.8484420776367188\n",
      "Loss: 4.08974552154541\n",
      "Loss: 4.177477836608887\n",
      "Loss: 4.162475109100342\n",
      "Loss: 4.373421669006348\n",
      "Loss: 3.990514039993286\n",
      "Loss: 4.431137561798096\n",
      "Loss: 4.449446678161621\n",
      "Loss: 4.051880359649658\n",
      "Loss: 4.425359725952148\n",
      "Loss: 4.078983306884766\n",
      "Loss: 4.190788745880127\n",
      "Loss: 4.119394302368164\n",
      "Loss: 3.889400005340576\n",
      "Loss: 4.800503730773926\n",
      "Loss: 4.337673187255859\n",
      "Loss: 3.9268441200256348\n",
      "Loss: 4.154179096221924\n",
      "Loss: 4.324435710906982\n",
      "Loss: 4.342301368713379\n",
      "Loss: 3.999347686767578\n",
      "Loss: 4.0244855880737305\n",
      "Loss: 3.927896738052368\n",
      "Loss: 4.0687150955200195\n",
      "Loss: 4.105047225952148\n",
      "Loss: 4.2110514640808105\n",
      "Loss: 4.243720531463623\n",
      "Loss: 4.23787784576416\n",
      "Loss: 4.401205062866211\n",
      "Loss: 3.710846185684204\n",
      "Loss: 4.537811279296875\n",
      "Loss: 4.214092254638672\n",
      "Loss: 4.198007106781006\n",
      "Loss: 4.192256450653076\n",
      "Loss: 4.517065525054932\n",
      "Loss: 4.101262092590332\n",
      "Loss: 4.198272705078125\n",
      "Loss: 3.974764108657837\n",
      "Loss: 3.9189069271087646\n",
      "Loss: 4.028746604919434\n",
      "Loss: 4.573215961456299\n",
      "Loss: 4.372765064239502\n",
      "Loss: 4.039186000823975\n",
      "Loss: 3.6696059703826904\n",
      "Loss: 4.176634788513184\n",
      "Loss: 3.8729407787323\n",
      "Loss: 4.171303749084473\n",
      "Loss: 4.238460540771484\n",
      "Loss: 3.81298565864563\n",
      "Loss: 4.122386932373047\n",
      "Loss: 3.7467586994171143\n",
      "Loss: 3.976189374923706\n",
      "Loss: 4.165657043457031\n",
      "Loss: 4.2718281745910645\n",
      "Loss: 4.298187732696533\n",
      "Loss: 4.027423858642578\n",
      "Loss: 4.251755714416504\n",
      "Loss: 3.7444863319396973\n",
      "Loss: 4.635827541351318\n",
      "Loss: 4.907845973968506\n",
      "Loss: 4.516171455383301\n",
      "Loss: 4.022211074829102\n",
      "Loss: 4.1901350021362305\n",
      "Loss: 4.359786033630371\n",
      "Loss: 3.9798848628997803\n",
      "Loss: 4.032205104827881\n",
      "Loss: 4.1403961181640625\n",
      "Loss: 4.241690635681152\n",
      "Loss: 4.111327648162842\n",
      "Loss: 4.220975399017334\n",
      "Loss: 3.7395238876342773\n",
      "Loss: 4.161263465881348\n",
      "Loss: 3.9173583984375\n",
      "Loss: 4.2434000968933105\n",
      "Loss: 3.8708291053771973\n",
      "Loss: 4.379013538360596\n",
      "Loss: 3.935767650604248\n",
      "Loss: 4.168025493621826\n",
      "Loss: 3.8292815685272217\n",
      "Loss: 4.210870742797852\n",
      "Loss: 4.095706939697266\n",
      "Loss: 4.116145610809326\n",
      "Loss: 4.125438213348389\n",
      "Loss: 4.1287126541137695\n",
      "Loss: 4.0252203941345215\n",
      "Loss: 4.351159572601318\n",
      "Loss: 4.341651439666748\n",
      "Loss: 4.328820705413818\n",
      "Loss: 4.045184135437012\n",
      "Loss: 4.053175926208496\n",
      "Loss: 4.0049967765808105\n",
      "Loss: 4.151708126068115\n",
      "Loss: 4.573930263519287\n",
      "Loss: 4.352410793304443\n",
      "Loss: 4.323770523071289\n",
      "Loss: 4.532098770141602\n",
      "Loss: 4.125668525695801\n",
      "Loss: 3.8653078079223633\n",
      "Loss: 4.385560035705566\n",
      "Loss: 4.190113067626953\n",
      "Loss: 4.376895427703857\n",
      "Loss: 3.743492364883423\n",
      "Loss: 4.043546199798584\n",
      "Loss: 4.053078651428223\n",
      "Loss: 4.370306491851807\n",
      "Loss: 4.338839054107666\n",
      "Loss: 3.9583466053009033\n",
      "Loss: 3.6680164337158203\n",
      "Loss: 4.0039215087890625\n",
      "Loss: 3.767230272293091\n",
      "Loss: 4.3489251136779785\n",
      "Loss: 3.8614394664764404\n",
      "Loss: 3.7530221939086914\n",
      "Loss: 4.470896244049072\n",
      "Loss: 4.347805976867676\n",
      "Loss: 4.269405841827393\n",
      "Loss: 4.015622138977051\n",
      "Loss: 4.275869369506836\n",
      "Loss: 4.378166198730469\n",
      "Loss: 4.1545562744140625\n",
      "Loss: 3.9100093841552734\n",
      "Loss: 3.8783798217773438\n",
      "Loss: 4.407757759094238\n",
      "Loss: 3.995380163192749\n",
      "Loss: 4.164317607879639\n",
      "Loss: 4.318114757537842\n",
      "Loss: 3.9098448753356934\n",
      "Loss: 4.283935546875\n",
      "Loss: 4.0687055587768555\n",
      "Loss: 4.396667957305908\n",
      "Loss: 4.149516582489014\n",
      "Loss: 3.9214789867401123\n",
      "Loss: 4.686408996582031\n",
      "Loss: 4.295619010925293\n",
      "Loss: 3.4449026584625244\n",
      "Loss: 4.30336332321167\n",
      "Loss: 4.3858842849731445\n",
      "Loss: 3.8408243656158447\n",
      "Loss: 4.203290939331055\n",
      "Loss: 3.8386728763580322\n",
      "Loss: 4.197350025177002\n",
      "Loss: 4.356086730957031\n",
      "Loss: 3.9612324237823486\n",
      "Loss: 4.331969738006592\n",
      "Loss: 4.100074291229248\n",
      "Loss: 4.312021732330322\n",
      "Loss: 4.033974647521973\n",
      "Loss: 4.255334377288818\n",
      "Loss: 3.987436294555664\n",
      "Loss: 4.015563011169434\n",
      "Loss: 3.917245388031006\n",
      "Loss: 4.319366455078125\n",
      "Loss: 4.33249568939209\n",
      "Loss: 4.14442253112793\n",
      "Loss: 4.384293079376221\n",
      "Loss: 4.036670207977295\n",
      "Loss: 4.470486164093018\n",
      "Loss: 3.972728967666626\n",
      "Loss: 3.8060193061828613\n",
      "Loss: 4.036781311035156\n",
      "Loss: 4.019092559814453\n",
      "Loss: 3.989598035812378\n",
      "Loss: 4.57404088973999\n",
      "Loss: 4.283839702606201\n",
      "Loss: 4.110133647918701\n",
      "Loss: 3.9763271808624268\n",
      "Loss: 4.270261764526367\n",
      "Loss: 4.352865219116211\n",
      "Loss: 4.088695049285889\n",
      "Loss: 3.9729268550872803\n",
      "Loss: 4.0629682540893555\n",
      "Loss: 4.507789134979248\n",
      "Loss: 4.178194522857666\n",
      "Loss: 4.090612888336182\n",
      "Loss: 4.134798526763916\n",
      "Loss: 3.8084189891815186\n",
      "Loss: 4.201655387878418\n",
      "Loss: 3.8967556953430176\n",
      "Loss: 3.747499704360962\n",
      "Loss: 4.140768051147461\n",
      "Loss: 3.9265265464782715\n",
      "Loss: 4.17009973526001\n",
      "Loss: 4.486161708831787\n",
      "Loss: 4.165495872497559\n",
      "Loss: 3.559004068374634\n",
      "Loss: 4.055037975311279\n",
      "Loss: 4.037796974182129\n",
      "Loss: 4.023991584777832\n",
      "Loss: 4.154510498046875\n",
      "Loss: 4.263627052307129\n",
      "Loss: 3.8525400161743164\n",
      "Loss: 4.007382869720459\n",
      "Loss: 4.634342193603516\n",
      "Loss: 3.9152302742004395\n",
      "Loss: 4.1823410987854\n",
      "Loss: 4.189346790313721\n",
      "Loss: 4.209628582000732\n",
      "Loss: 4.013817310333252\n",
      "Loss: 3.935379981994629\n",
      "Loss: 3.6694273948669434\n",
      "Loss: 3.7910451889038086\n",
      "Loss: 4.0860981941223145\n",
      "Loss: 3.9019761085510254\n",
      "Loss: 3.7799441814422607\n",
      "Loss: 4.378341197967529\n",
      "Loss: 3.8968331813812256\n",
      "Loss: 3.8192460536956787\n",
      "Loss: 4.395920276641846\n",
      "Loss: 4.25498104095459\n",
      "Loss: 3.6593093872070312\n",
      "Loss: 4.2269768714904785\n",
      "Loss: 4.150877475738525\n",
      "Loss: 3.7199835777282715\n",
      "Loss: 4.328092098236084\n",
      "Loss: 3.651820421218872\n",
      "Loss: 4.0049920082092285\n",
      "Loss: 4.076255798339844\n",
      "Loss: 4.133587837219238\n",
      "Loss: 4.140668869018555\n",
      "Loss: 4.174108028411865\n",
      "Loss: 4.143041610717773\n",
      "Loss: 3.796834707260132\n",
      "Loss: 4.414125442504883\n",
      "Loss: 3.8944835662841797\n",
      "Loss: 4.2172746658325195\n",
      "Loss: 3.707533121109009\n",
      "Loss: 3.6720242500305176\n",
      "Loss: 4.126036643981934\n",
      "Loss: 4.1160359382629395\n",
      "Loss: 3.481818199157715\n",
      "Loss: 4.215174198150635\n",
      "Loss: 4.1764655113220215\n",
      "Loss: 4.309813022613525\n",
      "Loss: 3.9900503158569336\n",
      "Loss: 3.9995341300964355\n",
      "Loss: 4.2420125007629395\n",
      "Loss: 3.8098113536834717\n",
      "Loss: 4.390821933746338\n",
      "Loss: 3.8599486351013184\n",
      "Loss: 4.334911346435547\n",
      "Loss: 4.1775970458984375\n",
      "Loss: 4.141143798828125\n",
      "Loss: 3.922436475753784\n",
      "Loss: 4.2341628074646\n",
      "Loss: 3.881877899169922\n",
      "Loss: 3.99817156791687\n",
      "Loss: 3.394932508468628\n",
      "Loss: 3.7809150218963623\n",
      "Loss: 4.349307537078857\n",
      "Loss: 4.046512603759766\n",
      "Loss: 3.9901626110076904\n",
      "Loss: 4.105993747711182\n",
      "Loss: 4.144232273101807\n",
      "Loss: 4.021604061126709\n",
      "Loss: 3.7886688709259033\n",
      "Loss: 3.9970247745513916\n",
      "Loss: 4.046509265899658\n",
      "Loss: 3.96020770072937\n",
      "Loss: 4.038883686065674\n",
      "Loss: 4.226119518280029\n",
      "Loss: 3.605668544769287\n",
      "Loss: 4.1296000480651855\n",
      "Loss: 3.8923239707946777\n",
      "Loss: 3.6296041011810303\n",
      "Loss: 3.7670698165893555\n",
      "Loss: 3.957148790359497\n",
      "Loss: 3.7563812732696533\n",
      "Loss: 4.0169525146484375\n",
      "Loss: 4.23301887512207\n",
      "Loss: 3.594148635864258\n",
      "Loss: 4.064255714416504\n",
      "Loss: 4.03436803817749\n",
      "Loss: 3.9676625728607178\n",
      "Loss: 4.099619388580322\n",
      "Loss: 4.385616302490234\n",
      "Loss: 3.627868413925171\n",
      "Loss: 4.181117534637451\n",
      "Loss: 4.219733715057373\n",
      "Loss: 3.992745876312256\n",
      "Loss: 3.9990437030792236\n",
      "Loss: 4.125656604766846\n",
      "Loss: 4.061849594116211\n",
      "Loss: 3.8230621814727783\n",
      "Loss: 4.1468939781188965\n",
      "Loss: 3.7963147163391113\n",
      "Loss: 4.039931774139404\n",
      "Loss: 4.492633819580078\n",
      "Loss: 4.09359073638916\n",
      "Loss: 4.197796821594238\n",
      "Loss: 4.251218795776367\n",
      "Loss: 4.3446807861328125\n",
      "Loss: 3.820063829421997\n",
      "Loss: 3.7829275131225586\n",
      "Loss: 4.2123003005981445\n",
      "Loss: 3.8988659381866455\n",
      "Loss: 4.057365894317627\n",
      "Loss: 4.134621620178223\n",
      "Loss: 4.035778999328613\n",
      "Loss: 4.247769832611084\n",
      "Loss: 4.147985935211182\n",
      "Loss: 4.119248390197754\n",
      "Loss: 3.9139115810394287\n",
      "Loss: 3.71209979057312\n",
      "Loss: 3.8284177780151367\n",
      "Loss: 3.5811190605163574\n",
      "Loss: 4.197107791900635\n",
      "Loss: 3.9882113933563232\n",
      "Loss: 4.023074150085449\n",
      "Loss: 4.197879791259766\n",
      "Loss: 3.9354634284973145\n",
      "Loss: 3.858999252319336\n",
      "Loss: 4.120823383331299\n",
      "Loss: 3.977283477783203\n",
      "Loss: 3.940549612045288\n",
      "Loss: 4.211747646331787\n",
      "Loss: 3.961223840713501\n",
      "Loss: 4.154151916503906\n",
      "Loss: 4.093236923217773\n",
      "Loss: 3.758902072906494\n",
      "Loss: 4.285604953765869\n",
      "Loss: 3.8106610774993896\n",
      "Loss: 3.8017024993896484\n",
      "Loss: 3.7998812198638916\n",
      "Loss: 4.089730739593506\n",
      "Loss: 4.290424346923828\n",
      "Loss: 4.081104278564453\n",
      "Loss: 3.770829677581787\n",
      "Loss: 3.7969889640808105\n",
      "Loss: 4.369148254394531\n",
      "Loss: 4.1028642654418945\n",
      "Loss: 3.8713972568511963\n",
      "Loss: 4.204663276672363\n",
      "Loss: 3.9944705963134766\n",
      "Loss: 3.690999984741211\n",
      "Loss: 3.6978368759155273\n",
      "Loss: 4.072818279266357\n",
      "Loss: 3.7419698238372803\n",
      "Loss: 4.148066997528076\n",
      "Loss: 3.5757689476013184\n",
      "Loss: 3.9905638694763184\n",
      "Loss: 3.822305679321289\n",
      "Loss: 3.483900547027588\n",
      "Loss: 4.188787937164307\n",
      "Loss: 3.8962759971618652\n",
      "Loss: 3.8890557289123535\n",
      "Loss: 4.205041408538818\n",
      "Loss: 4.005274295806885\n",
      "Loss: 3.960833787918091\n",
      "Loss: 3.958899974822998\n",
      "Loss: 4.279280185699463\n",
      "Loss: 4.16243839263916\n",
      "Loss: 3.8141422271728516\n",
      "Loss: 3.9343254566192627\n",
      "Loss: 4.087836265563965\n",
      "Loss: 4.14865255355835\n",
      "Loss: 3.8085806369781494\n",
      "Loss: 4.166760444641113\n",
      "Loss: 4.028265476226807\n",
      "Loss: 4.04132604598999\n",
      "Loss: 3.8419861793518066\n",
      "Loss: 4.18980073928833\n",
      "Loss: 4.040146350860596\n",
      "Loss: 4.123149394989014\n",
      "Loss: 3.9908876419067383\n",
      "Loss: 4.0958027839660645\n",
      "Loss: 3.926820993423462\n",
      "Loss: 4.017361640930176\n",
      "Loss: 4.290241241455078\n",
      "Loss: 3.697971820831299\n",
      "Loss: 4.1067328453063965\n",
      "Loss: 3.912837266921997\n",
      "Loss: 4.014214038848877\n",
      "Loss: 3.9276533126831055\n",
      "Loss: 4.463864326477051\n",
      "Loss: 4.047152996063232\n",
      "Loss: 3.971560001373291\n",
      "Loss: 3.7888402938842773\n",
      "Loss: 4.0033135414123535\n",
      "Loss: 3.7896194458007812\n",
      "Loss: 3.7777183055877686\n",
      "Loss: 3.92215633392334\n",
      "Loss: 3.7051849365234375\n",
      "Loss: 4.068380832672119\n",
      "Loss: 3.967832326889038\n",
      "Loss: 4.050127983093262\n",
      "Loss: 4.001750469207764\n",
      "Loss: 4.497835636138916\n",
      "Loss: 3.936115264892578\n",
      "Loss: 3.784160614013672\n",
      "Loss: 3.848400831222534\n",
      "Loss: 3.7251908779144287\n",
      "Loss: 3.609457015991211\n",
      "Loss: 3.8472259044647217\n",
      "Loss: 3.8960816860198975\n",
      "Loss: 3.7961463928222656\n",
      "Loss: 3.9656386375427246\n",
      "Loss: 3.9110474586486816\n",
      "Loss: 3.9380669593811035\n",
      "Loss: 3.605961561203003\n",
      "Loss: 4.214114189147949\n",
      "Loss: 3.9287590980529785\n",
      "Loss: 4.050642967224121\n",
      "Loss: 4.019540786743164\n",
      "Loss: 3.853048324584961\n",
      "Loss: 3.6688759326934814\n",
      "Loss: 3.9858505725860596\n",
      "Loss: 3.7498180866241455\n",
      "Loss: 4.000953674316406\n",
      "Loss: 3.7081358432769775\n",
      "Loss: 4.366113662719727\n",
      "Loss: 3.853559970855713\n",
      "Loss: 3.9922940731048584\n",
      "Loss: 4.2399468421936035\n",
      "Loss: 3.7459728717803955\n",
      "Loss: 3.6638147830963135\n",
      "Loss: 4.048055171966553\n",
      "Loss: 3.9726734161376953\n",
      "Loss: 3.6321802139282227\n",
      "Loss: 3.948375940322876\n",
      "Loss: 3.7220382690429688\n",
      "Loss: 3.8643558025360107\n",
      "Loss: 3.877070665359497\n",
      "Loss: 4.095247745513916\n",
      "Loss: 3.706796646118164\n",
      "Loss: 4.193057537078857\n",
      "Loss: 4.1205363273620605\n",
      "Loss: 3.8580548763275146\n",
      "Loss: 3.7569260597229004\n",
      "Loss: 4.022025108337402\n",
      "Loss: 3.672534704208374\n",
      "Loss: 3.420854330062866\n",
      "Loss: 3.7897732257843018\n",
      "Loss: 4.168651103973389\n",
      "Loss: 4.001595497131348\n",
      "Loss: 4.110968589782715\n",
      "Loss: 3.9267358779907227\n",
      "Loss: 4.071646213531494\n",
      "Loss: 3.785623550415039\n",
      "Loss: 3.666346549987793\n",
      "Loss: 3.4313125610351562\n",
      "Loss: 3.9372029304504395\n",
      "Loss: 3.489279270172119\n",
      "Loss: 3.9547410011291504\n",
      "Loss: 3.9217331409454346\n",
      "Loss: 3.871907949447632\n",
      "Loss: 3.7745423316955566\n",
      "Loss: 3.9361732006073\n",
      "Loss: 4.244873046875\n",
      "Loss: 4.1300554275512695\n",
      "Loss: 4.072760105133057\n",
      "Loss: 4.027155876159668\n",
      "Loss: 3.662611484527588\n",
      "Loss: 3.970745086669922\n",
      "Loss: 3.9290013313293457\n",
      "Loss: 4.0540385246276855\n",
      "Loss: 4.041119575500488\n",
      "Loss: 3.8662710189819336\n",
      "Loss: 4.020841121673584\n",
      "Loss: 3.863145112991333\n",
      "Loss: 4.049375534057617\n",
      "Loss: 4.244039535522461\n",
      "Loss: 3.6664724349975586\n",
      "Loss: 3.7770817279815674\n",
      "Loss: 3.7088124752044678\n",
      "Loss: 3.525028944015503\n",
      "Loss: 3.9910330772399902\n",
      "Loss: 3.539191961288452\n",
      "Loss: 4.24233865737915\n",
      "Loss: 3.8631532192230225\n",
      "Loss: 4.116034030914307\n",
      "Loss: 3.7382874488830566\n",
      "Loss: 4.158377170562744\n",
      "Loss: 3.885596990585327\n",
      "Loss: 3.8150277137756348\n",
      "Loss: 3.955493688583374\n",
      "Loss: 4.106770992279053\n",
      "Loss: 4.007715225219727\n",
      "Loss: 3.7050933837890625\n",
      "Loss: 3.553370714187622\n",
      "Loss: 4.2203898429870605\n",
      "Loss: 3.6666250228881836\n",
      "Loss: 4.252166271209717\n",
      "Loss: 3.982139825820923\n",
      "Loss: 3.9250853061676025\n",
      "Loss: 3.9339208602905273\n",
      "Loss: 4.017466068267822\n",
      "Loss: 3.7328040599823\n",
      "Loss: 3.548147678375244\n",
      "Loss: 3.7197682857513428\n",
      "Loss: 3.544327974319458\n",
      "Loss: 3.7166359424591064\n",
      "Loss: 4.247692108154297\n",
      "Loss: 3.6020326614379883\n",
      "Loss: 3.864013195037842\n",
      "Loss: 3.468909978866577\n",
      "Loss: 4.049149990081787\n",
      "Loss: 3.7078704833984375\n",
      "Loss: 3.9561173915863037\n",
      "Loss: 4.151517391204834\n",
      "Loss: 3.795790433883667\n",
      "Loss: 4.179146766662598\n",
      "Loss: 3.6695761680603027\n",
      "Loss: 3.7792885303497314\n",
      "Loss: 3.8724024295806885\n",
      "Loss: 3.8300468921661377\n",
      "Loss: 3.8607373237609863\n",
      "Loss: 3.6761090755462646\n",
      "Loss: 3.85364031791687\n",
      "Loss: 3.8506927490234375\n",
      "Loss: 3.8438832759857178\n",
      "Loss: 3.7453134059906006\n",
      "Loss: 3.7777018547058105\n",
      "Loss: 3.9596686363220215\n",
      "Loss: 3.7834312915802\n",
      "Loss: 4.126532077789307\n",
      "Loss: 3.674177646636963\n",
      "Loss: 3.6781535148620605\n",
      "Loss: 3.6404454708099365\n",
      "Loss: 4.21003532409668\n",
      "Loss: 3.963301658630371\n",
      "Loss: 3.8121440410614014\n",
      "Loss: 4.530893802642822\n",
      "Loss: 3.381239414215088\n",
      "Loss: 4.08823299407959\n",
      "Loss: 4.194000244140625\n",
      "Loss: 3.7774269580841064\n",
      "Loss: 4.124476909637451\n",
      "Loss: 3.842240333557129\n",
      "Loss: 3.648892402648926\n",
      "Loss: 3.8752591609954834\n",
      "Loss: 3.8781790733337402\n",
      "Loss: 3.774841785430908\n",
      "Loss: 3.86370587348938\n",
      "Loss: 3.852203845977783\n",
      "Loss: 4.011971950531006\n",
      "Loss: 3.8648478984832764\n",
      "Loss: 3.8085033893585205\n",
      "Loss: 4.131758213043213\n",
      "Loss: 4.031168460845947\n",
      "Loss: 3.757929563522339\n",
      "Loss: 3.701962947845459\n",
      "Loss: 3.5182740688323975\n",
      "Loss: 3.658579111099243\n",
      "Loss: 4.158505916595459\n",
      "Loss: 4.251005172729492\n",
      "Loss: 3.626772880554199\n",
      "Loss: 4.287810802459717\n",
      "Loss: 4.059881687164307\n",
      "Loss: 3.8104517459869385\n",
      "Loss: 4.035520076751709\n",
      "Loss: 4.095250606536865\n",
      "Loss: 3.6233997344970703\n",
      "Loss: 3.6759414672851562\n",
      "Loss: 3.7294042110443115\n",
      "Loss: 3.926356792449951\n",
      "Loss: 3.8707542419433594\n",
      "Loss: 3.703965187072754\n",
      "Loss: 3.948072910308838\n",
      "Loss: 3.997807741165161\n",
      "Loss: 4.063756465911865\n",
      "Loss: 3.7265114784240723\n",
      "Loss: 4.234607696533203\n",
      "Loss: 4.161507606506348\n",
      "Loss: 3.7509171962738037\n",
      "Loss: 3.874854564666748\n",
      "Loss: 3.655653238296509\n",
      "Loss: 4.091753005981445\n",
      "Loss: 3.785668134689331\n",
      "Loss: 4.302585601806641\n",
      "Loss: 3.9757134914398193\n",
      "Loss: 3.471323013305664\n",
      "Loss: 4.076379776000977\n",
      "Loss: 3.981201171875\n",
      "Loss: 3.8094053268432617\n",
      "Loss: 4.125758647918701\n",
      "Loss: 3.8462443351745605\n",
      "Loss: 3.4891459941864014\n",
      "Loss: 3.741642475128174\n",
      "Loss: 3.7589986324310303\n",
      "Loss: 3.7196290493011475\n",
      "Loss: 3.7541933059692383\n",
      "Loss: 3.4939112663269043\n",
      "Loss: 3.8153834342956543\n",
      "Loss: 3.9200847148895264\n",
      "Loss: 3.8257906436920166\n",
      "Loss: 3.922653913497925\n",
      "Loss: 4.119534015655518\n",
      "Loss: 3.8475162982940674\n",
      "Loss: 3.669802188873291\n",
      "Loss: 3.9065043926239014\n",
      "Loss: 3.8351242542266846\n",
      "Loss: 3.4378907680511475\n",
      "Loss: 3.6638450622558594\n",
      "Loss: 3.978180408477783\n",
      "Loss: 3.5886948108673096\n",
      "Loss: 3.8402199745178223\n",
      "Loss: 3.5470869541168213\n",
      "Loss: 3.8693110942840576\n",
      "Loss: 3.58315372467041\n",
      "Loss: 4.025383472442627\n",
      "Loss: 3.845353603363037\n",
      "Loss: 3.8397436141967773\n",
      "Loss: 3.860565423965454\n",
      "Loss: 4.128650665283203\n",
      "Loss: 4.230007648468018\n",
      "Loss: 3.9659690856933594\n",
      "Loss: 3.7953264713287354\n",
      "Loss: 3.8664631843566895\n",
      "Loss: 3.879591941833496\n",
      "Loss: 3.9557695388793945\n",
      "Loss: 3.688692808151245\n",
      "Loss: 3.821570634841919\n",
      "Loss: 3.531046152114868\n",
      "Loss: 3.3705036640167236\n",
      "Loss: 3.7068347930908203\n",
      "Loss: 3.790968418121338\n",
      "Loss: 4.018513202667236\n",
      "Loss: 3.5775833129882812\n",
      "Loss: 4.022425174713135\n",
      "Loss: 4.132880210876465\n",
      "Loss: 3.706281900405884\n",
      "Loss: 3.6123530864715576\n",
      "Loss: 4.10162878036499\n",
      "Loss: 4.075766086578369\n",
      "Loss: 4.11646032333374\n",
      "Loss: 3.882702112197876\n",
      "Loss: 3.9790971279144287\n",
      "Loss: 3.7035067081451416\n",
      "Loss: 3.6538474559783936\n",
      "Loss: 3.8090198040008545\n",
      "Loss: 3.8310439586639404\n",
      "Loss: 3.934563636779785\n",
      "Loss: 3.8703441619873047\n",
      "Loss: 4.272114276885986\n",
      "Loss: 4.006897449493408\n",
      "Loss: 3.617908000946045\n",
      "Loss: 3.785554885864258\n",
      "Loss: 3.853778839111328\n",
      "Loss: 3.4651551246643066\n",
      "Loss: 3.4651856422424316\n",
      "Loss: 3.8530325889587402\n",
      "Loss: 3.904148578643799\n",
      "Loss: 4.123026371002197\n",
      "Loss: 3.753563404083252\n",
      "Loss: 4.167459011077881\n",
      "Loss: 3.438598394393921\n",
      "Loss: 3.9235496520996094\n",
      "Loss: 4.012597560882568\n",
      "Loss: 3.7841084003448486\n",
      "Loss: 3.740407705307007\n",
      "Loss: 3.9234766960144043\n",
      "Loss: 3.7174994945526123\n",
      "Loss: 3.500220537185669\n",
      "Loss: 3.9693753719329834\n",
      "Loss: 4.144773483276367\n",
      "Loss: 3.7616872787475586\n",
      "Loss: 3.6383557319641113\n",
      "Loss: 3.552903413772583\n",
      "Loss: 3.685046434402466\n",
      "Loss: 4.0606913566589355\n",
      "Loss: 3.764832019805908\n",
      "Loss: 3.5866518020629883\n",
      "Loss: 3.678804636001587\n",
      "Loss: 3.7826385498046875\n",
      "Loss: 3.7566375732421875\n",
      "Loss: 3.578876256942749\n",
      "Loss: 3.7155306339263916\n",
      "Loss: 4.324802398681641\n",
      "Loss: 3.6994290351867676\n",
      "Loss: 3.6386020183563232\n",
      "Loss: 3.8996596336364746\n",
      "Loss: 4.226183891296387\n",
      "Loss: 3.866056442260742\n",
      "Loss: 3.7524633407592773\n",
      "Loss: 4.055644989013672\n",
      "Loss: 3.8148553371429443\n",
      "Loss: 3.9359591007232666\n",
      "Loss: 3.648674488067627\n",
      "Loss: 3.699758529663086\n",
      "Loss: 4.164460182189941\n",
      "Loss: 3.791062116622925\n",
      "Loss: 4.078402042388916\n",
      "Loss: 4.137951850891113\n",
      "Loss: 4.399656772613525\n",
      "Loss: 3.6971850395202637\n",
      "Loss: 3.7914881706237793\n",
      "Loss: 4.265011310577393\n",
      "Loss: 3.7239577770233154\n",
      "Loss: 3.6872479915618896\n",
      "Loss: 4.342050075531006\n",
      "Loss: 4.211452484130859\n",
      "Loss: 3.9395132064819336\n",
      "Loss: 3.629924774169922\n",
      "Loss: 3.7791764736175537\n",
      "Loss: 3.6385364532470703\n",
      "Loss: 3.562436580657959\n",
      "Loss: 3.186685562133789\n",
      "Loss: 3.7226455211639404\n",
      "Loss: 3.9116992950439453\n",
      "Loss: 3.77270770072937\n",
      "Loss: 3.7156670093536377\n",
      "Loss: 3.802330732345581\n",
      "Loss: 3.9667468070983887\n",
      "Loss: 4.266067028045654\n",
      "Loss: 3.223442554473877\n",
      "Loss: 3.717921495437622\n",
      "Loss: 3.8649165630340576\n",
      "Loss: 3.842745780944824\n",
      "Loss: 3.4507853984832764\n",
      "Loss: 3.582240104675293\n",
      "Loss: 3.835094690322876\n",
      "Loss: 3.6654841899871826\n",
      "Loss: 3.9063875675201416\n",
      "Loss: 3.9238436222076416\n",
      "Loss: 3.820200204849243\n",
      "Loss: 3.8308968544006348\n",
      "Loss: 3.6979808807373047\n",
      "Loss: 3.576253890991211\n",
      "Loss: 3.4968085289001465\n",
      "Loss: 4.110639572143555\n",
      "Loss: 3.744127035140991\n",
      "Loss: 4.1090989112854\n",
      "Loss: 3.9119880199432373\n",
      "Loss: 3.8555216789245605\n",
      "Loss: 4.058735370635986\n",
      "Loss: 3.3884975910186768\n",
      "Loss: 3.6693851947784424\n",
      "Loss: 3.5795235633850098\n",
      "Loss: 3.636545181274414\n",
      "Loss: 3.9497969150543213\n",
      "Loss: 3.6950271129608154\n",
      "Loss: 3.8822033405303955\n",
      "Loss: 3.7024362087249756\n",
      "Loss: 3.91706919670105\n",
      "Loss: 3.7565970420837402\n",
      "Loss: 3.8279318809509277\n",
      "Loss: 3.7658722400665283\n",
      "Loss: 4.087928771972656\n",
      "Loss: 3.8461694717407227\n",
      "Loss: 3.934751510620117\n",
      "Loss: 3.4624171257019043\n",
      "Loss: 3.8550913333892822\n",
      "Loss: 3.610109567642212\n",
      "Loss: 3.9104020595550537\n",
      "Loss: 3.611696481704712\n",
      "Loss: 3.5451505184173584\n",
      "Loss: 3.9637632369995117\n",
      "Loss: 3.6788644790649414\n",
      "Loss: 3.7888407707214355\n",
      "Loss: 4.181408405303955\n",
      "Loss: 3.709695339202881\n",
      "Loss: 4.016263484954834\n",
      "Loss: 3.392514228820801\n",
      "Loss: 3.81856632232666\n",
      "Loss: 3.7105791568756104\n",
      "Loss: 3.8341434001922607\n",
      "Loss: 3.9458134174346924\n",
      "Loss: 3.706568717956543\n",
      "Loss: 3.923330307006836\n",
      "Loss: 3.757495641708374\n",
      "Loss: 3.603752374649048\n",
      "Loss: 3.778123378753662\n",
      "Loss: 3.7066152095794678\n",
      "Loss: 4.106806755065918\n",
      "Loss: 3.357863426208496\n",
      "Loss: 3.7701847553253174\n",
      "Loss: 3.7824559211730957\n",
      "Loss: 3.5460240840911865\n",
      "Loss: 4.0415778160095215\n",
      "Loss: 3.39465069770813\n",
      "Loss: 3.821803569793701\n",
      "Loss: 3.709080457687378\n",
      "Loss: 3.788271427154541\n",
      "Loss: 3.5590643882751465\n",
      "Loss: 3.8742260932922363\n",
      "Loss: 3.97556471824646\n",
      "Loss: 3.352905750274658\n",
      "Loss: 3.9055850505828857\n",
      "Loss: 3.9394848346710205\n",
      "Loss: 3.8387036323547363\n",
      "Loss: 3.9533135890960693\n",
      "Loss: 3.855844736099243\n",
      "Loss: 3.8016061782836914\n",
      "Loss: 4.1572418212890625\n",
      "Loss: 3.7791810035705566\n",
      "Loss: 3.7624399662017822\n",
      "Loss: 3.7782137393951416\n",
      "Loss: 3.8313868045806885\n",
      "Loss: 4.009184837341309\n",
      "Loss: 3.668795347213745\n",
      "Loss: 3.8036513328552246\n",
      "Loss: 3.706059694290161\n",
      "Loss: 3.399549722671509\n",
      "Loss: 3.9694619178771973\n",
      "Loss: 4.040432929992676\n",
      "Loss: 3.971423387527466\n",
      "Loss: 3.8623239994049072\n",
      "Loss: 3.286562442779541\n",
      "Loss: 3.770301580429077\n",
      "Loss: 3.863994598388672\n",
      "Loss: 3.8483784198760986\n",
      "Loss: 3.978827953338623\n",
      "Loss: 3.734740972518921\n",
      "Loss: 3.8793458938598633\n",
      "Loss: 3.9796059131622314\n",
      "Loss: 3.6139302253723145\n",
      "Loss: 3.77813720703125\n",
      "Loss: 3.940892219543457\n",
      "Loss: 4.343652248382568\n",
      "Loss: 3.7682604789733887\n",
      "Loss: 3.7648141384124756\n",
      "Loss: 3.7450153827667236\n",
      "Loss: 3.6583304405212402\n",
      "Loss: 3.657794237136841\n",
      "Loss: 3.513164520263672\n",
      "Loss: 3.5857391357421875\n",
      "Loss: 3.967170476913452\n",
      "Loss: 3.8021740913391113\n",
      "Loss: 3.558966636657715\n",
      "Loss: 3.438602924346924\n",
      "Loss: 3.6493606567382812\n",
      "Loss: 3.7013444900512695\n",
      "Loss: 3.7262229919433594\n",
      "Loss: 3.6984915733337402\n",
      "Loss: 4.0752034187316895\n",
      "Loss: 4.131904125213623\n",
      "Loss: 3.6265206336975098\n",
      "Loss: 3.547544002532959\n",
      "Loss: 3.6702158451080322\n",
      "Loss: 3.83306622505188\n",
      "Loss: 3.3604230880737305\n",
      "Loss: 3.7205755710601807\n",
      "Loss: 3.8689515590667725\n",
      "Loss: 4.104371070861816\n",
      "Loss: 3.734431743621826\n",
      "Loss: 3.5600812435150146\n",
      "Loss: 3.993001699447632\n",
      "Loss: 3.9093687534332275\n",
      "Loss: 3.915534257888794\n",
      "Loss: 3.99752140045166\n",
      "Loss: 3.8345203399658203\n",
      "Loss: 4.212942600250244\n",
      "Loss: 3.8715691566467285\n",
      "Loss: 3.8481218814849854\n",
      "Loss: 3.5229992866516113\n",
      "Loss: 3.5575618743896484\n",
      "Loss: 3.6997406482696533\n",
      "Loss: 3.857367992401123\n",
      "Loss: 3.578312873840332\n",
      "Loss: 3.895278215408325\n",
      "Loss: 3.566314697265625\n",
      "Loss: 3.4628405570983887\n",
      "Loss: 3.9060630798339844\n",
      "Loss: 3.8062524795532227\n",
      "Loss: 3.6995022296905518\n",
      "Loss: 3.781216621398926\n",
      "Loss: 4.031403064727783\n",
      "Loss: 3.3820300102233887\n",
      "Loss: 3.951414108276367\n",
      "Loss: 3.3188791275024414\n",
      "Loss: 3.8630428314208984\n",
      "Loss: 3.3577630519866943\n",
      "Loss: 4.017014980316162\n",
      "Loss: 3.737837791442871\n",
      "Loss: 3.8626656532287598\n",
      "Loss: 4.080124378204346\n",
      "Loss: 3.361069440841675\n",
      "Loss: 3.7035372257232666\n",
      "Loss: 3.850541591644287\n",
      "Loss: 3.706822633743286\n",
      "Loss: 3.583641290664673\n",
      "Loss: 3.700493335723877\n",
      "Loss: 3.530625820159912\n",
      "Loss: 3.8427093029022217\n",
      "Loss: 3.9218266010284424\n",
      "Loss: 3.6196722984313965\n",
      "Loss: 3.556734561920166\n",
      "Loss: 3.7327399253845215\n",
      "Loss: 3.5632288455963135\n",
      "Loss: 3.7236995697021484\n",
      "Loss: 3.547924280166626\n",
      "Loss: 3.9158430099487305\n",
      "Loss: 3.8982036113739014\n",
      "Loss: 3.7753984928131104\n",
      "Loss: 3.3259618282318115\n",
      "Loss: 4.228522300720215\n",
      "Loss: 3.1280059814453125\n",
      "Loss: 3.748046398162842\n",
      "Loss: 3.53725266456604\n",
      "Loss: 3.309234380722046\n",
      "Loss: 3.9259774684906006\n",
      "Loss: 3.9283533096313477\n",
      "Loss: 3.5551910400390625\n",
      "Loss: 3.481426477432251\n",
      "Loss: 4.0204620361328125\n",
      "Loss: 3.8271071910858154\n",
      "Loss: 3.888033628463745\n",
      "Loss: 3.8053698539733887\n",
      "Loss: 3.6957385540008545\n",
      "Loss: 3.6859333515167236\n",
      "Loss: 3.8592145442962646\n",
      "Loss: 3.483386754989624\n",
      "Loss: 3.7154362201690674\n",
      "Loss: 3.5149428844451904\n",
      "Loss: 3.970853090286255\n",
      "Loss: 3.912191867828369\n",
      "Loss: 3.089722156524658\n",
      "Loss: 3.66898775100708\n",
      "Loss: 3.956697463989258\n",
      "Loss: 3.9896035194396973\n",
      "Loss: 3.23110294342041\n",
      "Loss: 3.9584972858428955\n",
      "Loss: 4.406242847442627\n",
      "Loss: 3.4704971313476562\n",
      "Loss: 3.623314619064331\n",
      "Loss: 3.793605327606201\n",
      "Loss: 3.4428281784057617\n",
      "Loss: 3.5678110122680664\n",
      "Loss: 3.592266321182251\n",
      "Loss: 3.906113386154175\n",
      "Loss: 4.070068836212158\n",
      "Loss: 3.575144052505493\n",
      "Loss: 3.7532858848571777\n",
      "Loss: 3.501640558242798\n",
      "Loss: 3.6396520137786865\n",
      "Loss: 3.587444305419922\n",
      "Loss: 3.5787792205810547\n",
      "Loss: 3.853778600692749\n",
      "Loss: 3.7193195819854736\n",
      "Loss: 4.199214935302734\n",
      "Loss: 3.869410514831543\n",
      "Loss: 3.717865467071533\n",
      "Loss: 3.993567705154419\n",
      "Loss: 3.760640859603882\n",
      "Loss: 3.7289676666259766\n",
      "Loss: 3.7913007736206055\n",
      "Loss: 3.6814844608306885\n",
      "Loss: 3.6945903301239014\n",
      "Loss: 3.797741413116455\n",
      "Loss: 3.412297487258911\n",
      "Loss: 3.682209014892578\n",
      "Loss: 3.6746275424957275\n",
      "Loss: 3.8269152641296387\n",
      "Loss: 3.6016128063201904\n",
      "Loss: 4.092635154724121\n",
      "Loss: 3.728203058242798\n",
      "Loss: 3.7720835208892822\n",
      "Loss: 4.036828994750977\n",
      "Loss: 3.789705753326416\n",
      "Loss: 3.636565685272217\n",
      "Loss: 3.548923969268799\n",
      "Loss: 3.645768404006958\n",
      "Loss: 3.9576923847198486\n",
      "Loss: 3.5392913818359375\n",
      "Loss: 3.5666229724884033\n",
      "Loss: 3.82635498046875\n",
      "Loss: 3.471118927001953\n",
      "Loss: 3.9635324478149414\n",
      "Loss: 3.8913967609405518\n",
      "Loss: 3.8335165977478027\n",
      "Loss: 4.008513927459717\n",
      "Loss: 3.7020134925842285\n",
      "Loss: 3.596184253692627\n",
      "Loss: 4.049530982971191\n",
      "Loss: 3.3793275356292725\n",
      "Loss: 4.0167951583862305\n",
      "Loss: 3.6321606636047363\n",
      "Loss: 3.9472389221191406\n",
      "Loss: 3.6183080673217773\n",
      "Loss: 4.115286827087402\n",
      "Loss: 3.6569883823394775\n",
      "Loss: 3.3774232864379883\n",
      "Loss: 3.7353129386901855\n",
      "Loss: 3.5917389392852783\n",
      "Loss: 3.6004250049591064\n",
      "Loss: 4.044491291046143\n",
      "Loss: 4.116237640380859\n",
      "Loss: 3.560838222503662\n",
      "Loss: 4.032569885253906\n",
      "Loss: 4.237332820892334\n",
      "Loss: 4.0700860023498535\n",
      "Loss: 4.024969100952148\n",
      "Loss: 3.4453060626983643\n",
      "Loss: 3.695331573486328\n",
      "Loss: 3.6076316833496094\n",
      "Loss: 3.868303060531616\n",
      "Loss: 3.5470030307769775\n",
      "Loss: 4.011122703552246\n",
      "Loss: 3.997300624847412\n",
      "Loss: 3.932051658630371\n",
      "Loss: 3.7544233798980713\n",
      "Loss: 3.310342788696289\n",
      "Loss: 3.421353578567505\n",
      "Loss: 3.8079049587249756\n",
      "Loss: 3.739006519317627\n",
      "Loss: 3.9145328998565674\n",
      "Loss: 3.4503231048583984\n",
      "Loss: 3.727108955383301\n",
      "Loss: 4.306391716003418\n",
      "Loss: 3.6814754009246826\n",
      "Loss: 3.564640760421753\n",
      "Loss: 3.652723789215088\n",
      "Loss: 3.976869583129883\n",
      "Loss: 3.7297353744506836\n",
      "Loss: 3.717406749725342\n",
      "Loss: 3.5505552291870117\n",
      "Loss: 3.3899223804473877\n",
      "Loss: 4.135668754577637\n",
      "Loss: 3.9904072284698486\n",
      "Loss: 3.9251508712768555\n",
      "Loss: 3.7925496101379395\n",
      "Loss: 3.8823347091674805\n",
      "Loss: 3.924175977706909\n",
      "Loss: 3.7030320167541504\n",
      "Loss: 3.461463212966919\n",
      "Loss: 3.1360561847686768\n",
      "Loss: 3.553494453430176\n",
      "Loss: 3.9029033184051514\n",
      "Loss: 3.442244052886963\n",
      "Loss: 3.198423385620117\n",
      "Loss: 3.732795476913452\n",
      "Loss: 3.6819417476654053\n",
      "Loss: 3.8771934509277344\n",
      "Loss: 3.6668295860290527\n",
      "Loss: 3.3499274253845215\n",
      "Loss: 3.559541702270508\n",
      "Loss: 3.598179817199707\n",
      "Loss: 3.674722194671631\n",
      "Loss: 3.627920150756836\n",
      "Loss: 3.6147189140319824\n",
      "Loss: 4.05121374130249\n",
      "Loss: 3.7496118545532227\n",
      "Loss: 3.1572835445404053\n",
      "Loss: 3.6033480167388916\n",
      "Loss: 3.1563808917999268\n",
      "Loss: 3.6411330699920654\n",
      "Loss: 3.882981061935425\n",
      "Loss: 3.7099852561950684\n",
      "Loss: 3.670991897583008\n",
      "Loss: 3.5229392051696777\n",
      "Loss: 3.6994333267211914\n",
      "Loss: 3.748602867126465\n",
      "Loss: 3.121626615524292\n",
      "Loss: 3.0836591720581055\n",
      "Loss: 3.440648078918457\n",
      "Loss: 3.9156315326690674\n",
      "Loss: 3.7597548961639404\n",
      "Loss: 4.031075477600098\n",
      "Loss: 4.05987548828125\n",
      "Loss: 3.820891857147217\n",
      "Loss: 3.714012384414673\n",
      "Loss: 3.255434274673462\n",
      "Loss: 3.6071648597717285\n",
      "Loss: 3.8599159717559814\n",
      "Loss: 3.6258347034454346\n",
      "Loss: 3.949385404586792\n",
      "Loss: 3.5285489559173584\n",
      "Loss: 3.665660858154297\n",
      "Loss: 3.5687263011932373\n",
      "Loss: 3.688297748565674\n",
      "Loss: 3.673393964767456\n",
      "Loss: 3.660710096359253\n",
      "Loss: 3.5154452323913574\n",
      "Loss: 3.6454646587371826\n",
      "Loss: 3.617403507232666\n",
      "Loss: 3.714787483215332\n",
      "Loss: 3.666659116744995\n",
      "Loss: 3.4387459754943848\n",
      "Loss: 3.5459818840026855\n",
      "Loss: 3.4252383708953857\n",
      "Loss: 3.687729597091675\n",
      "Loss: 3.838094472885132\n",
      "Loss: 3.4472568035125732\n",
      "Loss: 3.4737040996551514\n",
      "Loss: 3.775437593460083\n",
      "Loss: 3.398768186569214\n",
      "Loss: 3.851010322570801\n",
      "Loss: 3.909879684448242\n",
      "Loss: 4.211373329162598\n",
      "Loss: 3.112903356552124\n",
      "Loss: 3.299349784851074\n",
      "Loss: 3.484848737716675\n",
      "Loss: 3.7818572521209717\n",
      "Loss: 3.7103404998779297\n",
      "Loss: 3.6772501468658447\n",
      "Loss: 3.5756709575653076\n",
      "Loss: 4.086654186248779\n",
      "Loss: 3.660578966140747\n",
      "Loss: 3.686197280883789\n",
      "Loss: 3.451894998550415\n",
      "Loss: 3.6659600734710693\n",
      "Loss: 3.9160988330841064\n",
      "Loss: 3.973005533218384\n",
      "Loss: 3.7050859928131104\n",
      "Loss: 3.5857765674591064\n",
      "Loss: 3.9248619079589844\n",
      "Loss: 3.437406063079834\n",
      "Loss: 3.7677342891693115\n",
      "Loss: 3.359050989151001\n",
      "Loss: 3.6643989086151123\n",
      "Loss: 3.464063882827759\n",
      "Loss: 3.530022382736206\n",
      "Loss: 3.5210773944854736\n",
      "Loss: 3.6611058712005615\n",
      "Loss: 3.76139760017395\n",
      "Loss: 3.610849618911743\n",
      "Loss: 3.1092569828033447\n",
      "Loss: 3.6024632453918457\n",
      "Loss: 3.54237699508667\n",
      "Loss: 3.2673516273498535\n",
      "Loss: 4.160601615905762\n",
      "Loss: 3.336061477661133\n",
      "Loss: 4.081509590148926\n",
      "Loss: 3.7889339923858643\n",
      "Loss: 3.666409730911255\n",
      "Loss: 3.683485269546509\n",
      "Loss: 3.7230095863342285\n",
      "Loss: 3.6727633476257324\n",
      "Loss: 3.6046669483184814\n",
      "Loss: 3.401721477508545\n",
      "Loss: 3.7481796741485596\n",
      "Loss: 4.033742427825928\n",
      "Loss: 3.2362499237060547\n",
      "Loss: 3.515726089477539\n",
      "Loss: 3.5062146186828613\n",
      "Loss: 3.8166892528533936\n",
      "Loss: 3.5841331481933594\n",
      "Loss: 3.9158122539520264\n",
      "Loss: 4.0369415283203125\n",
      "Loss: 3.927290439605713\n",
      "Loss: 3.651909828186035\n",
      "Loss: 3.662736177444458\n",
      "Loss: 3.707045078277588\n",
      "Loss: 3.7748489379882812\n",
      "Loss: 3.932347297668457\n",
      "Loss: 3.5284805297851562\n",
      "Loss: 3.4753994941711426\n",
      "Loss: 4.000289440155029\n",
      "Loss: 3.7292661666870117\n",
      "Loss: 3.4511733055114746\n",
      "Loss: 3.513139486312866\n",
      "Loss: 3.5693118572235107\n",
      "Loss: 3.5461232662200928\n",
      "Loss: 3.9093799591064453\n",
      "Loss: 4.178845405578613\n",
      "Loss: 3.7656607627868652\n",
      "Loss: 3.487088680267334\n",
      "Loss: 3.4923043251037598\n",
      "Loss: 3.3930253982543945\n",
      "Loss: 3.4973464012145996\n",
      "Loss: 3.3422205448150635\n",
      "Loss: 3.7176733016967773\n",
      "Loss: 3.893860101699829\n",
      "Loss: 3.597982883453369\n",
      "Loss: 4.030514717102051\n",
      "Loss: 3.517899513244629\n",
      "Loss: 3.8482112884521484\n",
      "Loss: 3.9565632343292236\n",
      "Loss: 3.791670322418213\n",
      "Loss: 3.733856678009033\n",
      "Loss: 3.285792589187622\n",
      "Loss: 3.815553903579712\n",
      "Loss: 3.429065227508545\n",
      "Loss: 3.809691905975342\n",
      "Loss: 3.616764783859253\n",
      "Loss: 3.6161913871765137\n",
      "Loss: 3.4833827018737793\n",
      "Loss: 3.8000869750976562\n",
      "Loss: 3.123079299926758\n",
      "Loss: 3.454568862915039\n",
      "Loss: 3.576254367828369\n",
      "Loss: 3.883152723312378\n",
      "Loss: 3.661278247833252\n",
      "Loss: 3.479684352874756\n",
      "Loss: 3.332472562789917\n",
      "Loss: 3.529416799545288\n",
      "Loss: 3.9242160320281982\n",
      "Loss: 3.872951030731201\n",
      "Loss: 3.791853189468384\n",
      "Loss: 3.720694065093994\n",
      "Loss: 3.6216909885406494\n",
      "Loss: 3.8083925247192383\n",
      "Loss: 3.5871543884277344\n",
      "Loss: 3.3806302547454834\n",
      "Loss: 3.778127908706665\n",
      "Loss: 3.546323537826538\n",
      "Loss: 3.573732614517212\n",
      "Loss: 3.6464903354644775\n",
      "Loss: 3.3854589462280273\n",
      "Loss: 3.657020092010498\n",
      "Loss: 3.87678861618042\n",
      "Loss: 3.3359973430633545\n",
      "Loss: 3.8509674072265625\n",
      "Loss: 3.9211573600769043\n",
      "Loss: 3.4890174865722656\n",
      "Loss: 3.4370458126068115\n",
      "Loss: 3.8052923679351807\n",
      "Loss: 3.763324737548828\n",
      "Loss: 3.5701980590820312\n",
      "Loss: 3.370204210281372\n",
      "Loss: 3.727328300476074\n",
      "Loss: 3.7439229488372803\n",
      "Loss: 4.0176920890808105\n",
      "Loss: 3.4506475925445557\n",
      "Loss: 3.541043758392334\n",
      "Loss: 3.8241560459136963\n",
      "Loss: 3.543013095855713\n",
      "Loss: 3.3472654819488525\n",
      "Loss: 3.6556525230407715\n",
      "Loss: 3.87076997756958\n",
      "Loss: 3.501314401626587\n",
      "Loss: 3.413747549057007\n",
      "Loss: 3.4161601066589355\n",
      "Loss: 3.6924805641174316\n",
      "Loss: 3.5753679275512695\n",
      "Loss: 3.5902202129364014\n",
      "Loss: 3.4000277519226074\n",
      "Loss: 3.324286699295044\n",
      "Loss: 3.8442540168762207\n",
      "Loss: 3.6130027770996094\n",
      "Loss: 3.6504592895507812\n",
      "Loss: 3.599024534225464\n",
      "Loss: 3.5508456230163574\n",
      "Loss: 3.3138978481292725\n",
      "Loss: 3.5687925815582275\n",
      "Loss: 3.567185640335083\n",
      "Loss: 3.9761135578155518\n",
      "Loss: 3.2717196941375732\n",
      "Loss: 3.1353907585144043\n",
      "Loss: 3.5163848400115967\n",
      "Loss: 3.7332117557525635\n",
      "Loss: 3.508023500442505\n",
      "Loss: 3.528724193572998\n",
      "Loss: 3.7647478580474854\n",
      "Loss: 3.7205750942230225\n",
      "Loss: 3.423100471496582\n",
      "Loss: 3.463965654373169\n",
      "Loss: 3.6594533920288086\n",
      "Loss: 3.291100025177002\n",
      "Loss: 3.6956582069396973\n",
      "Loss: 4.013433933258057\n",
      "Loss: 3.4433934688568115\n",
      "Loss: 3.5387914180755615\n",
      "Loss: 3.2337684631347656\n",
      "Loss: 3.668098211288452\n",
      "Loss: 3.842679023742676\n",
      "Loss: 3.5472285747528076\n",
      "Loss: 3.5724966526031494\n",
      "Loss: 3.6876423358917236\n",
      "Loss: 3.7654590606689453\n",
      "Loss: 3.4730114936828613\n",
      "Loss: 3.5593464374542236\n",
      "Loss: 3.8789610862731934\n",
      "Loss: 3.6952877044677734\n",
      "Loss: 3.6908328533172607\n",
      "Loss: 3.6230580806732178\n",
      "Loss: 3.765367269515991\n",
      "Loss: 3.2130796909332275\n",
      "Loss: 3.7269444465637207\n",
      "Loss: 3.2787153720855713\n",
      "Loss: 3.5600595474243164\n",
      "Loss: 3.545848846435547\n",
      "Loss: 4.19206428527832\n",
      "Loss: 3.7465202808380127\n",
      "Loss: 3.9118642807006836\n",
      "Loss: 3.2792530059814453\n",
      "Loss: 3.767224073410034\n",
      "Loss: 3.906026601791382\n",
      "Loss: 3.9605038166046143\n",
      "Loss: 3.6219067573547363\n",
      "Loss: 3.564666748046875\n",
      "Loss: 3.6995186805725098\n",
      "Loss: 3.5902106761932373\n",
      "Loss: 3.5800795555114746\n",
      "Loss: 3.8149545192718506\n",
      "Loss: 3.570708751678467\n",
      "Loss: 3.795633554458618\n",
      "Loss: 3.9007298946380615\n",
      "Loss: 3.4323859214782715\n",
      "Loss: 3.809372901916504\n",
      "Loss: 3.810648202896118\n",
      "Loss: 3.224205493927002\n",
      "Loss: 3.1243107318878174\n",
      "Loss: 3.215397834777832\n",
      "Loss: 3.4489078521728516\n",
      "Loss: 3.5729291439056396\n",
      "Loss: 3.729963779449463\n",
      "Loss: 3.694164276123047\n",
      "Loss: 3.707590341567993\n",
      "Loss: 3.843923807144165\n",
      "Loss: 3.7866907119750977\n",
      "Loss: 3.3838322162628174\n",
      "Loss: 3.618513822555542\n",
      "Loss: 3.3977549076080322\n",
      "Loss: 3.8129494190216064\n",
      "Loss: 3.512120246887207\n",
      "Loss: 3.2943689823150635\n",
      "Loss: 3.8795158863067627\n",
      "Loss: 3.5412166118621826\n",
      "Loss: 3.4271678924560547\n",
      "Loss: 3.5337777137756348\n",
      "Loss: 3.553130626678467\n",
      "Loss: 3.98455810546875\n",
      "Loss: 3.634798526763916\n",
      "Loss: 3.4023706912994385\n",
      "Loss: 3.822110176086426\n",
      "Loss: 3.599583387374878\n",
      "Loss: 3.2842772006988525\n",
      "Loss: 3.646074056625366\n",
      "Loss: 3.60754656791687\n",
      "Loss: 3.31221079826355\n",
      "Loss: 3.6821510791778564\n",
      "Loss: 3.52346134185791\n",
      "Loss: 3.168976306915283\n",
      "Loss: 3.3930258750915527\n",
      "Loss: 3.456138849258423\n",
      "Loss: 3.4851198196411133\n",
      "Loss: 3.7766294479370117\n",
      "Loss: 3.555947780609131\n",
      "Loss: 3.3677661418914795\n",
      "Loss: 3.468759298324585\n",
      "Loss: 3.9001259803771973\n",
      "Loss: 3.7657623291015625\n",
      "Loss: 3.5527570247650146\n",
      "Loss: 3.4225871562957764\n",
      "Loss: 3.7461936473846436\n",
      "Loss: 3.7911672592163086\n",
      "Loss: 3.5287208557128906\n",
      "Loss: 3.6120684146881104\n",
      "Loss: 3.532435655593872\n",
      "Loss: 3.7820262908935547\n",
      "Loss: 3.5034191608428955\n",
      "Loss: 3.3851304054260254\n",
      "Loss: 3.4469234943389893\n",
      "Loss: 3.3866872787475586\n",
      "Loss: 3.7776403427124023\n",
      "Loss: 3.8188140392303467\n",
      "Loss: 3.738243579864502\n",
      "Loss: 3.6199417114257812\n",
      "Loss: 3.7283921241760254\n",
      "Loss: 3.932968854904175\n",
      "Loss: 3.660111665725708\n",
      "Loss: 3.477909803390503\n",
      "Loss: 3.7079250812530518\n",
      "Loss: 3.3114206790924072\n",
      "Loss: 3.872420072555542\n",
      "Loss: 3.21541690826416\n",
      "Loss: 3.545722723007202\n",
      "Loss: 3.5133299827575684\n",
      "Loss: 3.71820068359375\n",
      "Loss: 3.7718663215637207\n",
      "Loss: 3.9946765899658203\n",
      "Loss: 3.651886224746704\n",
      "Loss: 3.669666290283203\n",
      "Loss: 3.0528135299682617\n",
      "Loss: 3.8294003009796143\n",
      "Loss: 3.5548038482666016\n",
      "Loss: 3.694768190383911\n",
      "Loss: 3.7646515369415283\n",
      "Loss: 3.6205177307128906\n",
      "Loss: 3.738753318786621\n",
      "Loss: 3.247455596923828\n",
      "Loss: 3.534945249557495\n",
      "Loss: 3.4530160427093506\n",
      "Loss: 3.527287006378174\n",
      "Loss: 3.9256691932678223\n",
      "Loss: 3.521117687225342\n",
      "Loss: 3.4672720432281494\n",
      "Loss: 3.546630859375\n",
      "Loss: 3.4748568534851074\n",
      "Loss: 3.3208415508270264\n",
      "Loss: 3.302117347717285\n",
      "Loss: 3.8601245880126953\n",
      "Loss: 3.069889545440674\n",
      "Loss: 3.364348888397217\n",
      "Loss: 3.081587314605713\n",
      "Loss: 3.3902506828308105\n",
      "Loss: 4.034265041351318\n",
      "Loss: 4.097772598266602\n",
      "Loss: 3.1065101623535156\n",
      "Loss: 3.9827795028686523\n",
      "Loss: 3.5950276851654053\n",
      "Loss: 3.654977321624756\n",
      "Loss: 3.5235435962677\n",
      "Loss: 3.230717658996582\n",
      "Loss: 3.975977659225464\n",
      "Loss: 4.189135551452637\n",
      "Loss: 3.6984639167785645\n",
      "Loss: 3.569383144378662\n",
      "Loss: 3.606732130050659\n",
      "Loss: 3.1995513439178467\n",
      "Loss: 3.4691216945648193\n",
      "Loss: 3.3863184452056885\n",
      "Loss: 3.520732879638672\n",
      "Loss: 3.880596399307251\n",
      "Loss: 3.393264055252075\n",
      "Loss: 3.7306394577026367\n",
      "Loss: 3.485142946243286\n",
      "Loss: 3.4350063800811768\n",
      "Loss: 3.618447780609131\n",
      "Loss: 3.523789405822754\n",
      "Loss: 3.4964513778686523\n",
      "Loss: 3.2562389373779297\n",
      "Loss: 3.414562225341797\n",
      "Loss: 3.6219496726989746\n",
      "Loss: 3.685661554336548\n",
      "Loss: 3.968554735183716\n",
      "Loss: 3.711418390274048\n",
      "Loss: 3.5571866035461426\n",
      "Loss: 3.1158664226531982\n",
      "Loss: 3.0631253719329834\n",
      "Loss: 3.575927495956421\n",
      "Loss: 3.823004722595215\n",
      "Loss: 3.5950212478637695\n",
      "Loss: 3.454181432723999\n",
      "Loss: 3.444901466369629\n",
      "Loss: 3.645949125289917\n",
      "Loss: 3.327627658843994\n",
      "Loss: 3.3479883670806885\n",
      "Loss: 3.151951789855957\n",
      "Loss: 3.564405679702759\n",
      "Loss: 3.567044496536255\n",
      "Loss: 3.945570468902588\n",
      "Loss: 3.1884775161743164\n",
      "Loss: 3.4727563858032227\n",
      "Loss: 3.9178659915924072\n",
      "Loss: 3.3987390995025635\n",
      "Loss: 3.4451379776000977\n",
      "Loss: 3.5714478492736816\n",
      "Loss: 3.3997442722320557\n",
      "Loss: 3.8596603870391846\n",
      "Loss: 3.2364463806152344\n",
      "Loss: 3.418374538421631\n",
      "Loss: 3.80169939994812\n",
      "Loss: 4.111762046813965\n",
      "Loss: 3.393056869506836\n",
      "Loss: 3.2136881351470947\n",
      "Loss: 3.4565091133117676\n",
      "Loss: 3.428138017654419\n",
      "Loss: 3.5129013061523438\n",
      "Loss: 3.4734718799591064\n",
      "Loss: 3.5953941345214844\n",
      "Loss: 3.5984432697296143\n",
      "Loss: 3.6719682216644287\n",
      "Loss: 3.4466745853424072\n",
      "Loss: 3.2798407077789307\n",
      "Loss: 3.7927820682525635\n",
      "Loss: 3.3118226528167725\n",
      "Loss: 3.1712594032287598\n",
      "Loss: 3.6764817237854004\n",
      "Loss: 3.5076286792755127\n",
      "Loss: 3.6748881340026855\n",
      "Loss: 3.824326276779175\n",
      "Loss: 3.6261401176452637\n",
      "Loss: 3.7327845096588135\n",
      "Loss: 3.598818302154541\n",
      "Loss: 3.626305341720581\n",
      "Loss: 3.6519224643707275\n",
      "Loss: 3.5717740058898926\n",
      "Loss: 3.582040786743164\n",
      "Loss: 3.7528462409973145\n",
      "Loss: 3.712312698364258\n",
      "Loss: 3.4218294620513916\n",
      "Loss: 3.4537665843963623\n",
      "Loss: 3.2592787742614746\n",
      "Loss: 3.3277666568756104\n",
      "Loss: 3.441354513168335\n",
      "Loss: 3.375887155532837\n",
      "Loss: 3.325326681137085\n",
      "Loss: 3.4881787300109863\n",
      "Loss: 3.5935122966766357\n",
      "Loss: 3.711992025375366\n",
      "Loss: 3.180309295654297\n",
      "Loss: 3.298821449279785\n",
      "Loss: 3.9992549419403076\n",
      "Loss: 3.8543283939361572\n",
      "Loss: 3.5085155963897705\n",
      "Loss: 3.2333102226257324\n",
      "Loss: 3.5053000450134277\n",
      "Loss: 3.487074375152588\n",
      "Loss: 3.363032817840576\n",
      "Loss: 3.5738344192504883\n",
      "Loss: 3.8159823417663574\n",
      "Loss: 3.493485689163208\n",
      "Loss: 3.707399368286133\n",
      "Loss: 3.5066967010498047\n",
      "Loss: 3.6311750411987305\n",
      "Loss: 3.317490816116333\n",
      "Loss: 3.454719066619873\n",
      "Loss: 3.5368402004241943\n",
      "Loss: 3.6391167640686035\n",
      "Loss: 3.5257232189178467\n",
      "Loss: 3.4763858318328857\n",
      "Loss: 3.267423629760742\n",
      "Loss: 3.944737672805786\n",
      "Loss: 3.850998640060425\n",
      "Loss: 3.7236077785491943\n",
      "Loss: 3.721221923828125\n",
      "Loss: 3.3972957134246826\n",
      "Loss: 3.482107400894165\n",
      "Loss: 3.7639143466949463\n",
      "Loss: 3.521350383758545\n",
      "Loss: 3.472553253173828\n",
      "Loss: 3.3447680473327637\n",
      "Loss: 3.3263754844665527\n",
      "Loss: 3.4451851844787598\n",
      "Loss: 3.3366057872772217\n",
      "Loss: 3.354994535446167\n",
      "Loss: 3.533478021621704\n",
      "Loss: 3.3368773460388184\n",
      "Loss: 3.3545963764190674\n",
      "Loss: 3.25746488571167\n",
      "Loss: 3.633394718170166\n",
      "Loss: 3.2118942737579346\n",
      "Loss: 3.4704055786132812\n",
      "Loss: 3.5563547611236572\n",
      "Loss: 3.279405117034912\n",
      "Loss: 3.312185525894165\n",
      "Loss: 3.2748517990112305\n",
      "Loss: 3.5843505859375\n",
      "Loss: 3.7393298149108887\n",
      "Loss: 3.4219579696655273\n",
      "Loss: 4.015239715576172\n",
      "Loss: 3.4665307998657227\n",
      "Loss: 3.339545965194702\n",
      "Loss: 3.9580821990966797\n",
      "Loss: 3.6709232330322266\n",
      "Loss: 3.6427855491638184\n",
      "Loss: 3.6904397010803223\n",
      "Loss: 3.8227739334106445\n",
      "Loss: 3.186072826385498\n",
      "Loss: 3.506462574005127\n",
      "Loss: 3.923649311065674\n",
      "Loss: 3.3313097953796387\n",
      "Loss: 3.5180511474609375\n",
      "Loss: 3.5913174152374268\n",
      "Loss: 3.521656036376953\n",
      "Loss: 3.4530932903289795\n",
      "Loss: 3.132513999938965\n",
      "Loss: 3.6324331760406494\n",
      "Loss: 3.189539670944214\n",
      "Loss: 3.6674535274505615\n",
      "Loss: 3.6276087760925293\n",
      "Loss: 3.496537923812866\n",
      "Loss: 3.9113471508026123\n",
      "Loss: 3.590945243835449\n",
      "Loss: 3.490978240966797\n",
      "Loss: 3.343069076538086\n",
      "Loss: 3.563349485397339\n",
      "Loss: 3.39821720123291\n",
      "Loss: 3.4146509170532227\n",
      "Loss: 3.5703492164611816\n",
      "Loss: 3.1587982177734375\n",
      "Loss: 3.810880184173584\n",
      "Loss: 3.6899805068969727\n",
      "Loss: 3.099246025085449\n",
      "Loss: 3.570408582687378\n",
      "Loss: 3.370327949523926\n",
      "Loss: 3.9029741287231445\n",
      "Loss: 3.4659690856933594\n",
      "Loss: 3.445521354675293\n",
      "Loss: 3.2257981300354004\n",
      "Loss: 3.557248115539551\n",
      "Loss: 3.0717499256134033\n",
      "Loss: 3.7912557125091553\n",
      "Loss: 3.3346376419067383\n",
      "Loss: 3.4101004600524902\n",
      "Loss: 3.0515823364257812\n",
      "Loss: 3.297893524169922\n",
      "Loss: 3.684680461883545\n",
      "Loss: 3.441725254058838\n",
      "Loss: 3.457411289215088\n",
      "Loss: 3.7148382663726807\n",
      "Loss: 3.360427141189575\n",
      "Loss: 3.6412415504455566\n",
      "Loss: 3.5065362453460693\n",
      "Loss: 3.5764763355255127\n",
      "Loss: 3.575723171234131\n",
      "Loss: 3.4286303520202637\n",
      "Loss: 3.26521372795105\n",
      "Loss: 3.369572162628174\n",
      "Loss: 3.3313095569610596\n",
      "Loss: 3.3771352767944336\n",
      "Loss: 3.7947514057159424\n",
      "Loss: 3.704956293106079\n",
      "Loss: 3.326730728149414\n",
      "Loss: 3.24545955657959\n",
      "Loss: 3.482020616531372\n",
      "Loss: 3.3372175693511963\n",
      "Loss: 3.3819916248321533\n",
      "Loss: 3.158116102218628\n",
      "Loss: 3.3828396797180176\n",
      "Loss: 3.102998733520508\n",
      "Loss: 3.608384132385254\n",
      "Loss: 3.4142608642578125\n",
      "Loss: 3.7315073013305664\n",
      "Loss: 3.905144453048706\n",
      "Loss: 3.62325382232666\n",
      "Loss: 3.676384925842285\n",
      "Loss: 3.800607204437256\n",
      "Loss: 3.6987128257751465\n",
      "Loss: 3.404954433441162\n",
      "Loss: 3.3256072998046875\n",
      "Loss: 3.2142632007598877\n",
      "Loss: 3.441258430480957\n",
      "Loss: 3.458993434906006\n",
      "Loss: 3.5725221633911133\n",
      "Loss: 3.802048683166504\n",
      "Loss: 3.5648677349090576\n",
      "Loss: 3.893955707550049\n",
      "Loss: 3.3805949687957764\n",
      "Loss: 3.431959390640259\n",
      "Loss: 3.7204489707946777\n",
      "Loss: 3.3234496116638184\n",
      "Loss: 3.332242965698242\n",
      "Loss: 3.374675989151001\n",
      "Loss: 2.9184563159942627\n",
      "Loss: 3.286653995513916\n",
      "Loss: 3.1783206462860107\n",
      "Loss: 3.392164468765259\n",
      "Loss: 3.5449557304382324\n",
      "Loss: 3.5951437950134277\n",
      "Loss: 3.2763404846191406\n",
      "Loss: 3.755634307861328\n",
      "Loss: 3.7782843112945557\n",
      "Loss: 3.658433675765991\n",
      "Loss: 3.9740846157073975\n",
      "Loss: 3.401106834411621\n",
      "Loss: 3.9527952671051025\n",
      "Loss: 3.8594143390655518\n",
      "Loss: 3.584707736968994\n",
      "Loss: 3.6951286792755127\n",
      "Loss: 3.2758545875549316\n",
      "Loss: 3.4851834774017334\n",
      "Loss: 3.598419666290283\n",
      "Loss: 3.5421531200408936\n",
      "Loss: 3.8741989135742188\n",
      "Loss: 3.5220947265625\n",
      "Loss: 3.238802194595337\n",
      "Loss: 3.2035090923309326\n",
      "Loss: 3.6353964805603027\n",
      "Loss: 3.471338987350464\n",
      "Loss: 3.443016529083252\n",
      "Loss: 3.6203060150146484\n",
      "Loss: 3.616485118865967\n",
      "Loss: 3.496135711669922\n",
      "Loss: 3.6916375160217285\n",
      "Loss: 3.3431529998779297\n",
      "Loss: 3.629265069961548\n",
      "Loss: 3.3542068004608154\n",
      "Loss: 3.3579905033111572\n",
      "Loss: 3.523627758026123\n",
      "Loss: 3.465221405029297\n",
      "Loss: 3.479569911956787\n",
      "Loss: 3.7725937366485596\n",
      "Loss: 3.6180992126464844\n",
      "Loss: 3.4137556552886963\n",
      "Loss: 3.377018451690674\n",
      "Loss: 3.2305238246917725\n",
      "Loss: 3.455660104751587\n",
      "Loss: 3.3134260177612305\n",
      "Loss: 3.237236738204956\n",
      "Loss: 3.6637654304504395\n",
      "Loss: 3.411381959915161\n",
      "Loss: 3.240778684616089\n",
      "Loss: 3.7778706550598145\n",
      "Loss: 3.356400728225708\n",
      "Loss: 3.5277225971221924\n",
      "Loss: 3.520449638366699\n",
      "Loss: 3.221013069152832\n",
      "Loss: 3.392848014831543\n",
      "Loss: 3.618865489959717\n",
      "Loss: 3.5864474773406982\n",
      "Loss: 3.4786245822906494\n",
      "Loss: 3.584689140319824\n",
      "Loss: 3.4750471115112305\n",
      "Loss: 3.375657081604004\n",
      "Loss: 3.467552661895752\n",
      "Loss: 3.722752094268799\n",
      "Loss: 3.7232561111450195\n",
      "Loss: 3.8728816509246826\n",
      "Loss: 3.276707649230957\n",
      "Loss: 3.1978766918182373\n",
      "Loss: 3.6427059173583984\n",
      "Loss: 3.488722562789917\n",
      "Loss: 3.3649685382843018\n",
      "Loss: 3.074723720550537\n",
      "Loss: 3.2905771732330322\n",
      "Loss: 3.5906097888946533\n",
      "Loss: 3.723059892654419\n",
      "Loss: 3.6344757080078125\n",
      "Loss: 3.2974610328674316\n",
      "Loss: 3.40222430229187\n",
      "Loss: 3.1552658081054688\n",
      "Loss: 3.505740165710449\n",
      "Loss: 3.079134941101074\n",
      "Loss: 3.427832841873169\n",
      "Loss: 3.902859687805176\n",
      "Loss: 3.4971938133239746\n",
      "Loss: 3.7473065853118896\n",
      "Loss: 3.870394706726074\n",
      "Loss: 3.5248544216156006\n",
      "Loss: 3.3976569175720215\n",
      "Loss: 3.282205104827881\n",
      "Loss: 3.4107134342193604\n",
      "Loss: 3.4596056938171387\n",
      "Loss: 3.231405735015869\n",
      "Loss: 3.4772963523864746\n",
      "Loss: 3.0379605293273926\n",
      "Loss: 3.015773057937622\n",
      "Loss: 3.2182416915893555\n",
      "Loss: 3.297966241836548\n",
      "Loss: 3.467250108718872\n",
      "Loss: 3.4902212619781494\n",
      "Loss: 3.404977798461914\n",
      "Loss: 2.8842387199401855\n",
      "Loss: 3.425489902496338\n",
      "Loss: 3.627209424972534\n",
      "Loss: 3.47471022605896\n",
      "Loss: 3.418445348739624\n",
      "Loss: 3.377464532852173\n",
      "Loss: 3.072934150695801\n",
      "Loss: 3.5065765380859375\n",
      "Loss: 3.4208984375\n",
      "Loss: 3.366173505783081\n",
      "Loss: 3.4689080715179443\n",
      "Loss: 3.8960413932800293\n",
      "Loss: 3.493011951446533\n",
      "Loss: 3.732968807220459\n",
      "Loss: 3.479660987854004\n",
      "Loss: 3.3923251628875732\n",
      "Loss: 3.2895867824554443\n",
      "Loss: 3.187748908996582\n",
      "Loss: 3.5937952995300293\n",
      "Loss: 3.4517884254455566\n",
      "Loss: 3.032472610473633\n",
      "Loss: 3.411698579788208\n",
      "Loss: 3.496513605117798\n",
      "Loss: 3.6418144702911377\n",
      "Loss: 3.680232048034668\n",
      "Loss: 3.546426296234131\n",
      "Loss: 3.655217409133911\n",
      "Loss: 3.9269680976867676\n",
      "Loss: 3.3058948516845703\n",
      "Loss: 3.1275131702423096\n",
      "Loss: 3.654712677001953\n",
      "Loss: 3.2268989086151123\n",
      "Loss: 3.3570492267608643\n",
      "Loss: 3.220963716506958\n",
      "Loss: 3.1974406242370605\n",
      "Loss: 3.30818247795105\n",
      "Loss: 2.88100528717041\n",
      "Loss: 3.370765209197998\n",
      "Loss: 3.3425724506378174\n",
      "Loss: 3.7438831329345703\n",
      "Loss: 3.1678476333618164\n",
      "Loss: 3.2679529190063477\n",
      "Loss: 3.0786402225494385\n",
      "Loss: 3.5493686199188232\n",
      "Loss: 3.727307081222534\n",
      "Loss: 3.2671828269958496\n",
      "Loss: 3.092696189880371\n",
      "Loss: 3.5316975116729736\n",
      "Loss: 3.3112869262695312\n",
      "Loss: 3.2657105922698975\n",
      "Loss: 3.4756221771240234\n",
      "Loss: 3.0904791355133057\n",
      "Loss: 3.289562225341797\n",
      "Loss: 3.13808536529541\n",
      "Loss: 3.585681200027466\n",
      "Loss: 3.070394992828369\n",
      "Loss: 3.9398951530456543\n",
      "Loss: 3.4326565265655518\n",
      "Loss: 3.834653615951538\n",
      "Loss: 3.6290602684020996\n",
      "Loss: 3.5861656665802\n",
      "Loss: 3.310279607772827\n",
      "Loss: 3.4128258228302\n",
      "Loss: 3.0951714515686035\n",
      "Loss: 3.540586233139038\n",
      "Loss: 3.7777633666992188\n",
      "Loss: 3.590409278869629\n",
      "Loss: 3.673452854156494\n",
      "Loss: 3.263521194458008\n",
      "Loss: 3.4289748668670654\n",
      "Loss: 3.645869255065918\n",
      "Loss: 3.300961494445801\n",
      "Loss: 2.967359781265259\n",
      "Loss: 3.7947301864624023\n",
      "Loss: 3.1925764083862305\n",
      "Loss: 3.52547025680542\n",
      "Loss: 3.528994083404541\n",
      "Loss: 3.6124157905578613\n",
      "Loss: 3.688112735748291\n",
      "Loss: 3.4056499004364014\n",
      "Loss: 3.255030393600464\n",
      "Loss: 3.5173251628875732\n",
      "Loss: 3.693152666091919\n",
      "Loss: 3.505115509033203\n",
      "Loss: 3.619863510131836\n",
      "Loss: 3.7563858032226562\n",
      "Loss: 3.8043220043182373\n",
      "Loss: 3.193230628967285\n",
      "Loss: 3.4987499713897705\n",
      "Loss: 2.8624374866485596\n",
      "Loss: 3.8333728313446045\n",
      "Loss: 3.363879919052124\n",
      "Loss: 3.4750139713287354\n",
      "Loss: 3.383317708969116\n",
      "Loss: 3.4251599311828613\n",
      "Loss: 3.4658477306365967\n",
      "Loss: 3.41473388671875\n",
      "Loss: 3.19954776763916\n",
      "Loss: 3.3717994689941406\n",
      "Loss: 3.4780428409576416\n",
      "Loss: 3.677868366241455\n",
      "Loss: 3.210170269012451\n",
      "Loss: 2.751835584640503\n",
      "Loss: 3.492640733718872\n",
      "Loss: 3.4076662063598633\n",
      "Loss: 3.2660927772521973\n",
      "Loss: 3.23116397857666\n",
      "Loss: 3.315734386444092\n",
      "Loss: 3.176778793334961\n",
      "Loss: 3.3805150985717773\n",
      "Loss: 3.2267098426818848\n",
      "Loss: 3.635983943939209\n",
      "Loss: 3.291041135787964\n",
      "Loss: 3.7871298789978027\n",
      "Loss: 3.9067165851593018\n",
      "Loss: 2.8180670738220215\n",
      "Loss: 3.3189284801483154\n",
      "Loss: 3.55606746673584\n",
      "Loss: 3.4086737632751465\n",
      "Loss: 3.493391513824463\n",
      "Loss: 3.304436445236206\n",
      "Loss: 3.332188844680786\n",
      "Loss: 3.9716320037841797\n",
      "Loss: 3.204481363296509\n",
      "Loss: 3.6186420917510986\n",
      "Loss: 3.509647846221924\n",
      "Loss: 3.2928411960601807\n",
      "Loss: 3.3160643577575684\n",
      "Loss: 3.3614749908447266\n",
      "Loss: 3.4315390586853027\n",
      "Loss: 3.5140316486358643\n",
      "Loss: 3.8824517726898193\n",
      "Loss: 3.316154956817627\n",
      "Loss: 3.580207109451294\n",
      "Loss: 3.091874837875366\n",
      "Loss: 3.3995161056518555\n",
      "Loss: 3.3488857746124268\n",
      "Loss: 3.4029171466827393\n",
      "Loss: 3.4861340522766113\n",
      "Loss: 3.3331117630004883\n",
      "Loss: 3.0864131450653076\n",
      "Loss: 3.1995046138763428\n",
      "Loss: 3.7230722904205322\n",
      "Loss: 3.458002805709839\n",
      "Loss: 3.4892120361328125\n",
      "Loss: 3.5464534759521484\n",
      "Loss: 3.532700777053833\n",
      "Loss: 3.07489275932312\n",
      "Loss: 3.5079171657562256\n",
      "Loss: 3.1820099353790283\n",
      "Loss: 3.6514546871185303\n",
      "Loss: 3.3871665000915527\n",
      "Loss: 3.736910343170166\n",
      "Loss: 3.1324527263641357\n",
      "Loss: 3.3161919116973877\n",
      "Loss: 3.4300661087036133\n",
      "Loss: 3.56642746925354\n",
      "Loss: 3.193193197250366\n",
      "Loss: 3.6062004566192627\n",
      "Loss: 3.7351086139678955\n",
      "Loss: 3.0852954387664795\n",
      "Loss: 3.775784969329834\n",
      "Loss: 3.575824737548828\n",
      "Loss: 3.5626626014709473\n",
      "Loss: 3.309644937515259\n",
      "Loss: 3.3592264652252197\n",
      "Loss: 3.2879586219787598\n",
      "Loss: 3.280503273010254\n",
      "Loss: 3.6105663776397705\n",
      "Loss: 3.654886245727539\n",
      "Loss: 3.6724469661712646\n",
      "Loss: 2.9875879287719727\n",
      "Loss: 3.4221129417419434\n",
      "Loss: 3.211764335632324\n",
      "Loss: 3.26733136177063\n",
      "Loss: 3.432433843612671\n",
      "Loss: 3.020768642425537\n",
      "Loss: 3.3942549228668213\n",
      "Loss: 3.6547744274139404\n",
      "Loss: 3.3406291007995605\n",
      "Loss: 3.5657825469970703\n",
      "Loss: 3.3918020725250244\n",
      "Loss: 3.2632603645324707\n",
      "Loss: 3.6028542518615723\n",
      "Loss: 3.3174290657043457\n",
      "Loss: 3.580857038497925\n",
      "Loss: 3.6587698459625244\n",
      "Loss: 3.5694806575775146\n",
      "Loss: 3.722294569015503\n",
      "Loss: 3.2219557762145996\n",
      "Loss: 3.2794506549835205\n",
      "Loss: 3.5649631023406982\n",
      "Loss: 3.1080424785614014\n",
      "Loss: 3.3510143756866455\n",
      "Loss: 3.2391207218170166\n",
      "Loss: 3.4543118476867676\n",
      "Loss: 3.6546716690063477\n",
      "Loss: 3.440202236175537\n",
      "Loss: 3.6771204471588135\n",
      "Loss: 3.3346974849700928\n",
      "Loss: 3.441556692123413\n",
      "Loss: 3.1423308849334717\n",
      "Loss: 3.448988676071167\n",
      "Loss: 3.4449028968811035\n",
      "Loss: 3.637279510498047\n",
      "Loss: 3.2227783203125\n",
      "Loss: 3.1877052783966064\n",
      "Loss: 3.312113046646118\n",
      "Loss: 3.643913745880127\n",
      "Loss: 3.17474102973938\n",
      "Loss: 3.4760189056396484\n",
      "Loss: 3.3431904315948486\n",
      "Loss: 3.329465389251709\n",
      "Loss: 3.2812275886535645\n",
      "Loss: 3.8081977367401123\n",
      "Loss: 3.5063111782073975\n",
      "Loss: 3.353271245956421\n",
      "Loss: 3.2625012397766113\n",
      "Loss: 3.403505563735962\n",
      "Loss: 3.3490614891052246\n",
      "Loss: 3.666239023208618\n",
      "Loss: 3.240537166595459\n",
      "Loss: 3.706857204437256\n",
      "Loss: 3.133519411087036\n",
      "Loss: 3.5064544677734375\n",
      "Loss: 3.0345587730407715\n",
      "Loss: 3.279284954071045\n",
      "Loss: 3.8310413360595703\n",
      "Loss: 3.2356669902801514\n",
      "Loss: 3.264516830444336\n",
      "Loss: 3.7273507118225098\n",
      "Loss: 3.0197489261627197\n",
      "Loss: 3.5179433822631836\n",
      "Loss: 2.9497830867767334\n",
      "Loss: 3.2143373489379883\n",
      "Loss: 3.116624593734741\n",
      "Loss: 2.985004186630249\n",
      "Loss: 3.202688217163086\n",
      "Loss: 3.129120111465454\n",
      "Loss: 3.427330255508423\n",
      "Loss: 3.4313547611236572\n",
      "Loss: 3.349576473236084\n",
      "Loss: 2.993469715118408\n",
      "Loss: 3.275989055633545\n",
      "Loss: 3.2838897705078125\n",
      "Loss: 3.867748498916626\n",
      "Loss: 3.321341037750244\n",
      "Loss: 3.0575156211853027\n",
      "Loss: 3.7029573917388916\n",
      "Loss: 3.380357265472412\n",
      "Loss: 2.9559779167175293\n",
      "Loss: 3.3671607971191406\n",
      "Loss: 3.666839838027954\n",
      "Loss: 3.2031753063201904\n",
      "Loss: 3.3163185119628906\n",
      "Loss: 3.318161725997925\n",
      "Loss: 3.5280277729034424\n",
      "Loss: 3.8197011947631836\n",
      "Loss: 3.2790958881378174\n",
      "Loss: 3.7556405067443848\n",
      "Loss: 2.965362071990967\n",
      "Loss: 3.2647526264190674\n",
      "Loss: 3.6338303089141846\n",
      "Loss: 3.3143560886383057\n",
      "Loss: 3.104332447052002\n",
      "Loss: 3.2713491916656494\n",
      "Loss: 3.4068806171417236\n",
      "Loss: 2.7764029502868652\n",
      "Loss: 3.7557125091552734\n",
      "Loss: 3.312551259994507\n",
      "Loss: 3.3146629333496094\n",
      "Loss: 3.500319480895996\n",
      "Loss: 3.48612642288208\n",
      "Loss: 3.3582632541656494\n",
      "Loss: 3.0106875896453857\n",
      "Loss: 3.426105499267578\n",
      "Loss: 3.2555038928985596\n",
      "Loss: 3.5135815143585205\n",
      "Loss: 3.3778324127197266\n",
      "Loss: 3.484611988067627\n",
      "Loss: 3.616238594055176\n",
      "Loss: 3.4225993156433105\n",
      "Loss: 3.643949031829834\n",
      "Loss: 3.457353353500366\n",
      "Loss: 3.3744254112243652\n",
      "Loss: 3.0678904056549072\n",
      "Loss: 3.1518466472625732\n",
      "Loss: 3.2891149520874023\n",
      "Loss: 3.1849889755249023\n",
      "Loss: 3.3097434043884277\n",
      "Loss: 3.1604385375976562\n",
      "Loss: 2.8732128143310547\n",
      "Loss: 3.287576198577881\n",
      "Loss: 2.973118305206299\n",
      "Loss: 3.2308311462402344\n",
      "Loss: 3.297574281692505\n",
      "Loss: 3.361522674560547\n",
      "Loss: 3.4286811351776123\n",
      "Loss: 3.618513584136963\n",
      "Loss: 3.3398051261901855\n",
      "Loss: 3.0839927196502686\n",
      "Loss: 3.3779823780059814\n",
      "Loss: 3.2515463829040527\n",
      "Loss: 3.3385040760040283\n",
      "Loss: 2.996663808822632\n",
      "Loss: 3.4377448558807373\n",
      "Loss: 3.2277302742004395\n",
      "Loss: 3.160776376724243\n",
      "Loss: 3.4257030487060547\n",
      "Loss: 3.2591257095336914\n",
      "Loss: 3.6007163524627686\n",
      "Loss: 3.6063685417175293\n",
      "Loss: 3.056180953979492\n",
      "Loss: 3.2992165088653564\n",
      "Loss: 3.530242443084717\n",
      "Loss: 3.9082090854644775\n",
      "Loss: 3.327970027923584\n",
      "Loss: 3.20923113822937\n",
      "Loss: 3.1932220458984375\n",
      "Loss: 3.319093942642212\n",
      "Loss: 3.1486308574676514\n",
      "Loss: 3.1319804191589355\n",
      "Loss: 3.515087366104126\n",
      "Loss: 3.6215078830718994\n",
      "Loss: 3.060042142868042\n",
      "Loss: 3.5601820945739746\n",
      "Loss: 3.2072620391845703\n",
      "Loss: 3.5363895893096924\n",
      "Loss: 3.631880283355713\n",
      "Loss: 3.6867260932922363\n",
      "Loss: 3.0589747428894043\n",
      "Loss: 3.69869065284729\n",
      "Loss: 3.4826040267944336\n",
      "Loss: 3.6951403617858887\n",
      "Loss: 3.353429079055786\n",
      "Loss: 3.2715468406677246\n",
      "Loss: 3.260591745376587\n",
      "Loss: 3.2791454792022705\n",
      "Loss: 3.502875804901123\n",
      "Loss: 3.3767545223236084\n",
      "Loss: 3.4259495735168457\n",
      "Loss: 3.2500147819519043\n",
      "Loss: 3.2201759815216064\n",
      "Loss: 3.3787779808044434\n",
      "Loss: 3.1634621620178223\n",
      "Loss: 3.115200996398926\n",
      "Loss: 3.539170503616333\n",
      "Loss: 2.8730952739715576\n",
      "Loss: 3.1422834396362305\n",
      "Loss: 3.5362186431884766\n",
      "Loss: 3.576873779296875\n",
      "Loss: 3.228008508682251\n",
      "Loss: 3.0725815296173096\n",
      "Loss: 3.4391064643859863\n",
      "Loss: 3.4020211696624756\n",
      "Loss: 3.20456862449646\n",
      "Loss: 3.3378570079803467\n",
      "Loss: 3.1851119995117188\n",
      "Loss: 3.1062073707580566\n",
      "Loss: 3.6115262508392334\n",
      "Loss: 3.816676616668701\n",
      "Loss: 3.6755082607269287\n",
      "Loss: 2.9173388481140137\n",
      "Loss: 3.3840696811676025\n",
      "Loss: 3.2110085487365723\n",
      "Loss: 3.128392219543457\n",
      "Loss: 3.166964530944824\n",
      "Loss: 3.3115153312683105\n",
      "Loss: 3.5444791316986084\n",
      "Loss: 2.9923884868621826\n",
      "Loss: 3.290881633758545\n",
      "Loss: 3.2800793647766113\n",
      "Loss: 3.5598390102386475\n",
      "Loss: 3.2375752925872803\n",
      "Loss: 3.5651891231536865\n",
      "Loss: 3.1866672039031982\n",
      "Loss: 3.4977569580078125\n",
      "Loss: 3.0259103775024414\n",
      "Loss: 3.5555062294006348\n",
      "Loss: 3.3085551261901855\n",
      "Loss: 3.011465549468994\n",
      "Loss: 3.217639446258545\n",
      "Loss: 3.1849918365478516\n",
      "Loss: 3.242823839187622\n",
      "Loss: 3.0441582202911377\n",
      "Loss: 3.3424437046051025\n",
      "Loss: 3.3469157218933105\n",
      "Loss: 3.0927248001098633\n",
      "Loss: 3.0457115173339844\n",
      "Loss: 3.2079944610595703\n",
      "Loss: 3.2903778553009033\n",
      "Loss: 3.301906108856201\n",
      "Loss: 3.7468645572662354\n",
      "Loss: 2.8070595264434814\n",
      "Loss: 3.2481095790863037\n",
      "Loss: 2.9358017444610596\n",
      "Loss: 3.372368097305298\n",
      "Loss: 3.3931214809417725\n",
      "Loss: 3.0715672969818115\n",
      "Loss: 2.882718324661255\n",
      "Loss: 3.3816967010498047\n",
      "Loss: 2.8514842987060547\n",
      "Loss: 3.4772777557373047\n",
      "Loss: 2.9922380447387695\n",
      "Loss: 3.126389741897583\n",
      "Loss: 3.3932929039001465\n",
      "Loss: 3.0573718547821045\n",
      "Loss: 3.434093713760376\n",
      "Loss: 3.0914483070373535\n",
      "Loss: 3.3781440258026123\n",
      "Loss: 3.4181487560272217\n",
      "Loss: 3.342864513397217\n",
      "Loss: 3.5119056701660156\n",
      "Loss: 3.07810640335083\n",
      "Loss: 3.6935601234436035\n",
      "Loss: 3.5143659114837646\n",
      "Loss: 3.7914011478424072\n",
      "Loss: 2.9576644897460938\n",
      "Loss: 3.4616525173187256\n",
      "Loss: 3.2079803943634033\n",
      "Loss: 3.086759090423584\n",
      "Loss: 3.105130195617676\n",
      "Loss: 3.3036229610443115\n",
      "Loss: 3.340830087661743\n",
      "Loss: 3.4710440635681152\n",
      "Loss: 3.609740734100342\n",
      "Loss: 2.9503297805786133\n",
      "Loss: 3.6463658809661865\n",
      "Loss: 3.254375696182251\n",
      "Loss: 3.178924322128296\n",
      "Loss: 3.10357666015625\n",
      "Loss: 3.2098731994628906\n",
      "Loss: 3.3680269718170166\n",
      "Loss: 3.4502413272857666\n",
      "Loss: 3.4163784980773926\n",
      "Loss: 3.5196731090545654\n",
      "Loss: 3.074173927307129\n",
      "Loss: 3.130293369293213\n",
      "Loss: 3.3910012245178223\n",
      "Loss: 3.3971033096313477\n",
      "Loss: 3.1340315341949463\n",
      "Loss: 3.2474706172943115\n",
      "Loss: 3.11049747467041\n",
      "Loss: 3.4291422367095947\n",
      "Loss: 3.157724142074585\n",
      "Loss: 3.5485196113586426\n",
      "Loss: 3.383542537689209\n",
      "Loss: 2.9011290073394775\n",
      "Loss: 3.1798670291900635\n",
      "Loss: 3.4500460624694824\n",
      "Loss: 3.1584088802337646\n",
      "Loss: 3.4001877307891846\n",
      "Loss: 3.6249616146087646\n",
      "Loss: 3.564850330352783\n",
      "Loss: 3.2393558025360107\n",
      "Loss: 3.34757399559021\n",
      "Loss: 2.837111473083496\n",
      "Loss: 3.0933852195739746\n",
      "Loss: 3.497084379196167\n",
      "Loss: 3.0789103507995605\n",
      "Loss: 3.006484270095825\n",
      "Loss: 3.1259002685546875\n",
      "Loss: 3.20208740234375\n",
      "Loss: 3.647517204284668\n",
      "Loss: 3.672567129135132\n",
      "Loss: 3.3603169918060303\n",
      "Loss: 3.265131711959839\n",
      "Loss: 3.2623131275177\n",
      "Loss: 3.2280759811401367\n",
      "Loss: 3.3248915672302246\n",
      "Loss: 2.6385140419006348\n",
      "Loss: 3.4866504669189453\n",
      "Loss: 3.783743381500244\n",
      "Loss: 2.7655272483825684\n",
      "Loss: 3.1012208461761475\n",
      "Loss: 3.3762946128845215\n",
      "Loss: 3.467043161392212\n",
      "Loss: 3.501783847808838\n",
      "Loss: 3.2229480743408203\n",
      "Loss: 3.1076157093048096\n",
      "Loss: 3.067194700241089\n",
      "Loss: 3.4609298706054688\n",
      "Loss: 3.456608295440674\n",
      "Loss: 3.2035629749298096\n",
      "Loss: 3.5362656116485596\n",
      "Loss: 3.154494524002075\n",
      "Loss: 3.3969128131866455\n",
      "Loss: 2.9894556999206543\n",
      "Loss: 3.538341760635376\n",
      "Loss: 3.043743371963501\n",
      "Loss: 3.55367374420166\n",
      "Loss: 3.510925054550171\n",
      "Loss: 3.1211109161376953\n",
      "Loss: 3.3523197174072266\n",
      "Loss: 3.770714282989502\n",
      "Loss: 3.283292055130005\n",
      "Loss: 3.349774122238159\n",
      "Loss: 3.222137451171875\n",
      "Loss: 3.3291122913360596\n",
      "Loss: 3.574662446975708\n",
      "Loss: 3.199270725250244\n",
      "Loss: 3.1282856464385986\n",
      "Loss: 3.273094654083252\n",
      "Loss: 3.244513750076294\n",
      "Loss: 3.3250114917755127\n",
      "Loss: 2.909017562866211\n",
      "Loss: 3.3245174884796143\n",
      "Loss: 3.439008951187134\n",
      "Loss: 3.2324001789093018\n",
      "Loss: 3.321986436843872\n",
      "Loss: 2.7335433959960938\n",
      "Loss: 3.603118896484375\n",
      "Loss: 3.154611825942993\n",
      "Loss: 3.5156493186950684\n",
      "Loss: 2.988548755645752\n",
      "Loss: 3.5789623260498047\n",
      "Loss: 3.5480165481567383\n",
      "Loss: 2.8762168884277344\n",
      "Loss: 3.1536035537719727\n",
      "Loss: 2.7276740074157715\n",
      "Loss: 3.052582263946533\n",
      "Loss: 3.5773394107818604\n",
      "Loss: 3.2011642456054688\n",
      "Loss: 3.251314401626587\n",
      "Loss: 3.47806453704834\n",
      "Loss: 3.6637144088745117\n",
      "Loss: 3.546079158782959\n",
      "Loss: 3.292616367340088\n",
      "Loss: 3.0954723358154297\n",
      "Loss: 3.3020286560058594\n",
      "Loss: 3.4324159622192383\n",
      "Loss: 3.1216108798980713\n",
      "Loss: 3.0584864616394043\n",
      "Loss: 3.09118390083313\n",
      "Loss: 3.2625956535339355\n",
      "Loss: 3.184072732925415\n",
      "Loss: 3.109372138977051\n",
      "Loss: 3.337878942489624\n",
      "Loss: 3.251875400543213\n",
      "Loss: 3.2730066776275635\n",
      "Loss: 3.0635781288146973\n",
      "Loss: 3.5541329383850098\n",
      "Loss: 3.523221015930176\n",
      "Loss: 3.6417698860168457\n",
      "Loss: 3.0642237663269043\n",
      "Loss: 2.9305999279022217\n",
      "Loss: 3.0936551094055176\n",
      "Loss: 3.3181817531585693\n",
      "Loss: 3.372102975845337\n",
      "Loss: 3.6227002143859863\n",
      "Loss: 3.151042938232422\n",
      "Loss: 3.3591113090515137\n",
      "Loss: 3.0106348991394043\n",
      "Loss: 3.6208910942077637\n",
      "Loss: 3.206376075744629\n",
      "Loss: 3.3365769386291504\n",
      "Loss: 3.3267364501953125\n",
      "Loss: 3.5710866451263428\n",
      "Loss: 3.249439001083374\n",
      "Loss: 3.0513620376586914\n",
      "Loss: 3.381270170211792\n",
      "Loss: 3.578256130218506\n",
      "Loss: 3.3721728324890137\n",
      "Loss: 3.4015581607818604\n",
      "Loss: 3.2240939140319824\n",
      "Loss: 3.2826786041259766\n",
      "Loss: 3.390795946121216\n",
      "Loss: 2.998971700668335\n",
      "Loss: 3.0132503509521484\n",
      "Loss: 3.3716773986816406\n",
      "Loss: 3.120286703109741\n",
      "Loss: 3.112255811691284\n",
      "Loss: 3.0829203128814697\n",
      "Loss: 3.1455740928649902\n",
      "Loss: 3.163546085357666\n",
      "Loss: 2.823486328125\n",
      "Loss: 3.4287025928497314\n",
      "Loss: 2.780689001083374\n",
      "Loss: 3.550583600997925\n",
      "Loss: 3.288982391357422\n",
      "Loss: 3.371217727661133\n",
      "Loss: 3.6008460521698\n",
      "Loss: 3.6834564208984375\n",
      "Loss: 3.191063404083252\n",
      "Loss: 3.395498275756836\n",
      "Loss: 2.913443088531494\n",
      "Loss: 3.1690900325775146\n",
      "Loss: 3.2467923164367676\n",
      "Loss: 2.8297672271728516\n",
      "Loss: 3.657078266143799\n",
      "Loss: 3.4340009689331055\n",
      "Loss: 2.967672824859619\n",
      "Loss: 3.7395966053009033\n",
      "Loss: 3.34385085105896\n",
      "Loss: 3.3622610569000244\n",
      "Loss: 3.321455240249634\n",
      "Loss: 3.323411703109741\n",
      "Loss: 3.569291114807129\n",
      "Loss: 3.3489580154418945\n",
      "Loss: 3.5994677543640137\n",
      "Loss: 3.3361520767211914\n",
      "Loss: 3.1676535606384277\n",
      "Loss: 3.6156558990478516\n",
      "Loss: 3.3741326332092285\n",
      "Loss: 3.122943639755249\n",
      "Loss: 2.82517409324646\n",
      "Loss: 3.4232676029205322\n",
      "Loss: 3.5605247020721436\n",
      "Loss: 3.1742866039276123\n",
      "Loss: 3.209214925765991\n",
      "Loss: 3.2813053131103516\n",
      "Loss: 3.2998361587524414\n",
      "Loss: 3.281013011932373\n",
      "Loss: 2.9185056686401367\n",
      "Loss: 3.0217487812042236\n",
      "Loss: 2.908781051635742\n",
      "Loss: 3.173398017883301\n",
      "Loss: 3.7179152965545654\n",
      "Loss: 3.2763428688049316\n",
      "Loss: 3.2949378490448\n",
      "Loss: 3.100846529006958\n",
      "Loss: 3.4384121894836426\n",
      "Loss: 3.1094810962677\n",
      "Loss: 3.3963005542755127\n",
      "Loss: 3.2020909786224365\n",
      "Loss: 3.193636894226074\n",
      "Loss: 3.049171209335327\n",
      "Loss: 3.5707619190216064\n",
      "Loss: 3.554274559020996\n",
      "Loss: 3.09661865234375\n",
      "Loss: 3.247335910797119\n",
      "Loss: 3.1920924186706543\n",
      "Loss: 3.078892469406128\n",
      "Loss: 3.351022958755493\n",
      "Loss: 2.850275754928589\n",
      "Loss: 3.2061753273010254\n",
      "Loss: 2.8096868991851807\n",
      "Loss: 2.8917455673217773\n",
      "Loss: 3.027202606201172\n",
      "Loss: 2.838352680206299\n",
      "Loss: 3.098634958267212\n",
      "Loss: 3.647923231124878\n",
      "Loss: 3.1542162895202637\n",
      "Loss: 2.9426512718200684\n",
      "Loss: 3.281236171722412\n",
      "Loss: 3.1723110675811768\n",
      "Loss: 3.301255941390991\n",
      "Loss: 3.3161821365356445\n",
      "Loss: 3.2097253799438477\n",
      "Loss: 3.481015205383301\n",
      "Loss: 3.237621784210205\n",
      "Loss: 3.215656042098999\n",
      "Loss: 3.3969058990478516\n",
      "Loss: 2.8500723838806152\n",
      "Loss: 3.364866256713867\n",
      "Loss: 3.592235565185547\n",
      "Loss: 3.487067461013794\n",
      "Loss: 2.874445676803589\n",
      "Loss: 3.0986886024475098\n",
      "Loss: 3.467194080352783\n",
      "Loss: 3.0573790073394775\n",
      "Loss: 3.199817180633545\n",
      "Loss: 3.1745636463165283\n",
      "Loss: 3.098252058029175\n",
      "Loss: 2.7342941761016846\n",
      "Loss: 3.1158552169799805\n",
      "Loss: 3.649998903274536\n",
      "Loss: 2.92317795753479\n",
      "Loss: 3.6507785320281982\n",
      "Loss: 2.82204532623291\n",
      "Loss: 3.056321620941162\n",
      "Loss: 3.2762699127197266\n",
      "Loss: 3.120884418487549\n",
      "Loss: 3.1987721920013428\n",
      "Loss: 3.3880343437194824\n",
      "Loss: 3.126741647720337\n",
      "Loss: 3.6253671646118164\n",
      "Loss: 3.51071834564209\n",
      "Loss: 3.029224157333374\n",
      "Loss: 3.2912871837615967\n",
      "Loss: 2.9950320720672607\n",
      "Loss: 3.2259111404418945\n",
      "Loss: 3.155414581298828\n",
      "Loss: 3.6284444332122803\n",
      "Loss: 3.518982172012329\n",
      "Loss: 3.1451008319854736\n",
      "Loss: 3.1990442276000977\n",
      "Loss: 3.327606439590454\n",
      "Loss: 3.133692502975464\n",
      "Loss: 3.2006168365478516\n",
      "Loss: 3.072723150253296\n",
      "Loss: 3.0878148078918457\n",
      "Loss: 3.556440830230713\n",
      "Loss: 3.610745906829834\n",
      "Loss: 3.670259952545166\n",
      "Loss: 3.470698356628418\n",
      "Loss: 3.169726848602295\n",
      "Loss: 3.242302656173706\n",
      "Loss: 3.25020432472229\n",
      "Loss: 3.132725477218628\n",
      "Loss: 3.5793330669403076\n",
      "Loss: 3.2104125022888184\n",
      "Loss: 3.091012716293335\n",
      "Loss: 3.376638412475586\n",
      "Loss: 3.4134554862976074\n",
      "Loss: 3.1204581260681152\n",
      "Loss: 3.3641774654388428\n",
      "Loss: 3.143439769744873\n",
      "Loss: 3.321657657623291\n",
      "Loss: 3.271820306777954\n",
      "Loss: 2.7540221214294434\n",
      "Loss: 3.343658447265625\n",
      "Loss: 3.3452632427215576\n",
      "Loss: 3.164828062057495\n",
      "Loss: 2.9679083824157715\n",
      "Loss: 3.4010186195373535\n",
      "Loss: 3.6364150047302246\n",
      "Loss: 3.3597488403320312\n",
      "Loss: 3.1750760078430176\n",
      "Loss: 3.3902993202209473\n",
      "Loss: 3.65993595123291\n",
      "Loss: 3.3903348445892334\n",
      "Loss: 3.147888660430908\n",
      "Loss: 3.00165057182312\n",
      "Loss: 2.8688552379608154\n",
      "Loss: 3.4227840900421143\n",
      "Loss: 3.325529098510742\n",
      "Loss: 2.9741809368133545\n",
      "Loss: 3.050179958343506\n",
      "Loss: 3.101351737976074\n",
      "Loss: 2.697205066680908\n",
      "Loss: 3.109527349472046\n",
      "Loss: 3.361100435256958\n",
      "Loss: 3.385474920272827\n",
      "Loss: 3.2256920337677\n",
      "Loss: 3.2764720916748047\n",
      "Loss: 3.611621141433716\n",
      "Loss: 3.315000534057617\n",
      "Loss: 3.503765344619751\n",
      "Loss: 2.967111825942993\n",
      "Loss: 3.2802422046661377\n",
      "Loss: 2.9989511966705322\n",
      "Loss: 3.198111057281494\n",
      "Loss: 3.6025872230529785\n",
      "Loss: 3.412910223007202\n",
      "Loss: 3.3133585453033447\n",
      "Loss: 3.447666645050049\n",
      "Loss: 2.7645668983459473\n",
      "Loss: 3.4613165855407715\n",
      "Loss: 3.1035895347595215\n",
      "Loss: 3.4203083515167236\n",
      "Loss: 3.6455905437469482\n",
      "Loss: 3.331068515777588\n",
      "Loss: 3.2612719535827637\n",
      "Loss: 3.762364149093628\n",
      "Loss: 3.8221163749694824\n",
      "Loss: 3.3220982551574707\n",
      "Loss: 3.0592095851898193\n",
      "Loss: 3.3298046588897705\n",
      "Loss: 2.983945369720459\n",
      "Loss: 2.830570936203003\n",
      "Loss: 2.8763630390167236\n",
      "Loss: 3.3439292907714844\n",
      "Loss: 3.113713026046753\n",
      "Loss: 3.280573606491089\n",
      "Loss: 3.329408645629883\n",
      "Loss: 2.9773497581481934\n",
      "Loss: 3.4210293292999268\n",
      "Loss: 3.1745612621307373\n",
      "Loss: 3.2461748123168945\n",
      "Loss: 2.953258514404297\n",
      "Loss: 3.227809190750122\n",
      "Loss: 3.1829674243927\n",
      "Loss: 2.927891492843628\n",
      "Loss: 3.025758981704712\n",
      "Loss: 3.295809268951416\n",
      "Loss: 2.9875528812408447\n",
      "Loss: 3.2568166255950928\n",
      "Loss: 3.005885601043701\n",
      "Loss: 3.372892141342163\n",
      "Loss: 3.0315542221069336\n",
      "Loss: 3.1766185760498047\n",
      "Loss: 2.830589532852173\n",
      "Loss: 3.5027048587799072\n",
      "Loss: 3.3285324573516846\n",
      "Loss: 3.2514121532440186\n",
      "Loss: 2.831998109817505\n",
      "Loss: 3.1877050399780273\n",
      "Loss: 3.3571317195892334\n",
      "Loss: 3.0168704986572266\n",
      "Loss: 3.3299672603607178\n",
      "Loss: 3.3505959510803223\n",
      "Loss: 3.0802979469299316\n",
      "Loss: 3.712686061859131\n",
      "Loss: 3.066174030303955\n",
      "Loss: 3.0108208656311035\n",
      "Loss: 3.0135271549224854\n",
      "Loss: 3.2369303703308105\n",
      "Loss: 3.279428005218506\n",
      "Loss: 3.2522828578948975\n",
      "Loss: 3.1347591876983643\n",
      "Loss: 3.178518056869507\n",
      "Loss: 3.010695219039917\n",
      "Loss: 3.262230396270752\n",
      "Loss: 3.4476001262664795\n",
      "Loss: 3.578115463256836\n",
      "Loss: 3.233827829360962\n",
      "Loss: 3.3942902088165283\n",
      "Loss: 3.1739871501922607\n",
      "Loss: 3.5048089027404785\n",
      "Loss: 3.0521156787872314\n",
      "Loss: 3.3399851322174072\n",
      "Loss: 3.3775603771209717\n",
      "Loss: 3.315232515335083\n",
      "Loss: 3.208282947540283\n",
      "Loss: 2.89314603805542\n",
      "Loss: 3.14296555519104\n",
      "Loss: 3.3970417976379395\n",
      "Loss: 3.2009172439575195\n",
      "Loss: 2.9570367336273193\n",
      "Loss: 3.33953595161438\n",
      "Loss: 3.1884284019470215\n",
      "Loss: 3.398313045501709\n",
      "Loss: 3.1098034381866455\n",
      "Loss: 3.2424066066741943\n",
      "Loss: 3.643664836883545\n",
      "Loss: 3.277099609375\n",
      "Loss: 3.0228986740112305\n",
      "Loss: 3.0626473426818848\n",
      "Loss: 3.0778489112854004\n",
      "Loss: 3.024057626724243\n",
      "Loss: 3.1110479831695557\n",
      "Loss: 3.3398211002349854\n",
      "Loss: 2.942908525466919\n",
      "Loss: 3.5608465671539307\n",
      "Loss: 3.3927297592163086\n",
      "Loss: 3.160120725631714\n",
      "Loss: 2.8591930866241455\n",
      "Loss: 2.924062490463257\n",
      "Loss: 3.0999488830566406\n",
      "Loss: 3.0113565921783447\n",
      "Loss: 3.3285038471221924\n",
      "Loss: 3.286588430404663\n",
      "Loss: 3.184893846511841\n",
      "Loss: 3.054363250732422\n",
      "Loss: 2.8729841709136963\n",
      "Loss: 3.4993278980255127\n",
      "Loss: 2.8016514778137207\n",
      "Loss: 3.392284870147705\n",
      "Loss: 3.5546445846557617\n",
      "Loss: 3.3777976036071777\n",
      "Loss: 3.4263153076171875\n",
      "Loss: 3.1512815952301025\n",
      "Loss: 3.036308526992798\n",
      "Loss: 3.048859119415283\n",
      "Loss: 3.034069776535034\n",
      "Loss: 3.2823212146759033\n",
      "Loss: 2.8979740142822266\n",
      "Loss: 3.122608184814453\n",
      "Loss: 2.805356025695801\n",
      "Loss: 3.489150047302246\n",
      "Loss: 2.979346752166748\n",
      "Loss: 3.4993433952331543\n",
      "Loss: 3.293224573135376\n",
      "Loss: 3.4361093044281006\n",
      "Loss: 2.918586492538452\n",
      "Loss: 3.2968289852142334\n",
      "Loss: 3.17859148979187\n",
      "Loss: 2.7429163455963135\n",
      "Loss: 3.2373485565185547\n",
      "Loss: 3.0005924701690674\n",
      "Loss: 3.2023425102233887\n",
      "Loss: 3.5313596725463867\n",
      "Loss: 3.3823344707489014\n",
      "Loss: 3.3907718658447266\n",
      "Loss: 3.2344608306884766\n",
      "Loss: 2.9022939205169678\n",
      "Loss: 2.9748692512512207\n",
      "Loss: 3.144646644592285\n",
      "Loss: 3.158482789993286\n",
      "Loss: 3.0724878311157227\n",
      "Loss: 2.915306568145752\n",
      "Loss: 3.1355485916137695\n",
      "Loss: 3.3648345470428467\n",
      "Loss: 3.1651551723480225\n",
      "Loss: 3.217679500579834\n",
      "Loss: 2.9062085151672363\n",
      "Loss: 3.3284924030303955\n",
      "Loss: 3.340012788772583\n",
      "Loss: 3.47666335105896\n",
      "Loss: 3.221590042114258\n",
      "Loss: 3.089409589767456\n",
      "Loss: 3.162684679031372\n",
      "Loss: 2.9943251609802246\n",
      "Loss: 3.3481247425079346\n",
      "Loss: 2.9616754055023193\n",
      "Loss: 3.046563148498535\n",
      "Loss: 3.1039013862609863\n",
      "Loss: 3.508582592010498\n",
      "Loss: 3.29716157913208\n",
      "Loss: 2.7589399814605713\n",
      "Loss: 3.3514344692230225\n",
      "Loss: 3.172123432159424\n",
      "Loss: 3.3932337760925293\n",
      "Loss: 3.669020414352417\n",
      "Loss: 3.1831204891204834\n",
      "Loss: 3.271709442138672\n",
      "Loss: 3.3843021392822266\n",
      "Loss: 3.0565133094787598\n",
      "Loss: 3.5291998386383057\n",
      "Loss: 3.5557687282562256\n",
      "Loss: 3.5694708824157715\n",
      "Loss: 3.2103989124298096\n",
      "Loss: 3.4607608318328857\n",
      "Loss: 2.857497215270996\n",
      "Loss: 3.0286428928375244\n",
      "Loss: 3.0824644565582275\n",
      "Loss: 2.9346022605895996\n",
      "Loss: 2.9630510807037354\n",
      "Loss: 3.3870105743408203\n",
      "Loss: 3.143794536590576\n",
      "Loss: 2.927952527999878\n",
      "Loss: 3.1935033798217773\n",
      "Loss: 3.121497869491577\n",
      "Loss: 3.068404197692871\n",
      "Loss: 3.250309467315674\n",
      "Loss: 2.577289342880249\n",
      "Loss: 2.9261367321014404\n",
      "Loss: 3.1322102546691895\n",
      "Loss: 3.041583299636841\n",
      "Loss: 3.115192174911499\n",
      "Loss: 3.335949420928955\n",
      "Loss: 3.516472101211548\n",
      "Loss: 2.5124447345733643\n",
      "Loss: 3.4116687774658203\n",
      "Loss: 2.5919923782348633\n",
      "Loss: 3.1847686767578125\n",
      "Loss: 3.030372381210327\n",
      "Loss: 3.0277183055877686\n",
      "Loss: 2.9707891941070557\n",
      "Loss: 3.3099217414855957\n",
      "Loss: 3.3114800453186035\n",
      "Loss: 3.1164934635162354\n",
      "Loss: 3.3849427700042725\n",
      "Loss: 2.8882296085357666\n",
      "Loss: 2.9929749965667725\n",
      "Loss: 3.493088960647583\n",
      "Loss: 2.9236199855804443\n",
      "Loss: 3.02716326713562\n",
      "Loss: 2.6432602405548096\n",
      "Loss: 2.9153945446014404\n",
      "Loss: 2.7899439334869385\n",
      "Loss: 3.0060415267944336\n",
      "Loss: 3.076396942138672\n",
      "Loss: 2.955219268798828\n",
      "Loss: 3.1147284507751465\n",
      "Loss: 3.3059496879577637\n",
      "Loss: 2.9797303676605225\n",
      "Loss: 2.815974473953247\n",
      "Loss: 3.2969894409179688\n",
      "Loss: 2.9397571086883545\n",
      "Loss: 3.2726316452026367\n",
      "Loss: 3.132956027984619\n",
      "Loss: 2.9058597087860107\n",
      "Loss: 3.419144868850708\n",
      "Loss: 3.5472958087921143\n",
      "Loss: 3.153961420059204\n",
      "Loss: 3.1703450679779053\n",
      "Loss: 3.4268240928649902\n",
      "Loss: 2.9350337982177734\n",
      "Loss: 3.3994669914245605\n",
      "Loss: 3.5036461353302\n",
      "Loss: 3.3097589015960693\n",
      "Loss: 3.2643957138061523\n",
      "Loss: 3.2627885341644287\n",
      "Loss: 3.173091173171997\n",
      "Loss: 3.3893444538116455\n",
      "Loss: 2.878087043762207\n",
      "Loss: 2.8500664234161377\n",
      "Loss: 3.102964162826538\n",
      "Loss: 2.96370267868042\n",
      "Loss: 3.0260181427001953\n",
      "Loss: 3.2623589038848877\n",
      "Loss: 3.3561899662017822\n",
      "Loss: 3.235551357269287\n",
      "Loss: 3.2133610248565674\n",
      "Loss: 3.1855058670043945\n",
      "Loss: 3.1805977821350098\n",
      "Loss: 2.950019121170044\n",
      "Loss: 3.1899824142456055\n",
      "Loss: 3.1323633193969727\n",
      "Loss: 2.9963502883911133\n",
      "Loss: 2.6660068035125732\n",
      "Loss: 3.052234411239624\n",
      "Loss: 3.2080657482147217\n",
      "Loss: 3.0889480113983154\n",
      "Loss: 3.423421859741211\n",
      "Loss: 2.7331137657165527\n",
      "Loss: 2.933300018310547\n",
      "Loss: 3.295469045639038\n",
      "Loss: 3.1123528480529785\n",
      "Loss: 3.2367677688598633\n",
      "Loss: 3.2513532638549805\n",
      "Loss: 2.820133686065674\n",
      "Loss: 2.76373291015625\n",
      "Loss: 3.1883039474487305\n",
      "Loss: 3.2291736602783203\n",
      "Loss: 3.111475706100464\n",
      "Loss: 3.1208174228668213\n",
      "Loss: 2.991738796234131\n",
      "Loss: 3.0757317543029785\n",
      "Loss: 3.081552505493164\n",
      "Loss: 3.2147810459136963\n",
      "Loss: 3.2785325050354004\n",
      "Loss: 3.064592123031616\n",
      "Loss: 2.7340712547302246\n",
      "Loss: 2.872276782989502\n",
      "Loss: 2.795802116394043\n",
      "Loss: 2.6709837913513184\n",
      "Loss: 3.074434280395508\n",
      "Loss: 3.1176698207855225\n",
      "Loss: 3.0144965648651123\n",
      "Loss: 3.047330141067505\n",
      "Loss: 3.42084002494812\n",
      "Loss: 3.5338852405548096\n",
      "Loss: 3.6049983501434326\n",
      "Loss: 3.2002649307250977\n",
      "Loss: 3.382963180541992\n",
      "Loss: 3.056434392929077\n",
      "Loss: 3.3033485412597656\n",
      "Loss: 3.245575428009033\n",
      "Loss: 3.218416929244995\n",
      "Loss: 2.9144980907440186\n",
      "Loss: 3.1027207374572754\n",
      "Loss: 3.2909765243530273\n",
      "Loss: 3.324124813079834\n",
      "Loss: 2.6507444381713867\n",
      "Loss: 3.039076805114746\n",
      "Loss: 2.7820990085601807\n",
      "Loss: 3.0743167400360107\n",
      "Loss: 3.5174553394317627\n",
      "Loss: 2.9682281017303467\n",
      "Loss: 3.1532344818115234\n",
      "Loss: 2.8623063564300537\n",
      "Loss: 3.2708723545074463\n",
      "Loss: 2.749027729034424\n",
      "Loss: 3.1021714210510254\n",
      "Loss: 3.451406240463257\n",
      "Loss: 3.1376681327819824\n",
      "Loss: 2.86991024017334\n",
      "Loss: 2.9965438842773438\n",
      "Loss: 2.897916555404663\n",
      "Loss: 2.9346835613250732\n",
      "Loss: 2.8971941471099854\n",
      "Loss: 3.3678414821624756\n",
      "Loss: 3.0569913387298584\n",
      "Loss: 3.198007345199585\n",
      "Loss: 3.2074670791625977\n",
      "Loss: 3.1620116233825684\n",
      "Loss: 3.062485933303833\n",
      "Loss: 2.9998810291290283\n",
      "Loss: 3.2638323307037354\n",
      "Loss: 3.2275209426879883\n",
      "Loss: 2.9649360179901123\n",
      "Loss: 2.8268368244171143\n",
      "Loss: 3.160228967666626\n",
      "Loss: 3.3821487426757812\n",
      "Loss: 3.099907159805298\n",
      "Loss: 3.0344183444976807\n",
      "Loss: 3.2212769985198975\n",
      "Loss: 3.028564453125\n",
      "Loss: 2.892510414123535\n",
      "Loss: 3.042189836502075\n",
      "Loss: 3.324583053588867\n",
      "Loss: 2.8782598972320557\n",
      "Loss: 2.888932466506958\n",
      "Loss: 3.0525879859924316\n",
      "Loss: 3.239046096801758\n",
      "Loss: 2.9203381538391113\n",
      "Loss: 3.0580697059631348\n",
      "Loss: 3.4647881984710693\n",
      "Loss: 3.0102698802948\n",
      "Loss: 3.3941779136657715\n",
      "Loss: 3.0183653831481934\n",
      "Loss: 2.817004680633545\n",
      "Loss: 3.0413897037506104\n",
      "Loss: 3.173067092895508\n",
      "Loss: 3.2632620334625244\n",
      "Loss: 3.0317039489746094\n",
      "Loss: 3.2935595512390137\n",
      "Loss: 2.869386911392212\n",
      "Loss: 3.280067205429077\n",
      "Loss: 3.2065627574920654\n",
      "Loss: 3.1009843349456787\n",
      "Loss: 3.1973788738250732\n",
      "Loss: 3.286346912384033\n",
      "Loss: 3.268890857696533\n",
      "Loss: 3.1419711112976074\n",
      "Loss: 3.0018975734710693\n",
      "Loss: 3.1291074752807617\n",
      "Loss: 3.109039783477783\n",
      "Loss: 2.998670816421509\n",
      "Loss: 2.869335174560547\n",
      "Loss: 2.8018362522125244\n",
      "Loss: 3.624952554702759\n",
      "Loss: 3.5289294719696045\n",
      "Loss: 3.170180082321167\n",
      "Loss: 3.1441376209259033\n",
      "Loss: 3.139368772506714\n",
      "Loss: 2.9328792095184326\n",
      "Loss: 3.127408981323242\n",
      "Loss: 2.734896183013916\n",
      "Loss: 3.067739963531494\n",
      "Loss: 2.887885808944702\n",
      "Loss: 3.1968846321105957\n",
      "Loss: 2.8266210556030273\n",
      "Loss: 2.9561376571655273\n",
      "Loss: 3.0818631649017334\n",
      "Loss: 3.274106740951538\n",
      "Loss: 2.718632459640503\n",
      "val_loss at 1 epoch: 4.067277578874068\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:35:16.169964Z",
     "start_time": "2025-09-25T02:35:15.563154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained('/Users/zhangyf/llm/gpt2-sft')\n",
    "tokenizer.save_pretrained('/Users/zhangyf/llm/gpt2-sft')"
   ],
   "id": "46d6a70972fab639",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/zhangyf/llm/gpt2-sft/tokenizer_config.json',\n",
       " '/Users/zhangyf/llm/gpt2-sft/special_tokens_map.json',\n",
       " '/Users/zhangyf/llm/gpt2-sft/vocab.json',\n",
       " '/Users/zhangyf/llm/gpt2-sft/merges.txt',\n",
       " '/Users/zhangyf/llm/gpt2-sft/added_tokens.json',\n",
       " '/Users/zhangyf/llm/gpt2-sft/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:36:52.716663Z",
     "start_time": "2025-09-25T02:36:47.080496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from pprint import pprint\n",
    "g = pipeline('text-generation', model='/Users/zhangyf/llm/gpt2-sft')\n",
    "set_seed(42)\n",
    "pprint(g(\"this is a bad movie\", max_length=30, num_return_sequences=1))\n",
    "# 可以看到，经过了sft，模型不再乱输出了，而是输出了电影相关的描述"
   ],
   "id": "bd27877d7f23038b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'this is a bad movie... icky and uninspiring, icky and '\n",
      "                    'icky icky                              '\n",
      "                    '.                                                                                                                                                                                                                  '}]\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
