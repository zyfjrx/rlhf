{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-26T01:35:18.651430Z",
     "start_time": "2025-09-26T01:35:15.655154Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "class RewardHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # llm最后输出的隐藏层的维度\n",
    "        self.hidden_size = config.hidden_size\n",
    "        # 线性层用来对llm最后输出的隐藏层给奖励\n",
    "        self.reward = nn.Linear(self.hidden_size, 1)\n",
    "        self._post_init()\n",
    "\n",
    "    def _post_init(self):\n",
    "        # 使用正态分布初始化权重\n",
    "        nn.init.normal_(\n",
    "            self.reward.weight,\n",
    "            std=(1.0 / np.sqrt(self.hidden_size + 1))\n",
    "        )\n",
    "        # 将偏置初始化为0\n",
    "        nn.init.zeros_(self.reward.bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 给出奖励\n",
    "        return self.reward(hidden_states)\n",
    "\n",
    "class GPT2RewardHead(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.reward_head = RewardHead(self.llm.config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_outputs = self.llm.forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        last_hidden_state = transformer_outputs.hidden_states[-1]\n",
    "        # 给出奖励\n",
    "        reward = self.reward_head(last_hidden_state).squeeze(-1)\n",
    "        # sigmoid用来将奖励搞到(0,1)范围内\n",
    "        return torch.sigmoid(reward)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:35:39.495553Z",
     "start_time": "2025-09-26T01:35:39.183031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"/Users/zhangyf/llm/gpt2\"\n",
    "reward_model = GPT2RewardHead(model_name)\n",
    "reward_model.load_state_dict(torch.load(\"reward_model.pt\", map_location='cpu'))"
   ],
   "id": "c01d704052d58be7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:35:41.074284Z",
     "start_time": "2025-09-26T01:35:41.070641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"\n",
    "    ValueHead类为GPT2实现了一个“头”，会为输出的每个token返回一个标量值\n",
    "    标量值就是这个token的价值，ValueHead就是评论家。\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # llm最后输出的隐藏层的维度\n",
    "        self.hidden_size = config.hidden_size\n",
    "        # 价值网络的输出是标量\n",
    "        self.value = nn.Linear(self.hidden_size, 1)\n",
    "        self._post_init()\n",
    "\n",
    "    def _post_init(self):\n",
    "        nn.init.normal_(\n",
    "            self.value.weight,\n",
    "            std=(1.0 / np.sqrt(self.hidden_size + 1))\n",
    "        )\n",
    "        nn.init.zeros_(self.value.bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        output = hidden_states\n",
    "        return self.value(output)"
   ],
   "id": "4bfb81fbda5b0402",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:35:44.868870Z",
     "start_time": "2025-09-26T01:35:44.864304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ModelForCausalLMWithValueHead(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT2模型+一个价值头\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        # 这个要初始化为我们微调出来的gpt2-sft模型\n",
    "        # actor演员模型：策略模型\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "        # 添加价值头\n",
    "        # critic评论家模型：价值函数模型，价值头\n",
    "        self.v_head = ValueHead(self.llm.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "    ) -> Optional[torch.FloatTensor]:\n",
    "        # gpt2-sft模型的输出\n",
    "        transformer_outputs = self.llm.forward(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states = True,\n",
    "        )\n",
    "        # 输出的token的概率分布，维度为 `vocab_size`\n",
    "        lm_logits = transformer_outputs.logits\n",
    "        # 获取最后一层隐藏层\n",
    "        last_hidden_state = transformer_outputs.hidden_states[-1]\n",
    "\n",
    "        # 评估token的价值，评估的是最后一层隐藏层的价值\n",
    "        value = self.v_head(last_hidden_state).squeeze(-1)\n",
    "        # 返回输出的token的logits和token的价值\n",
    "        return lm_logits, value\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.llm.generate(*args, **kwargs)"
   ],
   "id": "dc4c2012e8a5bffd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:35:48.882258Z",
     "start_time": "2025-09-26T01:35:48.845640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = '/Users/zhangyf/llm/gpt2-sft'\n",
    "model = ModelForCausalLMWithValueHead(model_path)"
   ],
   "id": "572ec87e21b3b494",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:35:54.011430Z",
     "start_time": "2025-09-26T01:35:53.646952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"./sst2\")\n",
    "print(dataset)\n",
    "\n",
    "ds_train, ds_val = dataset['train'], dataset['validation']"
   ],
   "id": "11a1be8dedcb3a42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:03.301024Z",
     "start_time": "2025-09-26T01:36:03.292389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(ds_train))\n",
    "ds_train = ds_train.filter(lambda x: len(x['sentence'].split(' ')) > 8)\n",
    "ds_val = ds_val.filter(lambda x: len(x['sentence'].split(' ')) > 8)\n",
    "\n",
    "print(len(ds_train))\n",
    "print(len(ds_val))"
   ],
   "id": "d6df817c87bf6b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349\n",
      "31105\n",
      "807\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:08.126162Z",
     "start_time": "2025-09-26T01:36:08.123697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "input_min_token_length = 2\n",
    "input_max_token_length = 8\n",
    "input_token_length_range = list(range(\n",
    "    input_min_token_length,\n",
    "    input_max_token_length))\n",
    "print(input_token_length_range)\n",
    "print(random.choice(input_token_length_range))"
   ],
   "id": "5aed6ec2a0d70cc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7]\n",
      "7\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:13.743379Z",
     "start_time": "2025-09-26T01:36:13.707460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(sample):\n",
    "    input_size = random.choice(input_token_length_range)\n",
    "    sample['input_ids'] = tokenizer.encode(sample['sentence'])[:input_size]\n",
    "    sample['attention_mask'] = [1] * len(sample['input_ids'])\n",
    "    sample['query'] = tokenizer.decode(sample['input_ids'])\n",
    "    return sample\n",
    "\n",
    "map_kwargs = {\n",
    "    \"batched\": False,\n",
    "    \"remove_columns\": ['idx', 'sentence', 'label']\n",
    "}\n",
    "\n",
    "tokenized_dataset_train = ds_train.map(tokenize, **map_kwargs)\n",
    "tokenized_dataset_val = ds_val.map(tokenize, **map_kwargs)"
   ],
   "id": "b3d08d2ff80bc6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:16.898828Z",
     "start_time": "2025-09-26T01:36:16.896267Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_dataset_val[0]",
   "id": "802da6c4a4fa0073",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [270, 705, 82, 257, 23332, 290, 1690],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n",
       " 'query': \"it 's a charming and often\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:19.269998Z",
     "start_time": "2025-09-26T01:36:19.264833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_dataset_train.set_format(type='torch')\n",
    "tokenized_dataset_val.set_format(type='torch')\n",
    "\n",
    "print(tokenized_dataset_train[6])"
   ],
   "id": "126a1a21697066ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 1640,   883,  3807, 31006,   508, 13121]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1]), 'query': 'for those moviegoers who complain'}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:21.623005Z",
     "start_time": "2025-09-26T01:36:21.620812Z"
    }
   },
   "cell_type": "code",
   "source": "REWARD_TOKEN_ID = tokenizer.eos_token_id",
   "id": "dcbcdcdda0fafeeb",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:22.560547Z",
     "start_time": "2025-09-26T01:36:22.551648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def collator(batch):\n",
    "    return dict((key, [d[key] for d in batch]) for key in batch[0])\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset_train, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
    "val_dataloader = DataLoader(tokenized_dataset_val, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)"
   ],
   "id": "d2b4639301aa7ded",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [tensor([  11, 1534, 1531]), tensor([8043,  837, 2647,  837]), tensor([2502, 8988]), tensor([6888,  299,  470,  766, 1521,  597]), tensor([   76,  3316,   705,    82, 39769,   318]), tensor([ 1462,   852,   262, 25203,    12]), tensor([   64,  8303,   379, 11783,   289]), tensor([ 265, 1661,  257, 1643, 7758,  375]), tensor([19188,   423]), tensor([  271, 14169,   837,   572, 12945]), tensor([4360,  772,  981,  465, 3435]), tensor([ 270,  705,   82,  257, 9058,  523]), tensor([21754,  1577,  7559, 12692, 10148,   257]), tensor([ 338, 5729]), tensor([ 271,  517, 3499,  357,  290]), tensor([39240,   298,   290, 20868]), tensor([5562, 4071, 2646, 3025]), tensor([ 5661,  2646,   837,   588,   262, 12470,  2801]), tensor([1169, 1877,   12, 9526, 9891]), tensor([  272, 30690,  5022,   286,  3223, 35704,   290]), tensor([ 732, 1239, 1254]), tensor([ 11, 262, 717]), tensor([  271,   281,   555, 48544,   306]), tensor([ 4360,   991,  2407, 25103,   290, 23310,   477]), tensor([ 260, 2673]), tensor([270, 705,  82]), tensor([ 258, 4539, 1088,  290, 6529]), tensor([271, 663]), tensor([9930,  705]), tensor([4360,  340,  481,  655,  355, 1884]), tensor([  271, 22107]), tensor([  64, 4451, 3807,  326, 4206])], 'attention_mask': [tensor([1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1]), tensor([1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1]), tensor([1, 1]), tensor([1, 1, 1]), tensor([1, 1, 1, 1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1, 1, 1, 1, 1]), tensor([1, 1]), tensor([1, 1, 1, 1, 1])], 'query': [', profane', 'color , music ,', 'overcomes', \"ca n't see why any\", \"munch 's screenplay is\", 'to being the barn-', 'a stab at soccer h', 'at times a bit melod', 'would have', 'is clever , offbeat', 'but even while his characters', \"it 's a setup so\", \"should give `` scratch '' a\", \"'s apparently\", 'is more interesting ( and', 'talent and wit', 'that rare film whose', 'this film , like the similarly ill', 'the low-grade cheese', 'an uneven mix of dark satire and', 'we never feel', ', the first', 'is an ungainly', 'but still quite tasty and inviting all', 'reaction', \"it 's\", 'he runs around and acts', 'is its', \"they '\", 'but it will just as likely', 'is akin', 'a smart movie that knows']}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:29.678302Z",
     "start_time": "2025-09-26T01:36:29.676072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_min_length = 5\n",
    "output_max_length = 16\n",
    "\n",
    "# https://huggingface.co/docs/trl/how_to_train#how-to-generate-text-for-training\n",
    "# gpt2-sft输出的配置\n",
    "# - 模型会从整个词汇表中按照原始概率分布进行采样\n",
    "# - 每个词被选中的概率完全由模型的原始输出决定\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0, # 所有词汇表中的词都可能被选中\n",
    "    \"top_p\": 1.0, # 包含整个概率分布\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id\n",
    "}"
   ],
   "id": "2cf5a5257524037b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:32.705206Z",
     "start_time": "2025-09-26T01:36:32.702111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
    "generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
    "sample = tokenizer('Hi, this')\n",
    "print(sample)"
   ],
   "id": "92ba29a0bbb83c78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [17250, 11, 428], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:35.753Z",
     "start_time": "2025-09-26T01:36:35.620759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_response = model.generate(\n",
    "    input_ids=torch.tensor(sample['input_ids']).unsqueeze(0),\n",
    "    attention_mask=torch.tensor(sample['attention_mask']).unsqueeze(0),\n",
    "    **generation_kwargs\n",
    ").squeeze(0)\n",
    "print(query_response)"
   ],
   "id": "7e7bd48f46ec43be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17250,    11,   428,   318,   262,   845,   717, 25168,   286,   262,\n",
      "         8663,  2277,  3807])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:38.280959Z",
     "start_time": "2025-09-26T01:36:38.278924Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(query_response))",
   "id": "c95e2810a3949e34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, this is the very first installment of the franchise hit movie\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:39.572133Z",
     "start_time": "2025-09-26T01:36:39.569591Z"
    }
   },
   "cell_type": "code",
   "source": "new_tokens",
   "id": "8e7801a6a8b2253b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:42.311020Z",
     "start_time": "2025-09-26T01:36:42.254745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    query_response_score = torch.cat([\n",
    "        query_response,\n",
    "        torch.tensor([REWARD_TOKEN_ID])])\n",
    "    attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
    "    score = reward_model(\n",
    "        query_response_score.unsqueeze(0),\n",
    "        attention_mask.unsqueeze(0)\n",
    "    ).squeeze(0)[-1]\n",
    "print(score)"
   ],
   "id": "14e0630cfa68d7ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9939)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:53.767721Z",
     "start_time": "2025-09-26T01:36:49.517752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "reward_model = reward_model.to(device)\n",
    "\n",
    "query_tensors = batch['input_ids']\n",
    "query_attention_masks = batch['attention_mask']\n",
    "\n",
    "response_tensors = [] # 补全的张量\n",
    "query_response_tensors = [] # 提示词+补全 的张量\n",
    "score_tensors = [] # 奖励模型给的分数的张量\n",
    "\n",
    "for i, query in enumerate(query_tensors):\n",
    "    query = query.to(device)\n",
    "    query_attention_mask = query_attention_masks[i].to(device)\n",
    "    new_tokens = random.choice(list(range(output_min_length, output_max_length)))\n",
    "    generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
    "    query_response = model.generate(\n",
    "        input_ids=query.unsqueeze(0),\n",
    "        attention_mask=query_attention_mask.unsqueeze(0),\n",
    "        **generation_kwargs\n",
    "    ).squeeze(0)\n",
    "\n",
    "    response_len = len(query_response) - len(query)\n",
    "    response_tensors.append(query_response[-response_len:])\n",
    "    query_response_tensors.append(query_response)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_response_score = torch.cat([query_response, torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
    "        attention_mask = torch.ones_like(query_response_score, dtype=torch.long)\n",
    "        score = reward_model(\n",
    "            query_response_score.unsqueeze(0),\n",
    "            attention_mask.unsqueeze(0)\n",
    "        ).squeeze(0)[-1]\n",
    "        score = 2 * (score - 0.5)\n",
    "    score_tensors.append(score)\n",
    "\n",
    "batch[\"response\"] = [tokenizer.decode(response) for response in response_tensors]\n",
    "from pprint import pprint\n",
    "pprint(batch['response'])"
   ],
   "id": "d5c9207c45450ee8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' obsessions , icky jokes and',\n",
      " ' and whimsy of love . ',\n",
      " ' korean maternal instincts with vitality and tenderness , which is something '\n",
      " 'no other',\n",
      " ' movie could ever possibly be as successful in the',\n",
      " ' well on its way to',\n",
      " 'burningly bad version of the wildly over-',\n",
      " 'ooliganism in the head ̶ ̶ ̶ it',\n",
      " 'ramatic but  ive never felt more',\n",
      " ' a great recommendation to watch , especially iced tea',\n",
      " ' and hilarious    icated on a different ethnic background and',\n",
      " ' are intermittently funny , their jokes are',\n",
      " ' poorly shot that by mid',\n",
      " ' whirl       if it',\n",
      " ' amused at how engaging all this is    that the laughs are so',\n",
      " \" accurate ) than the man '\",\n",
      " ' really shines     as the world and empire decomposes . ',\n",
      " ' using of voice-over actors in action movies can only inspire awe ',\n",
      " '-timed bride-and-dame remake , is altern',\n",
      " ' ersatzness icky salt rises to the level of enthusiasm',\n",
      " ' , throughout , some deep sadness and psychological',\n",
      " \" we 're in danger when we see bad\",\n",
      " ' half of the film leads to utter fits and',\n",
      " ' living , breathing thing .          smart',\n",
      " \" the way through ichi 's twistedly comic debut\",\n",
      " ' and misplaced frustration vernier ',\n",
      " ' good for this fable , since you can feel',\n",
      " ' weird in his script , maybe because it tells us too much about why',\n",
      " ' effect - the library of ideas in everyone',\n",
      " 're told a convincing story',\n",
      " ' rebrand it as explanation movies .        ',\n",
      " ' to staring into a telescope .     ',\n",
      " ' how to balance the heartbreaking dramas between the heartfelt testimonies '\n",
      " 'of survivors and those']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:36:57.622784Z",
     "start_time": "2025-09-26T01:36:57.617545Z"
    }
   },
   "cell_type": "code",
   "source": "score_tensors",
   "id": "3e4e09f02878526e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.9561),\n",
       " tensor(0.9919),\n",
       " tensor(0.9979),\n",
       " tensor(-0.9763),\n",
       " tensor(0.9562),\n",
       " tensor(-0.9988),\n",
       " tensor(-0.9871),\n",
       " tensor(0.2387),\n",
       " tensor(0.8755),\n",
       " tensor(0.9992),\n",
       " tensor(0.2430),\n",
       " tensor(-0.9997),\n",
       " tensor(0.2599),\n",
       " tensor(0.9940),\n",
       " tensor(0.9944),\n",
       " tensor(0.9956),\n",
       " tensor(-0.8964),\n",
       " tensor(-0.9049),\n",
       " tensor(0.6486),\n",
       " tensor(0.7333),\n",
       " tensor(-0.9872),\n",
       " tensor(-0.1871),\n",
       " tensor(0.9091),\n",
       " tensor(0.9994),\n",
       " tensor(-0.9970),\n",
       " tensor(0.9938),\n",
       " tensor(-0.9895),\n",
       " tensor(0.9503),\n",
       " tensor(0.9925),\n",
       " tensor(-0.9017),\n",
       " tensor(-0.9826),\n",
       " tensor(0.9989)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:00.987926Z",
     "start_time": "2025-09-26T01:37:00.936637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "# 冻结的模型\n",
    "sft_model = deepcopy(model)"
   ],
   "id": "c602a691bbe8beae",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:03.548105Z",
     "start_time": "2025-09-26T01:37:03.542826Z"
    }
   },
   "cell_type": "code",
   "source": "query_response_tensors",
   "id": "6b4514db9310fb7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   11,  1534,  1531, 10201,  6202,   837,   220, 17479, 14532,   290]),\n",
       " tensor([ 8043,   837,  2647,   837,   290, 29923,  1837,   286,  1842,   764,\n",
       "           220]),\n",
       " tensor([ 2502,  8988,   479, 29456, 22160, 26744,   351, 41687,   290, 15403,\n",
       "          1108,   837,   543,   318,  1223,   645,   584]),\n",
       " tensor([6888,  299,  470,  766, 1521,  597, 3807,  714, 1683, 5457,  307,  355,\n",
       "         4388,  287,  262]),\n",
       " tensor([   76,  3316,   705,    82, 39769,   318,   880,   319,   663,   835,\n",
       "           284]),\n",
       " tensor([ 1462,   852,   262, 25203,    12, 10899,  4420,  2089,  2196,   286,\n",
       "           262, 20278,   625,    12]),\n",
       " tensor([   64,  8303,   379, 11783,   289,   970,  5516,  1042,   287,   262,\n",
       "          1182,   220, 48869,   220, 48869,   220, 48869,   340]),\n",
       " tensor([ 265, 1661,  257, 1643, 7758,  375,  859, 1512,  475,  220,  220,  425,\n",
       "         1239, 2936,  517]),\n",
       " tensor([19188,   423,   257,  1049, 15602,   284,  2342,   837,  2592,   220,\n",
       "          3711,  8887]),\n",
       " tensor([  271, 14169,   837,   572, 12945,   290, 20105,   220,   220,   220,\n",
       "           220,  3474,   319,   257,  1180,  9450,  4469,   290]),\n",
       " tensor([ 4360,   772,   981,   465,  3435,   389, 30598,  1473,  8258,   837,\n",
       "           511, 14532,   389]),\n",
       " tensor([  270,   705,    82,   257,  9058,   523, 13455,  2823,   326,   416,\n",
       "          3095]),\n",
       " tensor([21754,  1577,  7559, 12692, 10148,   257,   348,  1901,   220,   220,\n",
       "           220,   220,   220,   220,   611,   340]),\n",
       " tensor([  338,  5729, 36979,   379,   703, 11932,   477,   428,   318,   220,\n",
       "           220,   220,   326,   262, 22051,   389,   523]),\n",
       " tensor([ 271,  517, 3499,  357,  290, 7187, 1267,  621,  262,  582,  705]),\n",
       " tensor([39240,   298,   290, 20868,  1107, 32481,   220,   220,   220,   220,\n",
       "           355,   262,   995,   290, 13735, 26969,  4832,   764,   220]),\n",
       " tensor([ 5562,  4071,  2646,  3025,  1262,   286,  3809,    12,  2502, 10544,\n",
       "           287,  2223,  6918,   460,   691, 18330, 25030,   220]),\n",
       " tensor([ 5661,  2646,   837,   588,   262, 12470,  2801,    12, 16514,   276,\n",
       "         26619,    12,   392,    12,    67,   480, 28763,   837,   318,  3983]),\n",
       " tensor([ 1169,  1877,    12,  9526,  9891,   220,   364, 27906,  1108,   220,\n",
       "         17479,  8268, 16736,   284,   262,  1241,   286, 17131]),\n",
       " tensor([  272, 30690,  5022,   286,  3223, 35704,   290,   837,  3690,   837,\n",
       "           617,  2769, 25303,   290, 10590]),\n",
       " tensor([  732,  1239,  1254,   356,   705,   260,   287,  3514,   220, 12518,\n",
       "           356,   766,  2089]),\n",
       " tensor([   11,   262,   717,  2063,   286,   262,  2646,  5983,   284, 10517,\n",
       "         11414,   290]),\n",
       " tensor([  271,   281,   555, 48544,   306,  2877,   837, 12704,  1517,   764,\n",
       "           220,   220,   220,   220,   220,   220,   220,   220,   220,  4451]),\n",
       " tensor([ 4360,   991,  2407, 25103,   290, 23310,   477,   262,   835,   832,\n",
       "           220, 16590,   705,    82, 19074,   306,  9048,  8886]),\n",
       " tensor([  260,  2673,   290, 43746, 14285,   220,   933,   959,   220]),\n",
       " tensor([ 270,  705,   82,  922,  329,  428,  277,  540,  837, 1201,  345,  460,\n",
       "         1254]),\n",
       " tensor([ 258, 4539, 1088,  290, 6529, 7650,  287,  465, 4226,  837, 3863,  780,\n",
       "          340, 4952,  514, 1165,  881,  546, 1521]),\n",
       " tensor([ 271,  663, 1245,  532,  262, 5888,  286, 4213,  287, 2506]),\n",
       " tensor([ 9930,   705,   260,  1297,   257, 17101,  1621]),\n",
       " tensor([ 4360,   340,   481,   655,   355,  1884,   302, 17938,   340,   355,\n",
       "          7468,  6918,   764,   220,   220,   220,   220,   220,   220,   220,\n",
       "           220]),\n",
       " tensor([  271, 22107,   284, 16143,   656,   257, 24344,   764,   220,   220,\n",
       "           220,   220,   220]),\n",
       " tensor([   64,  4451,  3807,   326,  4206,   703,   284,  5236,   262, 37154,\n",
       "         43972,  1022,   262, 44018, 49904,   286, 13644,   290,   883])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:11.169472Z",
     "start_time": "2025-09-26T01:37:11.149903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "input_data = data_collator([\n",
    "    {'input_ids': ids,\n",
    "     'attention_mask': torch.ones_like(ids)} for ids in query_response_tensors\n",
    "]).to(device)\n",
    "print(input_data)"
   ],
   "id": "6a08461572e03e05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   11,  1534,  1531, 10201,  6202,   837,   220, 17479, 14532,   290,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 8043,   837,  2647,   837,   290, 29923,  1837,   286,  1842,   764,\n",
      "           220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 2502,  8988,   479, 29456, 22160, 26744,   351, 41687,   290, 15403,\n",
      "          1108,   837,   543,   318,  1223,   645,   584, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 6888,   299,   470,   766,  1521,   597,  3807,   714,  1683,  5457,\n",
      "           307,   355,  4388,   287,   262, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [   76,  3316,   705,    82, 39769,   318,   880,   319,   663,   835,\n",
      "           284, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 1462,   852,   262, 25203,    12, 10899,  4420,  2089,  2196,   286,\n",
      "           262, 20278,   625,    12, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [   64,  8303,   379, 11783,   289,   970,  5516,  1042,   287,   262,\n",
      "          1182,   220, 48869,   220, 48869,   220, 48869,   340, 50256, 50256,\n",
      "         50256],\n",
      "        [  265,  1661,   257,  1643,  7758,   375,   859,  1512,   475,   220,\n",
      "           220,   425,  1239,  2936,   517, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [19188,   423,   257,  1049, 15602,   284,  2342,   837,  2592,   220,\n",
      "          3711,  8887, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  271, 14169,   837,   572, 12945,   290, 20105,   220,   220,   220,\n",
      "           220,  3474,   319,   257,  1180,  9450,  4469,   290, 50256, 50256,\n",
      "         50256],\n",
      "        [ 4360,   772,   981,   465,  3435,   389, 30598,  1473,  8258,   837,\n",
      "           511, 14532,   389, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  270,   705,    82,   257,  9058,   523, 13455,  2823,   326,   416,\n",
      "          3095, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [21754,  1577,  7559, 12692, 10148,   257,   348,  1901,   220,   220,\n",
      "           220,   220,   220,   220,   611,   340, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  338,  5729, 36979,   379,   703, 11932,   477,   428,   318,   220,\n",
      "           220,   220,   326,   262, 22051,   389,   523, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  271,   517,  3499,   357,   290,  7187,  1267,   621,   262,   582,\n",
      "           705, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [39240,   298,   290, 20868,  1107, 32481,   220,   220,   220,   220,\n",
      "           355,   262,   995,   290, 13735, 26969,  4832,   764,   220, 50256,\n",
      "         50256],\n",
      "        [ 5562,  4071,  2646,  3025,  1262,   286,  3809,    12,  2502, 10544,\n",
      "           287,  2223,  6918,   460,   691, 18330, 25030,   220, 50256, 50256,\n",
      "         50256],\n",
      "        [ 5661,  2646,   837,   588,   262, 12470,  2801,    12, 16514,   276,\n",
      "         26619,    12,   392,    12,    67,   480, 28763,   837,   318,  3983,\n",
      "         50256],\n",
      "        [ 1169,  1877,    12,  9526,  9891,   220,   364, 27906,  1108,   220,\n",
      "         17479,  8268, 16736,   284,   262,  1241,   286, 17131, 50256, 50256,\n",
      "         50256],\n",
      "        [  272, 30690,  5022,   286,  3223, 35704,   290,   837,  3690,   837,\n",
      "           617,  2769, 25303,   290, 10590, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  732,  1239,  1254,   356,   705,   260,   287,  3514,   220, 12518,\n",
      "           356,   766,  2089, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [   11,   262,   717,  2063,   286,   262,  2646,  5983,   284, 10517,\n",
      "         11414,   290, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  271,   281,   555, 48544,   306,  2877,   837, 12704,  1517,   764,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,  4451,\n",
      "         50256],\n",
      "        [ 4360,   991,  2407, 25103,   290, 23310,   477,   262,   835,   832,\n",
      "           220, 16590,   705,    82, 19074,   306,  9048,  8886, 50256, 50256,\n",
      "         50256],\n",
      "        [  260,  2673,   290, 43746, 14285,   220,   933,   959,   220, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  270,   705,    82,   922,   329,   428,   277,   540,   837,  1201,\n",
      "           345,   460,  1254, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [  258,  4539,  1088,   290,  6529,  7650,   287,   465,  4226,   837,\n",
      "          3863,   780,   340,  4952,   514,  1165,   881,   546,  1521, 50256,\n",
      "         50256],\n",
      "        [  271,   663,  1245,   532,   262,  5888,   286,  4213,   287,  2506,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 9930,   705,   260,  1297,   257, 17101,  1621, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [ 4360,   340,   481,   655,   355,  1884,   302, 17938,   340,   355,\n",
      "          7468,  6918,   764,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220],\n",
      "        [  271, 22107,   284, 16143,   656,   257, 24344,   764,   220,   220,\n",
      "           220,   220,   220, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256],\n",
      "        [   64,  4451,  3807,   326,  4206,   703,   284,  5236,   262, 37154,\n",
      "         43972,  1022,   262, 44018, 49904,   286, 13644,   290,   883, 50256,\n",
      "         50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:278: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/utils/generic.py:278: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:17.346134Z",
     "start_time": "2025-09-26T01:37:17.341205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_rewards(\n",
    "    input_data,\n",
    "    query_tensors,\n",
    "    response_tensors,\n",
    "    score_tensors\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        # 正在微调的模型所输出的token的logits和token的价值\n",
    "        # 模型输出所有token的概率分布\n",
    "        logits, values = model(**input_data) # b, seq, vocab\n",
    "        # 冻结的模型的输出和价值\n",
    "        ref_logits, _ = sft_model(**input_data)\n",
    "        # 正在微调的模型的输出的对数概率\n",
    "        logp = torch.nn.functional.log_softmax(logits[:, :-1, :], dim=-1)\n",
    "        # 冻结的模型的输出的对数概率\n",
    "        ref_logp = torch.nn.functional.log_softmax(ref_logits[:, :-1, :], dim=-1)\n",
    "        # 实际生成的token序列\n",
    "        labels = input_data['input_ids'][:, 1:] # b, seq\n",
    "        # 使用gather提取实际token的概率\n",
    "        logp = torch.gather(\n",
    "            logp,\n",
    "            2,\n",
    "            labels.unsqueeze(-1)\n",
    "        ).squeeze(-1) # batch, seq\n",
    "        ref_logp = torch.gather(\n",
    "            ref_logp,\n",
    "            2,\n",
    "            labels.unsqueeze(-1)\n",
    "        ).squeeze(-1) # batch, seq\n",
    "        # kl散度\n",
    "        kl = logp - ref_logp\n",
    "        # kl散度的权重\n",
    "        beta = 0.2\n",
    "        # 最终奖励的计算\n",
    "        rewards = - beta * kl\n",
    "        attention_mask = input_data['attention_mask']\n",
    "        masks = torch.zeros_like(attention_mask[:, 1:])\n",
    "        masks[:,:] = attention_mask[:, 1:]\n",
    "        for j in range(len(query_tensors)):\n",
    "            start = len(query_tensors[j]) - 1\n",
    "            end = start + len(response_tensors[j])\n",
    "            masks[j, :start] = 0\n",
    "            masks[j, end:] = 0\n",
    "            rewards[j, end - 1] += score_tensors[j]\n",
    "            rewards[j, :] *= masks[j, :]\n",
    "            values[j, :-1] *= masks[j, :]\n",
    "\n",
    "    return logp, rewards, values[:, :-1], masks"
   ],
   "id": "f27a228df6fe32aa",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:21.972082Z",
     "start_time": "2025-09-26T01:37:21.966684Z"
    }
   },
   "cell_type": "code",
   "source": "score_tensors\n",
   "id": "3b1ad4867510ebd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.9561),\n",
       " tensor(0.9919),\n",
       " tensor(0.9979),\n",
       " tensor(-0.9763),\n",
       " tensor(0.9562),\n",
       " tensor(-0.9988),\n",
       " tensor(-0.9871),\n",
       " tensor(0.2387),\n",
       " tensor(0.8755),\n",
       " tensor(0.9992),\n",
       " tensor(0.2430),\n",
       " tensor(-0.9997),\n",
       " tensor(0.2599),\n",
       " tensor(0.9940),\n",
       " tensor(0.9944),\n",
       " tensor(0.9956),\n",
       " tensor(-0.8964),\n",
       " tensor(-0.9049),\n",
       " tensor(0.6486),\n",
       " tensor(0.7333),\n",
       " tensor(-0.9872),\n",
       " tensor(-0.1871),\n",
       " tensor(0.9091),\n",
       " tensor(0.9994),\n",
       " tensor(-0.9970),\n",
       " tensor(0.9938),\n",
       " tensor(-0.9895),\n",
       " tensor(0.9503),\n",
       " tensor(0.9925),\n",
       " tensor(-0.9017),\n",
       " tensor(-0.9826),\n",
       " tensor(0.9989)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:25.234315Z",
     "start_time": "2025-09-26T01:37:24.701994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logprobs, rewards, values, masks = compute_rewards(\n",
    "    input_data,\n",
    "    query_tensors,\n",
    "    response_tensors,\n",
    "    score_tensors\n",
    ")\n",
    "print(rewards[0])\n",
    "print(input_data['input_ids'][0])\n",
    "print(input_data['attention_mask'][0])\n",
    "print(masks[0])\n",
    "print(values[0])"
   ],
   "id": "9585acfd635eec35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.9561, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000])\n",
      "tensor([   11,  1534,  1531, 10201,  6202,   837,   220, 17479, 14532,   290,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0.0000, -0.0000,  4.9234, -4.2811,  1.9289,  2.6755,  0.7618,  5.4120,\n",
      "        -0.9051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:28.092078Z",
     "start_time": "2025-09-26T01:37:28.086806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def masked_mean(values, mask):\n",
    "    # 计算带掩码的平均值\n",
    "    return (values * mask).sum() / mask.sum()\n",
    "\n",
    "def masked_var(values, mask):\n",
    "    # 计算带掩码的方差\n",
    "    mean = masked_mean(values, mask)\n",
    "    centred_values = values - mean\n",
    "    return masked_mean(centred_values ** 2, mask)\n",
    "\n",
    "def masked_whiten(values, mask):\n",
    "    '''\n",
    "    对数据进行带掩码的白化处理，\n",
    "    让有效数据的方差变为1，但均值保持不变\n",
    "    '''\n",
    "    mean, var = masked_mean(values, mask), masked_var(values, mask)\n",
    "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
    "    whitened += mean\n",
    "    return whitened\n",
    "\n",
    "def compute_advantage(rewards, values, masks):\n",
    "    '''\n",
    "    广义优势估计（GAE）\n",
    "    '''\n",
    "    lastgae = 0.0\n",
    "    advantage_reversed = []\n",
    "    seq_length = rewards.shape[-1]\n",
    "    gamma, lam = 1.0, 0.95\n",
    "\n",
    "    for t in reversed(range(seq_length)):\n",
    "        nextvalues = values[:, t + 1] if t < seq_length - 1 else 0.0\n",
    "        delta = rewards[:, t] + gamma * nextvalues - values[:, t]\n",
    "        lastgae = delta + gamma * lam * lastgae\n",
    "        advantage_reversed.append(lastgae)\n",
    "    advantages = torch.stack(advantage_reversed[::-1], dim=1)\n",
    "    advantages = masked_whiten(advantages, masks)\n",
    "\n",
    "    returns = advantages + values\n",
    "    return advantages, returns"
   ],
   "id": "30ee73beabd17bad",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:33.257887Z",
     "start_time": "2025-09-26T01:37:33.253748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "advantages, returns = compute_advantage(rewards, values, masks)\n",
    "print(advantages[0])\n",
    "print(returns[0])"
   ],
   "id": "df729db87f972e89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8922, -0.8971, -3.2641,  1.0217, -1.8616, -2.2757, -1.4354, -3.6997,\n",
      "        -0.8219, -0.7987, -0.7987, -0.7987, -0.7987, -0.7987, -0.7987, -0.7987,\n",
      "        -0.7987, -0.7987, -0.7987, -0.7987])\n",
      "tensor([-0.8922, -0.8971,  1.6592, -3.2594,  0.0673,  0.3998, -0.6735,  1.7123,\n",
      "        -1.7270, -0.7987, -0.7987, -0.7987, -0.7987, -0.7987, -0.7987, -0.7987,\n",
      "        -0.7987, -0.7987, -0.7987, -0.7987])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:36.363462Z",
     "start_time": "2025-09-26T01:37:36.360034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# 随机排列一下各个批次大小\n",
    "np.random.permutation(batch_size)"
   ],
   "id": "6d7bcfee4a4c1f7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 28,  8, 15,  5, 14, 27, 23, 26,  7, 30, 20, 16, 11, 22, 24,  9,\n",
       "        0, 29,  4, 17, 13, 31, 21, 25, 18, 10,  3,  1, 19,  6, 12])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:38.769185Z",
     "start_time": "2025-09-26T01:37:38.763454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 最小的批次大小\n",
    "mini_batch_size = 4\n",
    "# 训练 4 个 epoch\n",
    "ppo_epochs = 4\n",
    "# ε = 0.2\n",
    "cliprange_ratio = 0.2\n",
    "\n",
    "v_loss_coeff = 0.1\n",
    "# 比例的阈值\n",
    "ratio_threshold = 10\n",
    "\n",
    "def compute_loss(\n",
    "    old_logprobs, # 冻结的一份概率\n",
    "    values, # 价值\n",
    "    logprobs, # 正在微调的模型输出的对数概率\n",
    "    vpreds, # 预测的价值\n",
    "    masks, # 掩码\n",
    "    advantages, # 广义优势估计\n",
    "    returns # 回报\n",
    "):\n",
    "    # 比率\n",
    "    ratio = torch.exp(logprobs - old_logprobs)\n",
    "    # 比率 * 广义优势估计\n",
    "    pg_loss1 = - ratio * advantages\n",
    "    # clip(比率，1-ϵ,1+ϵ) * 广义优势估计\n",
    "    pg_loss2 = - torch.clamp(\n",
    "        ratio,\n",
    "        1 - cliprange_ratio,\n",
    "        1 + cliprange_ratio\n",
    "    ) * advantages\n",
    "    # 策略（gpt2-sft）的损失\n",
    "    pg_loss = masked_mean(torch.max(pg_loss1, pg_loss2), masks)\n",
    "    # 价值网络（价值头）的损失，mse\n",
    "    v_loss = masked_mean((vpreds - returns) ** 2, masks)\n",
    "    # 由于 正在微调的模型 = gpt2-sft + value_head\n",
    "    # 总的损失 = 策略网络的损失 + 0.1 * 价值网络的损失\n",
    "    loss = pg_loss + v_loss_coeff * v_loss\n",
    "    # 计算平均比率\n",
    "    avg_ratio = masked_mean(ratio, masks)\n",
    "    # 这一步不在ppo公式中\n",
    "    # 如果平均比率 > 10\n",
    "    if avg_ratio > ratio_threshold:\n",
    "        pg_loss = pg_loss * 0.0\n",
    "        v_loss = v_loss * 0.0\n",
    "        loss = loss * 0.0\n",
    "\n",
    "    return loss, v_loss\n",
    "\n",
    "def mini_batch_train():\n",
    "    for ep in range(ppo_epochs):\n",
    "        batch_inds = np.random.permutation(batch_size)\n",
    "        # range(0, 32, 4)\n",
    "        for start in range(0, batch_size, mini_batch_size):\n",
    "            # start = 0; end = 4\n",
    "            end = start + mini_batch_size\n",
    "            mini_batch_inds = batch_inds[start:end]\n",
    "\n",
    "            mb_model_inputs = {\n",
    "                'input_ids': input_data \\\n",
    "                    ['input_ids'] \\\n",
    "                    [mini_batch_inds],\n",
    "                'attention_mask': input_data \\\n",
    "                    ['attention_mask'] \\\n",
    "                    [mini_batch_inds]\n",
    "            }\n",
    "            # 模型的输出是token的logits和value\n",
    "            mb_logits, mb_vpreds = model(**mb_model_inputs)\n",
    "            # 去掉最后一个token\n",
    "            mb_logits = torch.nn.functional.log_softmax(\n",
    "                mb_logits[:, :-1, :],\n",
    "                dim=-1\n",
    "            )\n",
    "            # 取出真实标签对应的概率\n",
    "            mb_logprobs = torch.gather(\n",
    "                mb_logits,\n",
    "                2,\n",
    "                mb_model_inputs['input_ids'][:, 1:].unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "\n",
    "            loss, loss_v = compute_loss(\n",
    "                logprobs[mini_batch_inds],\n",
    "                values[mini_batch_inds],\n",
    "                mb_logprobs,\n",
    "                mb_vpreds[:, :-1],\n",
    "                masks[mini_batch_inds],\n",
    "                advantages[mini_batch_inds],\n",
    "                returns[mini_batch_inds]\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('loss/total', loss.item())\n",
    "    print('mini-batch training finished')"
   ],
   "id": "7e15db42ac984db0",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:37:53.051225Z",
     "start_time": "2025-09-26T01:37:44.381729Z"
    }
   },
   "cell_type": "code",
   "source": "mini_batch_train()",
   "id": "93d27c46264dd5a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/total 1.5086708068847656\n",
      "loss/total 1.9685832262039185\n",
      "loss/total 1.4921576976776123\n",
      "loss/total 1.9890475273132324\n",
      "loss/total 1.4088155031204224\n",
      "loss/total 1.8091586828231812\n",
      "loss/total 1.9725093841552734\n",
      "loss/total 1.543008804321289\n",
      "loss/total 1.4625900983810425\n",
      "loss/total 1.6584177017211914\n",
      "loss/total 1.389695405960083\n",
      "loss/total 1.21249258518219\n",
      "loss/total 1.3319447040557861\n",
      "loss/total 1.2083057165145874\n",
      "loss/total 1.6089109182357788\n",
      "loss/total 1.2492483854293823\n",
      "loss/total 1.3308871984481812\n",
      "loss/total 0.9942128658294678\n",
      "loss/total 1.2945976257324219\n",
      "loss/total 1.1975150108337402\n",
      "loss/total 1.4258742332458496\n",
      "loss/total 1.4625556468963623\n",
      "loss/total 1.2934088706970215\n",
      "loss/total 1.2743065357208252\n",
      "loss/total 1.2525415420532227\n",
      "loss/total 1.312488079071045\n",
      "loss/total 1.1764097213745117\n",
      "loss/total 1.2857826948165894\n",
      "loss/total 1.299506425857544\n",
      "loss/total 1.6054877042770386\n",
      "loss/total 1.113140344619751\n",
      "loss/total 0.9546226263046265\n",
      "mini-batch training finished\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T01:58:23.440062Z",
     "start_time": "2025-09-26T01:45:15.269398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # 生成补全内容（回复）\n",
    "        query_tensors = batch['input_ids'] # 提示词的张量\n",
    "        query_attention_masks = batch['attention_mask']\n",
    "\n",
    "        response_tensors = [] # 补全的张量\n",
    "        query_response_tensors = [] # 提示词+补全的张量\n",
    "        score_tensors = [] # 分数的张量\n",
    "\n",
    "        for i, query in enumerate(query_tensors):\n",
    "            query = query.to(device)\n",
    "            query_attention_mask = query_attention_masks[i].to(device)\n",
    "            # 随机挑一个补全的长度\n",
    "            new_tokens = random.choice(list(range(\n",
    "                output_min_length,\n",
    "                output_max_length)))\n",
    "            # 设置补全长度属性\n",
    "            generation_kwargs[\"max_new_tokens\"] = new_tokens\n",
    "            # 提示词 + 补全\n",
    "            query_response = model.generate(\n",
    "                input_ids=query.unsqueeze(0),\n",
    "                attention_mask=query_attention_mask.unsqueeze(0),\n",
    "                **generation_kwargs\n",
    "            ).squeeze(0)\n",
    "            # 补全的长度\n",
    "            response_len = len(query_response) - len(query)\n",
    "            # 补全的张量\n",
    "            response_tensors.append(query_response[-response_len:])\n",
    "            query_response_tensors.append(query_response)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # 提示词 + 补全 + reward_token\n",
    "                query_response_score = torch.cat([\n",
    "                    query_response,\n",
    "                    torch.tensor([REWARD_TOKEN_ID]).to(device)])\n",
    "                attention_mask = torch.ones_like(\n",
    "                    query_response_score,\n",
    "                    dtype=torch.long)\n",
    "                # 奖励模型的评分\n",
    "                score = reward_model(\n",
    "                    query_response_score.unsqueeze(0),\n",
    "                    attention_mask.unsqueeze(0)\n",
    "                ).squeeze(0)[-1]\n",
    "                # 将奖励模型的评分从(0,1)缩放到(-1,1)\n",
    "                score = 2 * (score - 0.5)\n",
    "            score_tensors.append(score)\n",
    "\n",
    "        input_data = data_collator([\n",
    "            {\n",
    "                'input_ids': ids,\n",
    "                'attention_mask': torch.ones_like(ids)\n",
    "            }\n",
    "            for ids in query_response_tensors\n",
    "        ]).to(device)\n",
    "\n",
    "        # 奖励和优势\n",
    "        logprobs, rewards, values, masks = compute_rewards(\n",
    "            input_data,\n",
    "            query_tensors,\n",
    "            response_tensors,\n",
    "            score_tensors\n",
    "        )\n",
    "        advantages, returns = compute_advantage(rewards, values, masks)\n",
    "\n",
    "        # 小批次训练\n",
    "        mini_batch_train()\n",
    "    print(f'epoch {epoch + 1} finished')"
   ],
   "id": "756b2c47919483ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/total 0.7508983612060547\n",
      "loss/total 1.0324691534042358\n",
      "loss/total 1.0763437747955322\n",
      "loss/total 1.1622765064239502\n",
      "loss/total 1.107187271118164\n",
      "loss/total 0.945980429649353\n",
      "loss/total 1.047513723373413\n",
      "loss/total 0.9599798321723938\n",
      "loss/total 1.0468543767929077\n",
      "loss/total 1.109917163848877\n",
      "loss/total 0.4298473298549652\n",
      "loss/total 0.6093572974205017\n",
      "loss/total 1.1704490184783936\n",
      "loss/total 0.7546617388725281\n",
      "loss/total 0.8411296606063843\n",
      "loss/total 0.5838620066642761\n",
      "loss/total 0.4295908212661743\n",
      "loss/total 0.8948656320571899\n",
      "loss/total 0.6778674125671387\n",
      "loss/total 0.3236315846443176\n",
      "loss/total 0.8596951365470886\n",
      "loss/total 0.9914372563362122\n",
      "loss/total 0.7081918716430664\n",
      "loss/total 0.9933494329452515\n",
      "loss/total 0.4940328299999237\n",
      "loss/total 0.7501310110092163\n",
      "loss/total 0.37403276562690735\n",
      "loss/total 0.9266468286514282\n",
      "loss/total 0.9018948674201965\n",
      "loss/total 1.1137652397155762\n",
      "loss/total 0.5823561549186707\n",
      "loss/total 0.742218017578125\n",
      "mini-batch training finished\n",
      "loss/total 0.8161994218826294\n",
      "loss/total 0.36287781596183777\n",
      "loss/total 0.610303521156311\n",
      "loss/total 0.7566224336624146\n",
      "loss/total 0.24890901148319244\n",
      "loss/total 0.24640712141990662\n",
      "loss/total 0.16832822561264038\n",
      "loss/total 0.5576406717300415\n",
      "loss/total 0.3102419376373291\n",
      "loss/total 0.12889400124549866\n",
      "loss/total 0.16674268245697021\n",
      "loss/total 0.708191990852356\n",
      "loss/total 0.35863441228866577\n",
      "loss/total 0.18232791125774384\n",
      "loss/total 0.4840685725212097\n",
      "loss/total 0.431517094373703\n",
      "loss/total 0.38669154047966003\n",
      "loss/total 0.012684203684329987\n",
      "loss/total 0.18701019883155823\n",
      "loss/total 0.5850759148597717\n",
      "loss/total 0.6374306082725525\n",
      "loss/total 0.11830459535121918\n",
      "loss/total 0.27025172114372253\n",
      "loss/total 0.2868124544620514\n",
      "loss/total 0.4253487288951874\n",
      "loss/total 0.31613990664482117\n",
      "loss/total 0.34660404920578003\n",
      "loss/total 0.43210548162460327\n",
      "loss/total 0.3761174976825714\n",
      "loss/total 0.3172703981399536\n",
      "loss/total -0.23626446723937988\n",
      "loss/total 0.1079382598400116\n",
      "mini-batch training finished\n",
      "loss/total 0.035585254430770874\n",
      "loss/total 0.2498367875814438\n",
      "loss/total 0.2703092694282532\n",
      "loss/total 0.1568869948387146\n",
      "loss/total -0.1103997603058815\n",
      "loss/total -0.11758535355329514\n",
      "loss/total 0.17049703001976013\n",
      "loss/total -0.08257152140140533\n",
      "loss/total -0.044696055352687836\n",
      "loss/total 0.06715076416730881\n",
      "loss/total 0.3116377294063568\n",
      "loss/total -0.18062236905097961\n",
      "loss/total -0.6745108366012573\n",
      "loss/total 0.015114836394786835\n",
      "loss/total -0.27453646063804626\n",
      "loss/total 0.21528753638267517\n",
      "loss/total -0.41378873586654663\n",
      "loss/total -0.3721922039985657\n",
      "loss/total 0.4023164212703705\n",
      "loss/total -0.09813092648983002\n",
      "loss/total 0.08898354321718216\n",
      "loss/total -0.12666377425193787\n",
      "loss/total 0.14967145025730133\n",
      "loss/total -0.584816038608551\n",
      "loss/total 0.08462363481521606\n",
      "loss/total -0.19269053637981415\n",
      "loss/total 0.020538639277219772\n",
      "loss/total -0.5144019722938538\n",
      "loss/total 0.011619523167610168\n",
      "loss/total -0.20484542846679688\n",
      "loss/total -0.08959747105836868\n",
      "loss/total -0.081435926258564\n",
      "mini-batch training finished\n",
      "loss/total 0.061147116124629974\n",
      "loss/total -0.4127991795539856\n",
      "loss/total -0.3162960410118103\n",
      "loss/total -0.40047603845596313\n",
      "loss/total -0.21476051211357117\n",
      "loss/total -0.44319412112236023\n",
      "loss/total 0.05254923552274704\n",
      "loss/total -0.4207398593425751\n",
      "loss/total 0.03351797163486481\n",
      "loss/total -0.2561725378036499\n",
      "loss/total -0.599824070930481\n",
      "loss/total -0.7759739756584167\n",
      "loss/total -0.023230116814374924\n",
      "loss/total -0.05513131618499756\n",
      "loss/total -0.600004255771637\n",
      "loss/total -0.952994167804718\n",
      "loss/total -0.24637465178966522\n",
      "loss/total -0.28301095962524414\n",
      "loss/total -0.12871411442756653\n",
      "loss/total -0.7327607870101929\n",
      "loss/total -0.6309815049171448\n",
      "loss/total -0.3119088113307953\n",
      "loss/total -0.6317782402038574\n",
      "loss/total -0.5762908458709717\n",
      "loss/total -0.7978635430335999\n",
      "loss/total -0.1309066116809845\n",
      "loss/total -0.5807886123657227\n",
      "loss/total -0.8196582198143005\n",
      "loss/total -0.3251868486404419\n",
      "loss/total 0.09827061742544174\n",
      "loss/total -0.5796203017234802\n",
      "loss/total -0.38651999831199646\n",
      "mini-batch training finished\n",
      "loss/total 0.9257757663726807\n",
      "loss/total 0.2709581255912781\n",
      "loss/total 0.30936312675476074\n",
      "loss/total 0.34838390350341797\n",
      "loss/total 0.7281605005264282\n",
      "loss/total -0.38506367802619934\n",
      "loss/total -0.013630039989948273\n",
      "loss/total 0.6446874737739563\n",
      "loss/total 0.28320813179016113\n",
      "loss/total 0.1525004804134369\n",
      "loss/total 0.2049793303012848\n",
      "loss/total 0.29772329330444336\n",
      "loss/total 0.12273894250392914\n",
      "loss/total 0.25148648023605347\n",
      "loss/total 0.030898287892341614\n",
      "loss/total 0.22541676461696625\n",
      "loss/total -0.1107463464140892\n",
      "loss/total -0.11970866471529007\n",
      "loss/total 0.20143499970436096\n",
      "loss/total 0.1781138777732849\n",
      "loss/total 0.33220812678337097\n",
      "loss/total 0.6930241584777832\n",
      "loss/total 0.2213728278875351\n",
      "loss/total -0.04793785884976387\n",
      "loss/total -0.3695540130138397\n",
      "loss/total 0.9019211530685425\n",
      "loss/total 0.036980267614126205\n",
      "loss/total 0.05290089547634125\n",
      "loss/total 0.29662778973579407\n",
      "loss/total -0.055082712322473526\n",
      "loss/total 0.33226197957992554\n",
      "loss/total 0.1986193060874939\n",
      "mini-batch training finished\n",
      "loss/total 0.027850672602653503\n",
      "loss/total 0.31632936000823975\n",
      "loss/total 0.20903292298316956\n",
      "loss/total 0.574592113494873\n",
      "loss/total 0.12067878991365433\n",
      "loss/total 0.40950894355773926\n",
      "loss/total 0.394351065158844\n",
      "loss/total 0.31605201959609985\n",
      "loss/total -0.25432372093200684\n",
      "loss/total 0.21025128662586212\n",
      "loss/total 0.23414359986782074\n",
      "loss/total 0.3710057735443115\n",
      "loss/total 0.19940884411334991\n",
      "loss/total 0.3196403384208679\n",
      "loss/total 0.4147813320159912\n",
      "loss/total 0.05614518001675606\n",
      "loss/total 0.24194835126399994\n",
      "loss/total 0.44741278886795044\n",
      "loss/total -0.1851266622543335\n",
      "loss/total 0.30105745792388916\n",
      "loss/total 0.04670342803001404\n",
      "loss/total -0.2611462473869324\n",
      "loss/total 0.1322154402732849\n",
      "loss/total 0.4239500164985657\n",
      "loss/total 0.3153623640537262\n",
      "loss/total -0.17531637847423553\n",
      "loss/total 0.35623323917388916\n",
      "loss/total 0.3251381516456604\n",
      "loss/total -0.07059802114963531\n",
      "loss/total 0.0561496764421463\n",
      "loss/total 0.458374559879303\n",
      "loss/total -0.11822110414505005\n",
      "mini-batch training finished\n",
      "loss/total 0.11059953272342682\n",
      "loss/total 0.1224500983953476\n",
      "loss/total -0.519429624080658\n",
      "loss/total 0.1690019965171814\n",
      "loss/total -0.35510215163230896\n",
      "loss/total -0.6735852956771851\n",
      "loss/total -0.2724319100379944\n",
      "loss/total 0.034234173595905304\n",
      "loss/total -0.824055552482605\n",
      "loss/total 0.0007710009813308716\n",
      "loss/total 0.20224758982658386\n",
      "loss/total -0.19908446073532104\n",
      "loss/total -0.5245035290718079\n",
      "loss/total -0.6089889407157898\n",
      "loss/total -0.37281811237335205\n",
      "loss/total -0.1235852837562561\n",
      "loss/total 0.29083529114723206\n",
      "loss/total -0.6195465326309204\n",
      "loss/total -0.4427277147769928\n",
      "loss/total 0.08973333984613419\n",
      "loss/total -0.8766227960586548\n",
      "loss/total -0.5609188079833984\n",
      "loss/total -0.5685510635375977\n",
      "loss/total -0.19657470285892487\n",
      "loss/total -0.15412114560604095\n",
      "loss/total -0.3965911269187927\n",
      "loss/total 0.04857400059700012\n",
      "loss/total -0.8704754710197449\n",
      "loss/total -0.49101880192756653\n",
      "loss/total -0.265276700258255\n",
      "loss/total -0.2884526550769806\n",
      "loss/total -0.42843788862228394\n",
      "mini-batch training finished\n",
      "loss/total 0.25669318437576294\n",
      "loss/total 0.01514432579278946\n",
      "loss/total -0.048962004482746124\n",
      "loss/total 0.3567178249359131\n",
      "loss/total -0.05676665157079697\n",
      "loss/total -0.6537134647369385\n",
      "loss/total 0.3063638210296631\n",
      "loss/total 0.22245047986507416\n",
      "loss/total -0.33390605449676514\n",
      "loss/total 0.18124723434448242\n",
      "loss/total -0.546355664730072\n",
      "loss/total -0.1084900051355362\n",
      "loss/total 0.1855553388595581\n",
      "loss/total -0.5624180436134338\n",
      "loss/total 0.266154408454895\n",
      "loss/total 0.3057929575443268\n",
      "loss/total -0.38235509395599365\n",
      "loss/total 0.6029958128929138\n",
      "loss/total -0.10971267521381378\n",
      "loss/total -0.3213573396205902\n",
      "loss/total 0.044690635055303574\n",
      "loss/total -0.6000570058822632\n",
      "loss/total 0.2755320966243744\n",
      "loss/total -0.5968821048736572\n",
      "loss/total -0.165596142411232\n",
      "loss/total -0.27479714155197144\n",
      "loss/total -0.27301937341690063\n",
      "loss/total 0.27376073598861694\n",
      "loss/total 0.16993173956871033\n",
      "loss/total -0.03714882954955101\n",
      "loss/total -0.36050835251808167\n",
      "loss/total -0.49948832392692566\n",
      "mini-batch training finished\n",
      "loss/total 0.5408536195755005\n",
      "loss/total 0.2525131404399872\n",
      "loss/total 0.7915398478507996\n",
      "loss/total 0.3774700462818146\n",
      "loss/total 0.11620834469795227\n",
      "loss/total -0.27743881940841675\n",
      "loss/total 0.5618016719818115\n",
      "loss/total 0.3234335780143738\n",
      "loss/total 0.49665120244026184\n",
      "loss/total 0.14689487218856812\n",
      "loss/total 0.0715692937374115\n",
      "loss/total 0.1487252414226532\n",
      "loss/total 0.08250390738248825\n",
      "loss/total 0.06739174574613571\n",
      "loss/total 0.1993371993303299\n",
      "loss/total 0.4997062683105469\n",
      "loss/total 0.502768337726593\n",
      "loss/total -0.17560863494873047\n",
      "loss/total 0.2860223352909088\n",
      "loss/total 0.23193061351776123\n",
      "loss/total 0.19918854534626007\n",
      "loss/total -0.04581637680530548\n",
      "loss/total -0.025724217295646667\n",
      "loss/total 0.44828909635543823\n",
      "loss/total -0.13176801800727844\n",
      "loss/total 0.5745830535888672\n",
      "loss/total -0.09475409984588623\n",
      "loss/total 0.693864107131958\n",
      "loss/total -0.24469366669654846\n",
      "loss/total 0.09364362061023712\n",
      "loss/total 0.12490349262952805\n",
      "loss/total 0.15739591419696808\n",
      "mini-batch training finished\n",
      "loss/total -0.4700438380241394\n",
      "loss/total 0.02486167848110199\n",
      "loss/total -0.2032679319381714\n",
      "loss/total 0.34314918518066406\n",
      "loss/total 0.3563055992126465\n",
      "loss/total -0.2607786953449249\n",
      "loss/total -0.2228495478630066\n",
      "loss/total 0.011684760451316833\n",
      "loss/total -0.19435453414916992\n",
      "loss/total -0.03525415062904358\n",
      "loss/total 0.473368763923645\n",
      "loss/total -0.5938680171966553\n",
      "loss/total 0.022541437298059464\n",
      "loss/total -0.5020920038223267\n",
      "loss/total -0.21983203291893005\n",
      "loss/total -0.5927811861038208\n",
      "loss/total -0.03712918609380722\n",
      "loss/total -0.13895359635353088\n",
      "loss/total -0.11139173805713654\n",
      "loss/total 0.11939150840044022\n",
      "loss/total -0.8439916372299194\n",
      "loss/total -0.48930665850639343\n",
      "loss/total 0.590049147605896\n",
      "loss/total -0.8495622277259827\n",
      "loss/total -0.15517637133598328\n",
      "loss/total -0.008973881602287292\n",
      "loss/total -0.13109323382377625\n",
      "loss/total -0.09644807130098343\n",
      "loss/total -0.009721886366605759\n",
      "loss/total 0.01837553270161152\n",
      "loss/total -0.8123973608016968\n",
      "loss/total -0.474310964345932\n",
      "mini-batch training finished\n",
      "loss/total -0.1395353078842163\n",
      "loss/total -0.2097412347793579\n",
      "loss/total 0.0850798636674881\n",
      "loss/total -0.35338613390922546\n",
      "loss/total 0.2767272889614105\n",
      "loss/total 0.035935208201408386\n",
      "loss/total 0.509820818901062\n",
      "loss/total 0.020641226321458817\n",
      "loss/total 0.42145654559135437\n",
      "loss/total -0.6029554605484009\n",
      "loss/total -0.31607937812805176\n",
      "loss/total -0.2222122699022293\n",
      "loss/total 0.34731727838516235\n",
      "loss/total -0.24575792253017426\n",
      "loss/total -0.32651013135910034\n",
      "loss/total 0.20632722973823547\n",
      "loss/total 0.3396512567996979\n",
      "loss/total 0.4533599019050598\n",
      "loss/total -0.11252404749393463\n",
      "loss/total -0.061210229992866516\n",
      "loss/total -0.044235266745090485\n",
      "loss/total -0.3083995282649994\n",
      "loss/total -0.8798474669456482\n",
      "loss/total -0.3838806748390198\n",
      "loss/total -0.30828386545181274\n",
      "loss/total -0.1415380984544754\n",
      "loss/total 0.12385626137256622\n",
      "loss/total -0.19907626509666443\n",
      "loss/total 0.06148393452167511\n",
      "loss/total -0.5687702298164368\n",
      "loss/total 0.019252624362707138\n",
      "loss/total 0.08582139760255814\n",
      "mini-batch training finished\n",
      "loss/total -0.037381332367658615\n",
      "loss/total 0.08073791116476059\n",
      "loss/total -0.17166495323181152\n",
      "loss/total 0.4508884847164154\n",
      "loss/total -0.1571439951658249\n",
      "loss/total 0.021848164498806\n",
      "loss/total 0.6882553100585938\n",
      "loss/total 0.4350087642669678\n",
      "loss/total 0.40069252252578735\n",
      "loss/total 0.14527557790279388\n",
      "loss/total -0.2513499855995178\n",
      "loss/total 0.3986017107963562\n",
      "loss/total 0.36581334471702576\n",
      "loss/total -0.3465583026409149\n",
      "loss/total -0.21832425892353058\n",
      "loss/total -0.010274261236190796\n",
      "loss/total 0.2245180755853653\n",
      "loss/total 0.08108983188867569\n",
      "loss/total 0.20899248123168945\n",
      "loss/total 0.07074525952339172\n",
      "loss/total -0.22186395525932312\n",
      "loss/total -0.040844909846782684\n",
      "loss/total -0.0023504644632339478\n",
      "loss/total -0.20781488716602325\n",
      "loss/total -0.19304674863815308\n",
      "loss/total 0.3475917875766754\n",
      "loss/total 0.005971930921077728\n",
      "loss/total -0.006902199238538742\n",
      "loss/total -0.2877042293548584\n",
      "loss/total 0.4344336688518524\n",
      "loss/total -0.38653573393821716\n",
      "loss/total 0.15457218885421753\n",
      "mini-batch training finished\n",
      "loss/total -0.21110916137695312\n",
      "loss/total 0.08644795417785645\n",
      "loss/total -0.2778528928756714\n",
      "loss/total 0.2018650472164154\n",
      "loss/total 0.17653346061706543\n",
      "loss/total 0.004085607826709747\n",
      "loss/total 0.3244393467903137\n",
      "loss/total 0.22330747544765472\n",
      "loss/total -0.7316622138023376\n",
      "loss/total -0.18307557702064514\n",
      "loss/total 0.036524705588817596\n",
      "loss/total 0.07528901845216751\n",
      "loss/total -0.13102449476718903\n",
      "loss/total -0.011306270956993103\n",
      "loss/total 0.058582454919815063\n",
      "loss/total 0.4859285354614258\n",
      "loss/total -0.3963033854961395\n",
      "loss/total -0.008176520466804504\n",
      "loss/total -0.07582198083400726\n",
      "loss/total 0.23332542181015015\n",
      "loss/total 0.06525837630033493\n",
      "loss/total -0.3407360315322876\n",
      "loss/total -0.1795736849308014\n",
      "loss/total 0.31516796350479126\n",
      "loss/total -0.05316856503486633\n",
      "loss/total 0.45423874258995056\n",
      "loss/total -0.27271923422813416\n",
      "loss/total 0.0032792985439300537\n",
      "loss/total -0.050829026848077774\n",
      "loss/total -0.5649290680885315\n",
      "loss/total -0.21171250939369202\n",
      "loss/total -0.17833000421524048\n",
      "mini-batch training finished\n",
      "loss/total -0.2866308093070984\n",
      "loss/total -0.054490044713020325\n",
      "loss/total 0.12339755892753601\n",
      "loss/total 0.2477780282497406\n",
      "loss/total 0.16956230998039246\n",
      "loss/total -0.04662831872701645\n",
      "loss/total 0.3442223370075226\n",
      "loss/total 0.0439629852771759\n",
      "loss/total 0.030371777713298798\n",
      "loss/total 0.43562862277030945\n",
      "loss/total -0.09756854176521301\n",
      "loss/total -0.35348522663116455\n",
      "loss/total -0.16762347519397736\n",
      "loss/total -0.10842190682888031\n",
      "loss/total -0.25638771057128906\n",
      "loss/total 0.1703537255525589\n",
      "loss/total -0.6374537944793701\n",
      "loss/total -0.18983030319213867\n",
      "loss/total -0.04061003774404526\n",
      "loss/total -0.1916128396987915\n",
      "loss/total 0.1517271101474762\n",
      "loss/total 0.29653850197792053\n",
      "loss/total 0.05017654970288277\n",
      "loss/total -0.1360338032245636\n",
      "loss/total 0.06946103274822235\n",
      "loss/total 0.2319524884223938\n",
      "loss/total -0.1604360193014145\n",
      "loss/total -0.49697139859199524\n",
      "loss/total -0.18907202780246735\n",
      "loss/total -0.2971382737159729\n",
      "loss/total 0.25406405329704285\n",
      "loss/total -0.15098246932029724\n",
      "mini-batch training finished\n",
      "loss/total 0.5321206450462341\n",
      "loss/total -0.1259596049785614\n",
      "loss/total 0.25915223360061646\n",
      "loss/total 0.09937978535890579\n",
      "loss/total -0.3880327343940735\n",
      "loss/total 0.27750223875045776\n",
      "loss/total -0.11095856130123138\n",
      "loss/total 0.6342505812644958\n",
      "loss/total -0.20101362466812134\n",
      "loss/total -0.3761517107486725\n",
      "loss/total -0.16502530872821808\n",
      "loss/total 0.251044899225235\n",
      "loss/total 0.4319600462913513\n",
      "loss/total -0.3763314485549927\n",
      "loss/total 0.01921963319182396\n",
      "loss/total 0.6027663946151733\n",
      "loss/total 0.2806481719017029\n",
      "loss/total 0.051668133586645126\n",
      "loss/total -0.3017759621143341\n",
      "loss/total 0.24491947889328003\n",
      "loss/total -0.34049633145332336\n",
      "loss/total 0.16284538805484772\n",
      "loss/total -0.07760212570428848\n",
      "loss/total -0.01277298852801323\n",
      "loss/total 0.1591627597808838\n",
      "loss/total -0.1849665641784668\n",
      "loss/total 0.035183440893888474\n",
      "loss/total -0.5706896781921387\n",
      "loss/total -0.054750122129917145\n",
      "loss/total 0.8049779534339905\n",
      "loss/total -0.28904104232788086\n",
      "loss/total -0.025880835950374603\n",
      "mini-batch training finished\n",
      "loss/total 0.4898035526275635\n",
      "loss/total -0.2551005184650421\n",
      "loss/total -0.011993996798992157\n",
      "loss/total -0.07178649306297302\n",
      "loss/total 0.2692521810531616\n",
      "loss/total 0.2695397138595581\n",
      "loss/total 0.28532081842422485\n",
      "loss/total 0.31648164987564087\n",
      "loss/total -0.1780773103237152\n",
      "loss/total -0.04521981626749039\n",
      "loss/total -0.08428414911031723\n",
      "loss/total -0.07298335433006287\n",
      "loss/total -0.025598227977752686\n",
      "loss/total -0.018769487738609314\n",
      "loss/total 0.21503326296806335\n",
      "loss/total 0.3946986794471741\n",
      "loss/total -0.05851767212152481\n",
      "loss/total -0.46646609902381897\n",
      "loss/total 0.3997465670108795\n",
      "loss/total 0.2131175994873047\n",
      "loss/total 0.1471225917339325\n",
      "loss/total 0.2870524823665619\n",
      "loss/total -0.23937001824378967\n",
      "loss/total -0.2560979425907135\n",
      "loss/total -0.1420617252588272\n",
      "loss/total -0.6517944931983948\n",
      "loss/total 0.11541246622800827\n",
      "loss/total -0.07266184687614441\n",
      "loss/total 0.4945313036441803\n",
      "loss/total -0.5483836531639099\n",
      "loss/total 0.27973151206970215\n",
      "loss/total 0.05221391096711159\n",
      "mini-batch training finished\n",
      "loss/total 0.4221661686897278\n",
      "loss/total -0.22464948892593384\n",
      "loss/total -0.21625375747680664\n",
      "loss/total 0.2537733018398285\n",
      "loss/total 0.7411468029022217\n",
      "loss/total 0.33327749371528625\n",
      "loss/total 0.9786339998245239\n",
      "loss/total -0.16400058567523956\n",
      "loss/total 0.3935643434524536\n",
      "loss/total 0.07578504085540771\n",
      "loss/total 0.15505342185497284\n",
      "loss/total -0.0767807886004448\n",
      "loss/total 0.5251213312149048\n",
      "loss/total -0.4231198728084564\n",
      "loss/total 0.15746963024139404\n",
      "loss/total 0.25228768587112427\n",
      "loss/total 0.052879273891448975\n",
      "loss/total 0.08194605261087418\n",
      "loss/total 0.6616628766059875\n",
      "loss/total 0.3030352294445038\n",
      "loss/total 0.0492582768201828\n",
      "loss/total -0.4328487813472748\n",
      "loss/total 0.0728667601943016\n",
      "loss/total 0.10236568003892899\n",
      "loss/total -0.11337105929851532\n",
      "loss/total -0.052045971155166626\n",
      "loss/total 0.29302674531936646\n",
      "loss/total 0.010552063584327698\n",
      "loss/total 0.010472722351551056\n",
      "loss/total -0.11315008997917175\n",
      "loss/total 0.30700355768203735\n",
      "loss/total 0.44827789068222046\n",
      "mini-batch training finished\n",
      "loss/total -0.06156543269753456\n",
      "loss/total -0.19234836101531982\n",
      "loss/total 0.1437116116285324\n",
      "loss/total -0.15209151804447174\n",
      "loss/total 0.2453400194644928\n",
      "loss/total 0.20676620304584503\n",
      "loss/total -0.07633969187736511\n",
      "loss/total 0.46710896492004395\n",
      "loss/total 0.058391451835632324\n",
      "loss/total 0.27878013253211975\n",
      "loss/total 0.3843173384666443\n",
      "loss/total -0.745808482170105\n",
      "loss/total -0.30517128109931946\n",
      "loss/total -0.11910735815763474\n",
      "loss/total -0.3277088403701782\n",
      "loss/total 0.2830093204975128\n",
      "loss/total 0.5391026735305786\n",
      "loss/total -0.40117132663726807\n",
      "loss/total 0.0063357725739479065\n",
      "loss/total 0.3661714792251587\n",
      "loss/total -0.22313125431537628\n",
      "loss/total -0.4398157596588135\n",
      "loss/total 0.09947922825813293\n",
      "loss/total -0.72432941198349\n",
      "loss/total -0.10022233426570892\n",
      "loss/total 0.30347609519958496\n",
      "loss/total -0.5632338523864746\n",
      "loss/total 0.34563136100769043\n",
      "loss/total -0.45985037088394165\n",
      "loss/total -0.19968892633914948\n",
      "loss/total -0.0640912875533104\n",
      "loss/total 0.2742308974266052\n",
      "mini-batch training finished\n",
      "loss/total -0.06015421450138092\n",
      "loss/total -0.025396309792995453\n",
      "loss/total -0.10723506659269333\n",
      "loss/total 0.28762879967689514\n",
      "loss/total -0.2919655442237854\n",
      "loss/total -0.49731573462486267\n",
      "loss/total 0.058953262865543365\n",
      "loss/total 0.2934848368167877\n",
      "loss/total -0.48603594303131104\n",
      "loss/total -0.2845509350299835\n",
      "loss/total -0.11835887283086777\n",
      "loss/total -0.2651439309120178\n",
      "loss/total -0.5575674772262573\n",
      "loss/total -0.06057046353816986\n",
      "loss/total 0.39972931146621704\n",
      "loss/total 0.06842862069606781\n",
      "loss/total -0.0820586085319519\n",
      "loss/total -0.5028666257858276\n",
      "loss/total -0.02465582638978958\n",
      "loss/total 0.01923692226409912\n",
      "loss/total 0.15516901016235352\n",
      "loss/total -0.15715759992599487\n",
      "loss/total -0.4654879570007324\n",
      "loss/total -0.38507384061813354\n",
      "loss/total -0.020416773855686188\n",
      "loss/total -0.3030308485031128\n",
      "loss/total 0.03508392721414566\n",
      "loss/total -0.603637158870697\n",
      "loss/total -0.10859664529561996\n",
      "loss/total -0.4180583357810974\n",
      "loss/total -0.5956411361694336\n",
      "loss/total 0.19336621463298798\n",
      "mini-batch training finished\n",
      "loss/total 0.5087305903434753\n",
      "loss/total 0.04238617792725563\n",
      "loss/total 0.4120364785194397\n",
      "loss/total 0.4337000548839569\n",
      "loss/total 0.3185455799102783\n",
      "loss/total -0.15655776858329773\n",
      "loss/total 0.2125583291053772\n",
      "loss/total 0.4262998402118683\n",
      "loss/total -0.014608681201934814\n",
      "loss/total -0.18104711174964905\n",
      "loss/total 0.013847477734088898\n",
      "loss/total 0.3741954565048218\n",
      "loss/total 0.5216776132583618\n",
      "loss/total -0.09755787998437881\n",
      "loss/total 0.0672777071595192\n",
      "loss/total 0.7210742831230164\n",
      "loss/total -0.1491762101650238\n",
      "loss/total 0.6777352094650269\n",
      "loss/total 0.0035227909684181213\n",
      "loss/total -0.3806252181529999\n",
      "loss/total -0.12651313841342926\n",
      "loss/total 0.32569971680641174\n",
      "loss/total -0.2459566444158554\n",
      "loss/total 0.6426752209663391\n",
      "loss/total 0.3541959524154663\n",
      "loss/total 0.44964176416397095\n",
      "loss/total -0.14418835937976837\n",
      "loss/total -0.04485650360584259\n",
      "loss/total 0.37143170833587646\n",
      "loss/total -0.20413607358932495\n",
      "loss/total 0.5710567235946655\n",
      "loss/total -0.33516231179237366\n",
      "mini-batch training finished\n",
      "loss/total 0.10497258603572845\n",
      "loss/total 0.25192081928253174\n",
      "loss/total -0.3844110667705536\n",
      "loss/total -0.35783374309539795\n",
      "loss/total -0.6269095540046692\n",
      "loss/total -0.01138962060213089\n",
      "loss/total -0.02605869621038437\n",
      "loss/total 0.46390557289123535\n",
      "loss/total 0.21965566277503967\n",
      "loss/total -0.5249890089035034\n",
      "loss/total 0.49389177560806274\n",
      "loss/total -0.37995511293411255\n",
      "loss/total 0.19153302907943726\n",
      "loss/total -0.4040895998477936\n",
      "loss/total -0.3091697096824646\n",
      "loss/total -0.7555516958236694\n",
      "loss/total -0.16965359449386597\n",
      "loss/total -0.018475640565156937\n",
      "loss/total -0.343160480260849\n",
      "loss/total -0.005477689206600189\n",
      "loss/total -0.36303433775901794\n",
      "loss/total -0.15334643423557281\n",
      "loss/total -0.7135665416717529\n",
      "loss/total 0.169564887881279\n",
      "loss/total -0.31184107065200806\n",
      "loss/total 0.08750282973051071\n",
      "loss/total 0.2597430944442749\n",
      "loss/total -0.38294532895088196\n",
      "loss/total -0.4738040864467621\n",
      "loss/total -0.47046443819999695\n",
      "loss/total -0.5014368891716003\n",
      "loss/total -0.03491854667663574\n",
      "mini-batch training finished\n",
      "loss/total 0.31168362498283386\n",
      "loss/total -0.1300649493932724\n",
      "loss/total -0.3003769516944885\n",
      "loss/total 0.35727226734161377\n",
      "loss/total 0.36029475927352905\n",
      "loss/total -0.05832814425230026\n",
      "loss/total 0.86930912733078\n",
      "loss/total 0.16780905425548553\n",
      "loss/total 0.04119041562080383\n",
      "loss/total 0.33196452260017395\n",
      "loss/total -0.2549362778663635\n",
      "loss/total 0.2161359190940857\n",
      "loss/total -0.1264759749174118\n",
      "loss/total 0.09269430488348007\n",
      "loss/total 0.4520922303199768\n",
      "loss/total -0.1426468789577484\n",
      "loss/total 0.02367280423641205\n",
      "loss/total -0.03453870862722397\n",
      "loss/total 0.048605065792798996\n",
      "loss/total 0.5008233785629272\n",
      "loss/total 0.09453269094228745\n",
      "loss/total 0.13839027285575867\n",
      "loss/total -0.08173158764839172\n",
      "loss/total -0.5802252292633057\n",
      "loss/total -0.09673263877630234\n",
      "loss/total -0.12019117176532745\n",
      "loss/total 0.192037433385849\n",
      "loss/total -0.06783376634120941\n",
      "loss/total -0.07140565663576126\n",
      "loss/total -0.12588238716125488\n",
      "loss/total -0.09526044875383377\n",
      "loss/total 0.562615692615509\n",
      "mini-batch training finished\n",
      "loss/total 0.11653608083724976\n",
      "loss/total 1.166352391242981\n",
      "loss/total 0.17943677306175232\n",
      "loss/total 0.5796542167663574\n",
      "loss/total 0.3691904842853546\n",
      "loss/total 0.5667351484298706\n",
      "loss/total 0.20655034482479095\n",
      "loss/total 0.01271575316786766\n",
      "loss/total 0.1886192113161087\n",
      "loss/total 0.5861541628837585\n",
      "loss/total 0.3233107626438141\n",
      "loss/total 0.015763908624649048\n",
      "loss/total 0.28892287611961365\n",
      "loss/total -0.2743845582008362\n",
      "loss/total 0.33334964513778687\n",
      "loss/total 0.6084108352661133\n",
      "loss/total -0.03511899709701538\n",
      "loss/total 0.4675326645374298\n",
      "loss/total 0.5463106036186218\n",
      "loss/total -0.20091399550437927\n",
      "loss/total 0.3170730769634247\n",
      "loss/total 0.19167083501815796\n",
      "loss/total 0.4995579719543457\n",
      "loss/total 0.18772460520267487\n",
      "loss/total 0.00804482027888298\n",
      "loss/total -0.39746445417404175\n",
      "loss/total 0.16238167881965637\n",
      "loss/total 0.18385638296604156\n",
      "loss/total 0.05758281797170639\n",
      "loss/total 0.2682054936885834\n",
      "loss/total 0.6306964755058289\n",
      "loss/total 0.7924440503120422\n",
      "mini-batch training finished\n",
      "loss/total -0.36033642292022705\n",
      "loss/total 1.0536860227584839\n",
      "loss/total -0.5121381282806396\n",
      "loss/total 0.36351901292800903\n",
      "loss/total 0.07935869693756104\n",
      "loss/total -0.47489356994628906\n",
      "loss/total -0.5096156597137451\n",
      "loss/total -0.05920756608247757\n",
      "loss/total -0.6622693538665771\n",
      "loss/total -0.5590935945510864\n",
      "loss/total -0.3581194579601288\n",
      "loss/total 0.3351563811302185\n",
      "loss/total 0.045654796063899994\n",
      "loss/total 0.20022723078727722\n",
      "loss/total 0.2973407506942749\n",
      "loss/total -0.5130101442337036\n",
      "loss/total -0.037012822926044464\n",
      "loss/total -0.48092150688171387\n",
      "loss/total -0.596400260925293\n",
      "loss/total -0.1907513439655304\n",
      "loss/total -0.15259085595607758\n",
      "loss/total 0.6850016117095947\n",
      "loss/total -0.6368880271911621\n",
      "loss/total -0.009314574301242828\n",
      "loss/total 0.9887768030166626\n",
      "loss/total -0.6011971831321716\n",
      "loss/total -0.3104765713214874\n",
      "loss/total -0.4072158932685852\n",
      "loss/total -0.3991944491863251\n",
      "loss/total -0.1742246448993683\n",
      "loss/total -0.2555690109729767\n",
      "loss/total -0.3213953971862793\n",
      "mini-batch training finished\n",
      "loss/total -0.061821721494197845\n",
      "loss/total -0.4111209809780121\n",
      "loss/total -0.4438842833042145\n",
      "loss/total -0.36312875151634216\n",
      "loss/total -0.401284396648407\n",
      "loss/total 0.4525911808013916\n",
      "loss/total -0.17564836144447327\n",
      "loss/total -0.41465821862220764\n",
      "loss/total -0.7285193800926208\n",
      "loss/total -0.26038676500320435\n",
      "loss/total -0.5670495629310608\n",
      "loss/total -0.08445461839437485\n",
      "loss/total -0.45708346366882324\n",
      "loss/total -0.24521949887275696\n",
      "loss/total 0.05380113422870636\n",
      "loss/total -0.4191370904445648\n",
      "loss/total -0.5078361630439758\n",
      "loss/total -0.37105125188827515\n",
      "loss/total -0.45526033639907837\n",
      "loss/total -0.7803877592086792\n",
      "loss/total 0.3530251979827881\n",
      "loss/total -0.6075013875961304\n",
      "loss/total -0.23826992511749268\n",
      "loss/total -0.37503787875175476\n",
      "loss/total -0.5144447684288025\n",
      "loss/total -0.32616761326789856\n",
      "loss/total -0.4077664315700531\n",
      "loss/total -0.798680305480957\n",
      "loss/total -0.19511772692203522\n",
      "loss/total -0.8785155415534973\n",
      "loss/total 0.16071316599845886\n",
      "loss/total -0.5244988799095154\n",
      "mini-batch training finished\n",
      "loss/total 0.711552619934082\n",
      "loss/total -0.03629845753312111\n",
      "loss/total -0.1984003186225891\n",
      "loss/total 0.8557873964309692\n",
      "loss/total 0.18621140718460083\n",
      "loss/total 0.22145313024520874\n",
      "loss/total 0.38654935359954834\n",
      "loss/total -0.0592205747961998\n",
      "loss/total -0.18199022114276886\n",
      "loss/total -0.22673675417900085\n",
      "loss/total -0.05580949783325195\n",
      "loss/total 0.44215157628059387\n",
      "loss/total 0.11682106554508209\n",
      "loss/total 0.19270636141300201\n",
      "loss/total -0.10998070240020752\n",
      "loss/total 0.744696319103241\n",
      "loss/total 0.09686285257339478\n",
      "loss/total 0.13626515865325928\n",
      "loss/total 0.028176050633192062\n",
      "loss/total -0.4683836102485657\n",
      "loss/total -0.3077535629272461\n",
      "loss/total 0.8987032175064087\n",
      "loss/total 0.4875568151473999\n",
      "loss/total -0.40597617626190186\n",
      "loss/total 0.09564178436994553\n",
      "loss/total -0.07076146453619003\n",
      "loss/total -0.038284555077552795\n",
      "loss/total -0.4065435826778412\n",
      "loss/total 0.36099255084991455\n",
      "loss/total 0.1381511390209198\n",
      "loss/total -0.12875795364379883\n",
      "loss/total 0.6872237920761108\n",
      "mini-batch training finished\n",
      "loss/total 0.1091240793466568\n",
      "loss/total 0.949560821056366\n",
      "loss/total 0.26536107063293457\n",
      "loss/total -0.11781399697065353\n",
      "loss/total 0.24890750646591187\n",
      "loss/total -0.025276482105255127\n",
      "loss/total -0.05216751992702484\n",
      "loss/total 0.2928198575973511\n",
      "loss/total 0.05449962988495827\n",
      "loss/total 0.0156022310256958\n",
      "loss/total 0.07091480493545532\n",
      "loss/total 0.7661908864974976\n",
      "loss/total 0.17140638828277588\n",
      "loss/total -0.19610196352005005\n",
      "loss/total 0.5720938444137573\n",
      "loss/total -0.5258739590644836\n",
      "loss/total -0.28579893708229065\n",
      "loss/total -0.4263575077056885\n",
      "loss/total 0.42337897419929504\n",
      "loss/total 0.7755218148231506\n",
      "loss/total 0.016255084425210953\n",
      "loss/total 0.45490747690200806\n",
      "loss/total -0.11587197333574295\n",
      "loss/total -0.08346913754940033\n",
      "loss/total 0.3544119596481323\n",
      "loss/total 0.1839669644832611\n",
      "loss/total 0.35048550367355347\n",
      "loss/total -0.12076540291309357\n",
      "loss/total -0.2560299336910248\n",
      "loss/total -0.740646243095398\n",
      "loss/total 0.7617412805557251\n",
      "loss/total -0.11046678572893143\n",
      "mini-batch training finished\n",
      "loss/total -0.3636276125907898\n",
      "loss/total 0.2697036862373352\n",
      "loss/total 0.2597692012786865\n",
      "loss/total 0.49228811264038086\n",
      "loss/total -0.22449080646038055\n",
      "loss/total -0.01958300918340683\n",
      "loss/total 0.6549222469329834\n",
      "loss/total 0.5981680154800415\n",
      "loss/total 0.9158773422241211\n",
      "loss/total 0.09851882606744766\n",
      "loss/total 0.028139598667621613\n",
      "loss/total -0.5970898270606995\n",
      "loss/total 0.14203909039497375\n",
      "loss/total -0.18482157588005066\n",
      "loss/total -0.008409656584262848\n",
      "loss/total 0.322675883769989\n",
      "loss/total -0.29866063594818115\n",
      "loss/total 0.6963695883750916\n",
      "loss/total 0.45431774854660034\n",
      "loss/total -0.8975654244422913\n",
      "loss/total 0.03352426365017891\n",
      "loss/total 0.4077175259590149\n",
      "loss/total -0.12115657329559326\n",
      "loss/total 0.14721719920635223\n",
      "loss/total 0.12352591753005981\n",
      "loss/total 0.19039258360862732\n",
      "loss/total 0.005815416574478149\n",
      "loss/total -0.02456994354724884\n",
      "loss/total -0.7241686582565308\n",
      "loss/total -0.06587888300418854\n",
      "loss/total -0.09109846502542496\n",
      "loss/total 0.5794860124588013\n",
      "mini-batch training finished\n",
      "loss/total -0.11116307228803635\n",
      "loss/total -0.17708319425582886\n",
      "loss/total 0.5051733255386353\n",
      "loss/total -0.33465108275413513\n",
      "loss/total 0.38509616255760193\n",
      "loss/total -0.009102575480937958\n",
      "loss/total 0.24463216960430145\n",
      "loss/total -0.4229571223258972\n",
      "loss/total -0.009106583893299103\n",
      "loss/total 0.010956227779388428\n",
      "loss/total -0.3307195007801056\n",
      "loss/total -0.3852226138114929\n",
      "loss/total -0.5190831422805786\n",
      "loss/total 0.024659886956214905\n",
      "loss/total 0.10589589923620224\n",
      "loss/total 0.32299646735191345\n",
      "loss/total -0.1375037133693695\n",
      "loss/total 0.07048757374286652\n",
      "loss/total -0.12279806286096573\n",
      "loss/total -0.03553573042154312\n",
      "loss/total 0.0004356205463409424\n",
      "loss/total -0.02321096509695053\n",
      "loss/total -0.20574982464313507\n",
      "loss/total -0.6795448660850525\n",
      "loss/total 0.07292068004608154\n",
      "loss/total -0.3319716155529022\n",
      "loss/total -0.4401390254497528\n",
      "loss/total -0.34250831604003906\n",
      "loss/total 0.19417071342468262\n",
      "loss/total -0.613791286945343\n",
      "loss/total -0.4370017945766449\n",
      "loss/total 0.7682459950447083\n",
      "mini-batch training finished\n",
      "loss/total -0.36606070399284363\n",
      "loss/total 0.34516602754592896\n",
      "loss/total 0.6084570288658142\n",
      "loss/total -0.006199628114700317\n",
      "loss/total 0.3288719058036804\n",
      "loss/total 0.14511556923389435\n",
      "loss/total 0.1789059191942215\n",
      "loss/total 0.29818132519721985\n",
      "loss/total -0.18760207295417786\n",
      "loss/total -0.06932546198368073\n",
      "loss/total 0.545303225517273\n",
      "loss/total 0.1747797578573227\n",
      "loss/total -0.08522526919841766\n",
      "loss/total -0.41332265734672546\n",
      "loss/total -0.1292971670627594\n",
      "loss/total 0.520418107509613\n",
      "loss/total -0.2995443344116211\n",
      "loss/total 0.29695606231689453\n",
      "loss/total -0.2074771523475647\n",
      "loss/total 0.01844964176416397\n",
      "loss/total 0.18031996488571167\n",
      "loss/total 0.1814197599887848\n",
      "loss/total 0.03694579005241394\n",
      "loss/total -0.06689072400331497\n",
      "loss/total -0.47548437118530273\n",
      "loss/total -0.2678510248661041\n",
      "loss/total -0.4795735478401184\n",
      "loss/total 0.4878670275211334\n",
      "loss/total 0.29559198021888733\n",
      "loss/total 0.19551314413547516\n",
      "loss/total -0.161270409822464\n",
      "loss/total 0.35919713973999023\n",
      "mini-batch training finished\n",
      "loss/total -0.03487321734428406\n",
      "loss/total 0.48340654373168945\n",
      "loss/total -0.43187734484672546\n",
      "loss/total 0.5196132659912109\n",
      "loss/total 0.174667626619339\n",
      "loss/total -0.07268892973661423\n",
      "loss/total 0.5142491459846497\n",
      "loss/total 0.7400795817375183\n",
      "loss/total 0.24527227878570557\n",
      "loss/total 0.07127795368432999\n",
      "loss/total -0.10106826573610306\n",
      "loss/total 0.08574753254652023\n",
      "loss/total -0.26166102290153503\n",
      "loss/total -0.1202205941081047\n",
      "loss/total 0.5235844850540161\n",
      "loss/total 0.8025734424591064\n",
      "loss/total -0.011483922600746155\n",
      "loss/total 0.09397532790899277\n",
      "loss/total 0.23149016499519348\n",
      "loss/total 0.3924849033355713\n",
      "loss/total 0.1492171436548233\n",
      "loss/total 0.06533969938755035\n",
      "loss/total -0.13072046637535095\n",
      "loss/total -0.22860074043273926\n",
      "loss/total -0.1597091406583786\n",
      "loss/total -0.28317955136299133\n",
      "loss/total -0.07027687132358551\n",
      "loss/total 0.4543081820011139\n",
      "loss/total 0.27978578209877014\n",
      "loss/total 0.5901073813438416\n",
      "loss/total 0.16319990158081055\n",
      "loss/total -0.3125673532485962\n",
      "mini-batch training finished\n",
      "loss/total 0.43784642219543457\n",
      "loss/total 0.010061249136924744\n",
      "loss/total 0.019456084817647934\n",
      "loss/total 0.24780835211277008\n",
      "loss/total 0.017302215099334717\n",
      "loss/total -0.3264678418636322\n",
      "loss/total -0.09005768597126007\n",
      "loss/total -0.1390083134174347\n",
      "loss/total -0.693430483341217\n",
      "loss/total 0.7165952324867249\n",
      "loss/total -0.6372298002243042\n",
      "loss/total 0.45159080624580383\n",
      "loss/total 0.046143919229507446\n",
      "loss/total -0.4302577078342438\n",
      "loss/total -0.31423190236091614\n",
      "loss/total -0.10621140897274017\n",
      "loss/total -0.019524872303009033\n",
      "loss/total -0.17496225237846375\n",
      "loss/total -0.16289889812469482\n",
      "loss/total -0.3459046185016632\n",
      "loss/total -0.2614251375198364\n",
      "loss/total -0.02247259020805359\n",
      "loss/total 0.3349139988422394\n",
      "loss/total -0.4362209439277649\n",
      "loss/total 0.14568427205085754\n",
      "loss/total -0.3019220530986786\n",
      "loss/total -0.25836315751075745\n",
      "loss/total -0.19951879978179932\n",
      "loss/total -0.1908116340637207\n",
      "loss/total 0.08731379359960556\n",
      "loss/total -0.6397805213928223\n",
      "loss/total 0.24165767431259155\n",
      "mini-batch training finished\n",
      "loss/total 0.26602423191070557\n",
      "loss/total 0.6702404618263245\n",
      "loss/total 1.2358976602554321\n",
      "loss/total 0.007459919899702072\n",
      "loss/total 0.48380839824676514\n",
      "loss/total 0.09864932298660278\n",
      "loss/total -0.41062062978744507\n",
      "loss/total 0.20396128296852112\n",
      "loss/total 0.6580260992050171\n",
      "loss/total 0.39357155561447144\n",
      "loss/total -0.06851761043071747\n",
      "loss/total 0.5060062408447266\n",
      "loss/total 0.24138110876083374\n",
      "loss/total -0.28972575068473816\n",
      "loss/total 0.46516263484954834\n",
      "loss/total -0.14454516768455505\n",
      "loss/total 0.3609265983104706\n",
      "loss/total 0.09722309559583664\n",
      "loss/total 0.47649359703063965\n",
      "loss/total -0.20887725055217743\n",
      "loss/total 0.1478365808725357\n",
      "loss/total 0.7439699769020081\n",
      "loss/total -0.25184398889541626\n",
      "loss/total 0.15533119440078735\n",
      "loss/total 0.04072646051645279\n",
      "loss/total 0.3121063709259033\n",
      "loss/total 0.16399438679218292\n",
      "loss/total -0.02474071830511093\n",
      "loss/total 0.37523597478866577\n",
      "loss/total 0.22013896703720093\n",
      "loss/total -0.007732100784778595\n",
      "loss/total 0.3639734983444214\n",
      "mini-batch training finished\n",
      "loss/total -0.9814062118530273\n",
      "loss/total 0.1315205693244934\n",
      "loss/total -0.5903488993644714\n",
      "loss/total -0.12630534172058105\n",
      "loss/total -0.22957579791545868\n",
      "loss/total -0.6299160718917847\n",
      "loss/total -0.42878711223602295\n",
      "loss/total -0.062130529433488846\n",
      "loss/total 0.007246922701597214\n",
      "loss/total -0.19636473059654236\n",
      "loss/total -0.3926791846752167\n",
      "loss/total -0.3853186368942261\n",
      "loss/total -0.44354942440986633\n",
      "loss/total -1.3987884521484375\n",
      "loss/total -0.47110551595687866\n",
      "loss/total -0.7376366853713989\n",
      "loss/total -0.8468209505081177\n",
      "loss/total -0.3030449450016022\n",
      "loss/total -0.755403459072113\n",
      "loss/total -0.31081622838974\n",
      "loss/total -0.23959332704544067\n",
      "loss/total -0.9996752142906189\n",
      "loss/total -0.2810857594013214\n",
      "loss/total -0.21585708856582642\n",
      "loss/total -0.801862359046936\n",
      "loss/total -0.03044113516807556\n",
      "loss/total -0.4080904722213745\n",
      "loss/total -0.7038474082946777\n",
      "loss/total -0.41059231758117676\n",
      "loss/total -1.1483993530273438\n",
      "loss/total -0.8717811107635498\n",
      "loss/total -0.27186277508735657\n",
      "mini-batch training finished\n",
      "loss/total -0.09546556323766708\n",
      "loss/total 0.18482935428619385\n",
      "loss/total 0.9719968438148499\n",
      "loss/total 0.2936500012874603\n",
      "loss/total 0.1219884604215622\n",
      "loss/total 0.2251468300819397\n",
      "loss/total 0.7027329802513123\n",
      "loss/total 0.32644569873809814\n",
      "loss/total 0.24390894174575806\n",
      "loss/total -0.18824154138565063\n",
      "loss/total 0.15722326934337616\n",
      "loss/total -0.0010902360081672668\n",
      "loss/total 0.13186249136924744\n",
      "loss/total 0.48688915371894836\n",
      "loss/total 0.17722949385643005\n",
      "loss/total 0.37634557485580444\n",
      "loss/total 0.7288316488265991\n",
      "loss/total 0.3442148268222809\n",
      "loss/total -0.05703633278608322\n",
      "loss/total -0.22545883059501648\n",
      "loss/total 0.09712865948677063\n",
      "loss/total 0.45525527000427246\n",
      "loss/total -0.33852022886276245\n",
      "loss/total -0.0033571571111679077\n",
      "loss/total 0.40462565422058105\n",
      "loss/total 0.2238730490207672\n",
      "loss/total 0.18962275981903076\n",
      "loss/total 0.011052858084440231\n",
      "loss/total 0.2314295768737793\n",
      "loss/total -0.1609918773174286\n",
      "loss/total 0.12096600234508514\n",
      "loss/total 0.10965205729007721\n",
      "mini-batch training finished\n",
      "loss/total 0.37645086646080017\n",
      "loss/total 0.48685064911842346\n",
      "loss/total -0.20697814226150513\n",
      "loss/total 0.7047545313835144\n",
      "loss/total 0.3073883354663849\n",
      "loss/total -0.2896748185157776\n",
      "loss/total -0.006046898663043976\n",
      "loss/total -0.28966566920280457\n",
      "loss/total -0.04052737355232239\n",
      "loss/total 0.560623288154602\n",
      "loss/total 0.4049796760082245\n",
      "loss/total 0.011494498699903488\n",
      "loss/total 0.10268531739711761\n",
      "loss/total 0.014182768762111664\n",
      "loss/total -0.48895028233528137\n",
      "loss/total -0.1875392198562622\n",
      "loss/total 0.20706495642662048\n",
      "loss/total 0.2230972945690155\n",
      "loss/total -0.2660048007965088\n",
      "loss/total -0.24480164051055908\n",
      "loss/total -0.28679072856903076\n",
      "loss/total 0.051062267273664474\n",
      "loss/total 0.3459664583206177\n",
      "loss/total 0.0304059237241745\n",
      "loss/total 0.15778669714927673\n",
      "loss/total 0.1691094934940338\n",
      "loss/total -0.2240525186061859\n",
      "loss/total 0.0706806406378746\n",
      "loss/total -0.39562085270881653\n",
      "loss/total 0.743476390838623\n",
      "loss/total -0.5508726239204407\n",
      "loss/total 0.014634791761636734\n",
      "mini-batch training finished\n",
      "loss/total 0.3005484938621521\n",
      "loss/total 0.009412821382284164\n",
      "loss/total -0.23313245177268982\n",
      "loss/total 0.7033870816230774\n",
      "loss/total 0.8060299754142761\n",
      "loss/total 0.879715085029602\n",
      "loss/total 0.6285949349403381\n",
      "loss/total 0.11993131041526794\n",
      "loss/total 0.3760712742805481\n",
      "loss/total 0.8347894549369812\n",
      "loss/total -0.048756420612335205\n",
      "loss/total 0.24588477611541748\n",
      "loss/total 0.4435221552848816\n",
      "loss/total 0.4870411157608032\n",
      "loss/total -0.19258873164653778\n",
      "loss/total 0.006626628339290619\n",
      "loss/total 0.11524850130081177\n",
      "loss/total 0.31661954522132874\n",
      "loss/total -0.24093016982078552\n",
      "loss/total -0.18030112981796265\n",
      "loss/total 0.4205366373062134\n",
      "loss/total 0.42620110511779785\n",
      "loss/total 0.4685470461845398\n",
      "loss/total 0.6348328590393066\n",
      "loss/total 0.3520335257053375\n",
      "loss/total 0.7402475476264954\n",
      "loss/total -0.29520121216773987\n",
      "loss/total 0.183086097240448\n",
      "loss/total -0.02956954389810562\n",
      "loss/total -0.378536194562912\n",
      "loss/total 0.8481018543243408\n",
      "loss/total 0.14769870042800903\n",
      "mini-batch training finished\n",
      "loss/total 0.7956989407539368\n",
      "loss/total 0.24326305091381073\n",
      "loss/total 0.47275495529174805\n",
      "loss/total -0.27954497933387756\n",
      "loss/total -0.16472706198692322\n",
      "loss/total -0.21893709897994995\n",
      "loss/total -0.21236242353916168\n",
      "loss/total 0.9484953880310059\n",
      "loss/total -0.34386971592903137\n",
      "loss/total 0.09719327092170715\n",
      "loss/total 0.7325578927993774\n",
      "loss/total 0.14816266298294067\n",
      "loss/total 0.14497999846935272\n",
      "loss/total -0.2872438132762909\n",
      "loss/total -0.20724815130233765\n",
      "loss/total 0.13733218610286713\n",
      "loss/total -0.08694343268871307\n",
      "loss/total -0.6330201625823975\n",
      "loss/total 0.5941570997238159\n",
      "loss/total 0.09122174233198166\n",
      "loss/total -0.051356010138988495\n",
      "loss/total 0.09909139573574066\n",
      "loss/total 0.23086756467819214\n",
      "loss/total -0.05019537732005119\n",
      "loss/total -0.11992385983467102\n",
      "loss/total 0.2315160036087036\n",
      "loss/total -0.20657646656036377\n",
      "loss/total 1.010187029838562\n",
      "loss/total -0.03232209384441376\n",
      "loss/total -0.6126160621643066\n",
      "loss/total -0.22233697772026062\n",
      "loss/total -0.18558716773986816\n",
      "mini-batch training finished\n",
      "loss/total 0.07278602570295334\n",
      "loss/total -0.06783618777990341\n",
      "loss/total -0.587709903717041\n",
      "loss/total 0.027006447315216064\n",
      "loss/total -0.4987640976905823\n",
      "loss/total 0.004752777516841888\n",
      "loss/total -0.051894478499889374\n",
      "loss/total -0.3033057451248169\n",
      "loss/total -0.5381962656974792\n",
      "loss/total 0.022374656051397324\n",
      "loss/total -0.006801530718803406\n",
      "loss/total -0.4658884108066559\n",
      "loss/total 0.09383061528205872\n",
      "loss/total -0.418448805809021\n",
      "loss/total -0.3732237219810486\n",
      "loss/total -0.906627893447876\n",
      "loss/total -0.6787462830543518\n",
      "loss/total -0.5891252756118774\n",
      "loss/total -0.07785581797361374\n",
      "loss/total -0.4258774518966675\n",
      "loss/total 0.02919413149356842\n",
      "loss/total -0.36666274070739746\n",
      "loss/total -0.38413533568382263\n",
      "loss/total -0.04944176226854324\n",
      "loss/total -0.5056711435317993\n",
      "loss/total -0.7935469746589661\n",
      "loss/total -0.543807864189148\n",
      "loss/total -0.04777407646179199\n",
      "loss/total -0.1450982391834259\n",
      "loss/total -0.39036592841148376\n",
      "loss/total -0.6103341579437256\n",
      "loss/total 0.059855882078409195\n",
      "mini-batch training finished\n",
      "loss/total 0.14790041744709015\n",
      "loss/total 0.4686117172241211\n",
      "loss/total 0.6353846192359924\n",
      "loss/total 0.03640497103333473\n",
      "loss/total 0.20847128331661224\n",
      "loss/total 0.3318059742450714\n",
      "loss/total -0.3422040343284607\n",
      "loss/total -0.16148382425308228\n",
      "loss/total 0.28157004714012146\n",
      "loss/total 0.026044189929962158\n",
      "loss/total -0.09717628359794617\n",
      "loss/total 0.36045041680336\n",
      "loss/total 0.13171197474002838\n",
      "loss/total -0.13711681962013245\n",
      "loss/total -0.010154269635677338\n",
      "loss/total -0.0903729647397995\n",
      "loss/total -0.39208027720451355\n",
      "loss/total 0.5067811608314514\n",
      "loss/total 0.10712587088346481\n",
      "loss/total 0.2636118233203888\n",
      "loss/total 0.45368820428848267\n",
      "loss/total -0.0\n",
      "loss/total -0.3828986883163452\n",
      "loss/total 0.03653871640563011\n",
      "loss/total 0.0\n",
      "loss/total 0.04644409939646721\n",
      "loss/total -0.15060757100582123\n",
      "loss/total 0.5876484513282776\n",
      "loss/total -0.5892147421836853\n",
      "loss/total -0.1797613501548767\n",
      "loss/total 0.022757258266210556\n",
      "loss/total 0.00198962539434433\n",
      "mini-batch training finished\n",
      "loss/total 0.4532971680164337\n",
      "loss/total 0.6521191596984863\n",
      "loss/total -0.21329621970653534\n",
      "loss/total 0.22905796766281128\n",
      "loss/total -0.15059950947761536\n",
      "loss/total 0.36689630150794983\n",
      "loss/total 0.24633964896202087\n",
      "loss/total -0.2141145020723343\n",
      "loss/total -0.4530595541000366\n",
      "loss/total -0.4736483693122864\n",
      "loss/total -0.07162710279226303\n",
      "loss/total -0.14572584629058838\n",
      "loss/total -0.12262334674596786\n",
      "loss/total 0.25755828619003296\n",
      "loss/total 0.6841639876365662\n",
      "loss/total 0.3824506402015686\n",
      "loss/total -0.53547602891922\n",
      "loss/total 0.08530572056770325\n",
      "loss/total 0.12475341558456421\n",
      "loss/total 0.3179028034210205\n",
      "loss/total -0.2789677679538727\n",
      "loss/total 0.17240959405899048\n",
      "loss/total -0.5755991339683533\n",
      "loss/total 0.6525354385375977\n",
      "loss/total 0.16525736451148987\n",
      "loss/total -0.6096620559692383\n",
      "loss/total -0.1836627721786499\n",
      "loss/total -0.5120581984519958\n",
      "loss/total 0.4086317718029022\n",
      "loss/total 0.6786512732505798\n",
      "loss/total 0.5438465476036072\n",
      "loss/total -0.5917268395423889\n",
      "mini-batch training finished\n",
      "loss/total 0.672610342502594\n",
      "loss/total -0.09209681302309036\n",
      "loss/total -0.2783477008342743\n",
      "loss/total -0.3391979932785034\n",
      "loss/total -0.013727858662605286\n",
      "loss/total 0.13761667907238007\n",
      "loss/total -0.29587554931640625\n",
      "loss/total -0.48111194372177124\n",
      "loss/total 0.043095674365758896\n",
      "loss/total 0.3732585310935974\n",
      "loss/total 0.3864119052886963\n",
      "loss/total -0.6639804840087891\n",
      "loss/total -0.7204466462135315\n",
      "loss/total -0.37052038311958313\n",
      "loss/total -0.34890657663345337\n",
      "loss/total -0.622142493724823\n",
      "loss/total -0.35269302129745483\n",
      "loss/total 0.010845106095075607\n",
      "loss/total -0.559211015701294\n",
      "loss/total -0.5532943606376648\n",
      "loss/total -0.09631891548633575\n",
      "loss/total -0.7696918845176697\n",
      "loss/total 0.3612669110298157\n",
      "loss/total 0.0029056817293167114\n",
      "loss/total -0.164116770029068\n",
      "loss/total -0.4289553165435791\n",
      "loss/total 0.5115673542022705\n",
      "loss/total -0.22719861567020416\n",
      "loss/total -0.7713921070098877\n",
      "loss/total -0.40090978145599365\n",
      "loss/total -0.28641635179519653\n",
      "loss/total -0.12409741431474686\n",
      "mini-batch training finished\n",
      "loss/total -0.07238996028900146\n",
      "loss/total -0.19273704290390015\n",
      "loss/total 0.23942549526691437\n",
      "loss/total 0.0896109864115715\n",
      "loss/total 0.2691827118396759\n",
      "loss/total 0.21792438626289368\n",
      "loss/total -0.13296930491924286\n",
      "loss/total -0.05666929483413696\n",
      "loss/total -0.7080031633377075\n",
      "loss/total 0.03258485347032547\n",
      "loss/total -0.1987660527229309\n",
      "loss/total 0.12897028028964996\n",
      "loss/total -0.05285853520035744\n",
      "loss/total -0.4995719790458679\n",
      "loss/total -0.11504863947629929\n",
      "loss/total 0.6749046444892883\n",
      "loss/total 0.13257171213626862\n",
      "loss/total 0.19649189710617065\n",
      "loss/total -0.5180250406265259\n",
      "loss/total 0.11666814982891083\n",
      "loss/total -0.3603280186653137\n",
      "loss/total -0.32553747296333313\n",
      "loss/total 0.4006314277648926\n",
      "loss/total -0.49347078800201416\n",
      "loss/total -0.1937316656112671\n",
      "loss/total -0.2356923520565033\n",
      "loss/total 0.4164915084838867\n",
      "loss/total -0.24045684933662415\n",
      "loss/total -0.2969186305999756\n",
      "loss/total -0.23785048723220825\n",
      "loss/total 0.20302227139472961\n",
      "loss/total -0.48678460717201233\n",
      "mini-batch training finished\n",
      "loss/total 0.00955285131931305\n",
      "loss/total 0.6777119040489197\n",
      "loss/total -0.26772409677505493\n",
      "loss/total 0.6256242394447327\n",
      "loss/total 0.06896034628152847\n",
      "loss/total 0.07175596058368683\n",
      "loss/total 0.037630025297403336\n",
      "loss/total 0.07367029041051865\n",
      "loss/total -0.060196489095687866\n",
      "loss/total -0.6004396677017212\n",
      "loss/total 0.03151915222406387\n",
      "loss/total 0.3508257567882538\n",
      "loss/total 0.021896041929721832\n",
      "loss/total 0.12665049731731415\n",
      "loss/total 0.9934563636779785\n",
      "loss/total -0.04882226884365082\n",
      "loss/total 0.7000255584716797\n",
      "loss/total -0.14804965257644653\n",
      "loss/total -0.23404468595981598\n",
      "loss/total 0.15019093453884125\n",
      "loss/total 0.3427814841270447\n",
      "loss/total 0.03430533409118652\n",
      "loss/total 0.010091360658407211\n",
      "loss/total -0.34105196595191956\n",
      "loss/total 0.31707772612571716\n",
      "loss/total 0.4252776801586151\n",
      "loss/total -0.2590409517288208\n",
      "loss/total -0.16622433066368103\n",
      "loss/total 0.09062263369560242\n",
      "loss/total -0.09127362817525864\n",
      "loss/total 0.2391711175441742\n",
      "loss/total -0.4027908146381378\n",
      "mini-batch training finished\n",
      "loss/total 0.9885727167129517\n",
      "loss/total 0.37500712275505066\n",
      "loss/total -0.2824253439903259\n",
      "loss/total -0.07605835795402527\n",
      "loss/total -0.16305895149707794\n",
      "loss/total 0.629950225353241\n",
      "loss/total -0.17502182722091675\n",
      "loss/total 0.3537498116493225\n",
      "loss/total -0.09629102051258087\n",
      "loss/total 0.1839396357536316\n",
      "loss/total 0.619342565536499\n",
      "loss/total 0.1467299461364746\n",
      "loss/total -0.2561050355434418\n",
      "loss/total -0.0037940964102745056\n",
      "loss/total -0.3289065361022949\n",
      "loss/total 0.474639356136322\n",
      "loss/total 0.33406752347946167\n",
      "loss/total -0.08910366892814636\n",
      "loss/total -0.24946385622024536\n",
      "loss/total -0.41462406516075134\n",
      "loss/total 0.37769171595573425\n",
      "loss/total 0.8272610902786255\n",
      "loss/total -0.22796253859996796\n",
      "loss/total 0.04244252294301987\n",
      "loss/total -0.05559304356575012\n",
      "loss/total 0.32988715171813965\n",
      "loss/total 0.21576109528541565\n",
      "loss/total 0.4738655090332031\n",
      "loss/total -0.5331542491912842\n",
      "loss/total 0.20739829540252686\n",
      "loss/total -0.3244877755641937\n",
      "loss/total 0.1937556117773056\n",
      "mini-batch training finished\n",
      "loss/total -0.14806941151618958\n",
      "loss/total -0.313532292842865\n",
      "loss/total 0.29306742548942566\n",
      "loss/total 0.054916128516197205\n",
      "loss/total -0.1662975400686264\n",
      "loss/total 0.3690786063671112\n",
      "loss/total -0.17851053178310394\n",
      "loss/total 0.2381916046142578\n",
      "loss/total -0.2675358057022095\n",
      "loss/total -0.08831636607646942\n",
      "loss/total -0.168579563498497\n",
      "loss/total -0.2088499814271927\n",
      "loss/total 0.03249295428395271\n",
      "loss/total -0.23182636499404907\n",
      "loss/total -0.31308794021606445\n",
      "loss/total 0.43304628133773804\n",
      "loss/total -0.5264312624931335\n",
      "loss/total 0.5940780639648438\n",
      "loss/total 0.173814594745636\n",
      "loss/total 0.09191446751356125\n",
      "loss/total -0.22395773231983185\n",
      "loss/total -0.20165106654167175\n",
      "loss/total -0.30012157559394836\n",
      "loss/total -0.44795823097229004\n",
      "loss/total -0.0710049495100975\n",
      "loss/total -0.3273760676383972\n",
      "loss/total 0.4058520793914795\n",
      "loss/total -0.7462592720985413\n",
      "loss/total -0.1366681009531021\n",
      "loss/total -0.819474995136261\n",
      "loss/total -0.02053695172071457\n",
      "loss/total 0.30601125955581665\n",
      "mini-batch training finished\n",
      "loss/total -0.42960307002067566\n",
      "loss/total -0.040121786296367645\n",
      "loss/total 0.031112082302570343\n",
      "loss/total 0.1250784546136856\n",
      "loss/total 0.18023838102817535\n",
      "loss/total -0.014591872692108154\n",
      "loss/total 0.6136184334754944\n",
      "loss/total 0.7220199704170227\n",
      "loss/total 0.22324204444885254\n",
      "loss/total 0.2784285843372345\n",
      "loss/total -0.4863188564777374\n",
      "loss/total -0.002779826521873474\n",
      "loss/total 0.2105681449174881\n",
      "loss/total -0.05349355190992355\n",
      "loss/total 0.07050886005163193\n",
      "loss/total 0.15037427842617035\n",
      "loss/total -0.015607878565788269\n",
      "loss/total 0.5014474391937256\n",
      "loss/total 0.1704479604959488\n",
      "loss/total 0.3065451383590698\n",
      "loss/total -0.16045644879341125\n",
      "loss/total -0.36921417713165283\n",
      "loss/total -0.24080216884613037\n",
      "loss/total -0.2364863157272339\n",
      "loss/total 0.08742718398571014\n",
      "loss/total -0.24622586369514465\n",
      "loss/total 0.5786591172218323\n",
      "loss/total -0.07862316071987152\n",
      "loss/total 0.34639936685562134\n",
      "loss/total -0.026832126080989838\n",
      "loss/total -0.20023009181022644\n",
      "loss/total -0.6056294441223145\n",
      "mini-batch training finished\n",
      "loss/total 0.43183228373527527\n",
      "loss/total 0.5782403349876404\n",
      "loss/total 0.17950743436813354\n",
      "loss/total -0.10239686816930771\n",
      "loss/total 0.4695169925689697\n",
      "loss/total 0.9980717301368713\n",
      "loss/total -0.1072155088186264\n",
      "loss/total 0.38194137811660767\n",
      "loss/total -0.18199199438095093\n",
      "loss/total 0.5272667407989502\n",
      "loss/total 0.5640648007392883\n",
      "loss/total -0.2276601493358612\n",
      "loss/total 0.3406140208244324\n",
      "loss/total 0.05689329653978348\n",
      "loss/total 0.39794498682022095\n",
      "loss/total 0.6793751120567322\n",
      "loss/total 0.49459409713745117\n",
      "loss/total 0.37322747707366943\n",
      "loss/total -0.2875758707523346\n",
      "loss/total 0.47596198320388794\n",
      "loss/total 0.10967095196247101\n",
      "loss/total 0.42223048210144043\n",
      "loss/total -0.240200012922287\n",
      "loss/total 0.9152432084083557\n",
      "loss/total 0.6616221070289612\n",
      "loss/total 0.4877520799636841\n",
      "loss/total 0.3652532398700714\n",
      "loss/total 0.07964248210191727\n",
      "loss/total -0.016563162207603455\n",
      "loss/total -0.0816526710987091\n",
      "loss/total 0.3580195903778076\n",
      "loss/total -0.07737746089696884\n",
      "mini-batch training finished\n",
      "loss/total -0.16792000830173492\n",
      "loss/total -0.5038141012191772\n",
      "loss/total -0.407416969537735\n",
      "loss/total 0.4827313721179962\n",
      "loss/total -0.26347142457962036\n",
      "loss/total 0.3136383593082428\n",
      "loss/total -0.0753912404179573\n",
      "loss/total -0.551521360874176\n",
      "loss/total 0.09169378876686096\n",
      "loss/total 0.09370145201683044\n",
      "loss/total 0.15653343498706818\n",
      "loss/total 0.04021541029214859\n",
      "loss/total -0.46948331594467163\n",
      "loss/total -1.0830235481262207\n",
      "loss/total -0.5664530992507935\n",
      "loss/total -0.6079476475715637\n",
      "loss/total 0.026215851306915283\n",
      "loss/total -0.45454588532447815\n",
      "loss/total -0.2805774509906769\n",
      "loss/total 0.007402241230010986\n",
      "loss/total -0.21065804362297058\n",
      "loss/total -0.4201902449131012\n",
      "loss/total -0.8144271969795227\n",
      "loss/total -0.6965817213058472\n",
      "loss/total -0.702582061290741\n",
      "loss/total -0.18307963013648987\n",
      "loss/total 0.10751810669898987\n",
      "loss/total -0.5507031679153442\n",
      "loss/total -0.9500665068626404\n",
      "loss/total 0.3723515272140503\n",
      "loss/total -0.6829660534858704\n",
      "loss/total -0.3360253572463989\n",
      "mini-batch training finished\n",
      "loss/total -0.034466251730918884\n",
      "loss/total -0.4697597622871399\n",
      "loss/total 0.7157642245292664\n",
      "loss/total 0.253822922706604\n",
      "loss/total 0.0351320244371891\n",
      "loss/total 0.3221295475959778\n",
      "loss/total 0.6405411958694458\n",
      "loss/total 0.32304656505584717\n",
      "loss/total 0.4282267093658447\n",
      "loss/total 0.20943191647529602\n",
      "loss/total 0.5477036237716675\n",
      "loss/total 0.48053011298179626\n",
      "loss/total -0.29701462388038635\n",
      "loss/total 0.002324320375919342\n",
      "loss/total -0.547251284122467\n",
      "loss/total -0.20110860466957092\n",
      "loss/total -0.3070705533027649\n",
      "loss/total 0.2825615108013153\n",
      "loss/total 0.003243234008550644\n",
      "loss/total 0.451144814491272\n",
      "loss/total -0.18317270278930664\n",
      "loss/total 0.26996833086013794\n",
      "loss/total -0.1175854504108429\n",
      "loss/total 0.18625512719154358\n",
      "loss/total -0.26373031735420227\n",
      "loss/total 0.5042469501495361\n",
      "loss/total -0.9126483798027039\n",
      "loss/total -0.14990536868572235\n",
      "loss/total 0.7772166132926941\n",
      "loss/total 0.5071372389793396\n",
      "loss/total -0.07849398255348206\n",
      "loss/total -0.09464466571807861\n",
      "mini-batch training finished\n",
      "loss/total -0.15696698427200317\n",
      "loss/total -0.054916415363550186\n",
      "loss/total 0.13667748868465424\n",
      "loss/total -0.3113667368888855\n",
      "loss/total 0.522106409072876\n",
      "loss/total 0.08271080255508423\n",
      "loss/total 1.0827590227127075\n",
      "loss/total 0.420472115278244\n",
      "loss/total 0.2059570997953415\n",
      "loss/total -0.06101401522755623\n",
      "loss/total -0.07598765194416046\n",
      "loss/total 0.2539680600166321\n",
      "loss/total -0.31523823738098145\n",
      "loss/total -0.04742060601711273\n",
      "loss/total 0.03432832285761833\n",
      "loss/total 0.5730446577072144\n",
      "loss/total 0.060521528124809265\n",
      "loss/total 0.5439196228981018\n",
      "loss/total -0.009332884103059769\n",
      "loss/total -0.5226119160652161\n",
      "loss/total 0.31186026334762573\n",
      "loss/total -0.665759801864624\n",
      "loss/total 0.22913216054439545\n",
      "loss/total 0.1893472671508789\n",
      "loss/total -0.6354774236679077\n",
      "loss/total -0.2462605983018875\n",
      "loss/total -0.264029860496521\n",
      "loss/total -0.1460716277360916\n",
      "loss/total 0.0638391375541687\n",
      "loss/total 0.48281165957450867\n",
      "loss/total -0.008611373603343964\n",
      "loss/total 0.9987829923629761\n",
      "mini-batch training finished\n",
      "loss/total 0.1511891782283783\n",
      "loss/total -0.2769568860530853\n",
      "loss/total -0.2795138955116272\n",
      "loss/total -0.43870481848716736\n",
      "loss/total -0.08737839758396149\n",
      "loss/total -0.03022831678390503\n",
      "loss/total -0.11594676971435547\n",
      "loss/total 0.3429422974586487\n",
      "loss/total -0.1634370982646942\n",
      "loss/total 0.012390665709972382\n",
      "loss/total -0.28402596712112427\n",
      "loss/total -0.5727962851524353\n",
      "loss/total 0.037435296922922134\n",
      "loss/total -0.4475613534450531\n",
      "loss/total 0.12764900922775269\n",
      "loss/total -0.5830134153366089\n",
      "loss/total 0.30903059244155884\n",
      "loss/total -0.6467348337173462\n",
      "loss/total -0.646726131439209\n",
      "loss/total 0.04468608647584915\n",
      "loss/total -0.6741788387298584\n",
      "loss/total 0.2790072560310364\n",
      "loss/total -0.7979260087013245\n",
      "loss/total -0.38525256514549255\n",
      "loss/total -1.0333118438720703\n",
      "loss/total -0.35243988037109375\n",
      "loss/total 0.26647284626960754\n",
      "loss/total -0.5797964930534363\n",
      "loss/total 0.5270423293113708\n",
      "loss/total -0.9686460494995117\n",
      "loss/total -0.19602903723716736\n",
      "loss/total -0.1405736804008484\n",
      "mini-batch training finished\n",
      "loss/total 0.2582387924194336\n",
      "loss/total 0.08392442017793655\n",
      "loss/total 0.10765929520130157\n",
      "loss/total 0.6703507304191589\n",
      "loss/total 0.11912408471107483\n",
      "loss/total -0.06482226401567459\n",
      "loss/total -0.13537141680717468\n",
      "loss/total 0.20562666654586792\n",
      "loss/total 0.8589147329330444\n",
      "loss/total -0.08556954562664032\n",
      "loss/total -0.22207137942314148\n",
      "loss/total 0.22075968980789185\n",
      "loss/total 0.045018378645181656\n",
      "loss/total 0.07256276905536652\n",
      "loss/total -0.04694318026304245\n",
      "loss/total -0.14924004673957825\n",
      "loss/total 0.04569239914417267\n",
      "loss/total 0.23441475629806519\n",
      "loss/total 0.10463456809520721\n",
      "loss/total -0.376060426235199\n",
      "loss/total 0.11457300186157227\n",
      "loss/total -0.17249652743339539\n",
      "loss/total -0.3280351459980011\n",
      "loss/total 0.3823462128639221\n",
      "loss/total -0.13994430005550385\n",
      "loss/total 0.3150775730609894\n",
      "loss/total -0.19648206233978271\n",
      "loss/total -0.036620840430259705\n",
      "loss/total 0.17669238150119781\n",
      "loss/total -0.02633010409772396\n",
      "loss/total -0.17005449533462524\n",
      "loss/total 0.3026196360588074\n",
      "mini-batch training finished\n",
      "loss/total 0.24547991156578064\n",
      "loss/total 0.09856061637401581\n",
      "loss/total -0.33635735511779785\n",
      "loss/total 0.008538160473108292\n",
      "loss/total 0.10239604115486145\n",
      "loss/total 0.9858309626579285\n",
      "loss/total -0.19977512955665588\n",
      "loss/total -0.47633716464042664\n",
      "loss/total 0.6719599962234497\n",
      "loss/total -0.05800313502550125\n",
      "loss/total -0.1773945838212967\n",
      "loss/total -0.3914717435836792\n",
      "loss/total 0.35293227434158325\n",
      "loss/total -0.36330389976501465\n",
      "loss/total 0.27417367696762085\n",
      "loss/total -0.4020281136035919\n",
      "loss/total -0.3255844712257385\n",
      "loss/total 0.4343341588973999\n",
      "loss/total 0.14793412387371063\n",
      "loss/total -0.012430641800165176\n",
      "loss/total -0.20061469078063965\n",
      "loss/total -0.49455636739730835\n",
      "loss/total 0.35011956095695496\n",
      "loss/total -0.16910526156425476\n",
      "loss/total 0.020897064357995987\n",
      "loss/total -0.3107990026473999\n",
      "loss/total -0.22385534644126892\n",
      "loss/total -0.18729965388774872\n",
      "loss/total 0.6185934543609619\n",
      "loss/total -0.032009877264499664\n",
      "loss/total 0.07843358814716339\n",
      "loss/total -0.6622617840766907\n",
      "mini-batch training finished\n",
      "loss/total 0.21736904978752136\n",
      "loss/total 0.18980242311954498\n",
      "loss/total 0.04782306030392647\n",
      "loss/total 0.3967708945274353\n",
      "loss/total 0.041430264711380005\n",
      "loss/total 0.21301966905593872\n",
      "loss/total -0.4244134724140167\n",
      "loss/total 0.7304511666297913\n",
      "loss/total 0.46524205803871155\n",
      "loss/total -0.03116343915462494\n",
      "loss/total 0.14822345972061157\n",
      "loss/total -0.026462987065315247\n",
      "loss/total 0.4055047631263733\n",
      "loss/total -0.2392585277557373\n",
      "loss/total -0.2069205790758133\n",
      "loss/total -0.24106839299201965\n",
      "loss/total -0.4844930171966553\n",
      "loss/total 0.6035815477371216\n",
      "loss/total 0.3287270665168762\n",
      "loss/total -0.7719825506210327\n",
      "loss/total 0.38181114196777344\n",
      "loss/total -0.06560508906841278\n",
      "loss/total 0.09515824168920517\n",
      "loss/total -0.01677282154560089\n",
      "loss/total 0.12788590788841248\n",
      "loss/total 0.35908037424087524\n",
      "loss/total 0.05721082538366318\n",
      "loss/total 0.5786798596382141\n",
      "loss/total -0.111032634973526\n",
      "loss/total -0.2132313996553421\n",
      "loss/total -0.7180536389350891\n",
      "loss/total -0.012626081705093384\n",
      "mini-batch training finished\n",
      "loss/total -0.30904000997543335\n",
      "loss/total -0.1238761618733406\n",
      "loss/total -0.5915044546127319\n",
      "loss/total 0.45020484924316406\n",
      "loss/total 0.5729777216911316\n",
      "loss/total 0.29941368103027344\n",
      "loss/total 0.01868480257689953\n",
      "loss/total 0.6198712587356567\n",
      "loss/total 0.4354184865951538\n",
      "loss/total -0.18713468313217163\n",
      "loss/total 0.13550378382205963\n",
      "loss/total -0.16126355528831482\n",
      "loss/total 0.1448662281036377\n",
      "loss/total -0.6390485763549805\n",
      "loss/total -0.03247997909784317\n",
      "loss/total -0.054980773478746414\n",
      "loss/total -0.00918889045715332\n",
      "loss/total -0.9263725280761719\n",
      "loss/total 0.32730990648269653\n",
      "loss/total -0.2667903006076813\n",
      "loss/total 0.3776944875717163\n",
      "loss/total -0.2872309982776642\n",
      "loss/total 0.7787595987319946\n",
      "loss/total -0.6547564268112183\n",
      "loss/total 0.38537171483039856\n",
      "loss/total 0.5048964619636536\n",
      "loss/total -0.3671874701976776\n",
      "loss/total -0.34245407581329346\n",
      "loss/total -0.008881747722625732\n",
      "loss/total -0.1164853423833847\n",
      "loss/total -0.6885025501251221\n",
      "loss/total -0.023120447993278503\n",
      "mini-batch training finished\n",
      "loss/total 1.2393577098846436\n",
      "loss/total 0.534917950630188\n",
      "loss/total -0.13655824959278107\n",
      "loss/total -0.027026206254959106\n",
      "loss/total 0.3112649619579315\n",
      "loss/total -0.2664104104042053\n",
      "loss/total 0.30350926518440247\n",
      "loss/total 0.5683308243751526\n",
      "loss/total 0.7487188577651978\n",
      "loss/total -0.06938551366329193\n",
      "loss/total 0.6722759008407593\n",
      "loss/total 0.4454495906829834\n",
      "loss/total 0.07807178795337677\n",
      "loss/total -0.21787749230861664\n",
      "loss/total -0.37220510840415955\n",
      "loss/total 0.20166221261024475\n",
      "loss/total 1.054956078529358\n",
      "loss/total -0.09542515873908997\n",
      "loss/total -0.5068000555038452\n",
      "loss/total -0.1172613650560379\n",
      "loss/total -0.2348974049091339\n",
      "loss/total 0.08488035947084427\n",
      "loss/total -0.047096192836761475\n",
      "loss/total 0.476805716753006\n",
      "loss/total 0.23539365828037262\n",
      "loss/total -0.12847301363945007\n",
      "loss/total 0.10223060846328735\n",
      "loss/total -0.27612894773483276\n",
      "loss/total -0.03451497107744217\n",
      "loss/total 0.8334332704544067\n",
      "loss/total 1.0457934141159058\n",
      "loss/total -0.529730498790741\n",
      "mini-batch training finished\n",
      "loss/total 0.028606165200471878\n",
      "loss/total 0.0746460109949112\n",
      "loss/total -0.12230709940195084\n",
      "loss/total 0.2524191737174988\n",
      "loss/total 0.2369687706232071\n",
      "loss/total 0.36410215497016907\n",
      "loss/total -0.11248693615198135\n",
      "loss/total -0.08216127008199692\n",
      "loss/total 0.05621570348739624\n",
      "loss/total -0.1381927877664566\n",
      "loss/total -0.04196156561374664\n",
      "loss/total -0.27751895785331726\n",
      "loss/total -0.16081392765045166\n",
      "loss/total 0.10702560096979141\n",
      "loss/total 0.3125067353248596\n",
      "loss/total -0.0393032506108284\n",
      "loss/total -0.12942136824131012\n",
      "loss/total -0.14390811324119568\n",
      "loss/total -0.37578916549682617\n",
      "loss/total 0.2766011357307434\n",
      "loss/total -0.22478505969047546\n",
      "loss/total 0.3379557132720947\n",
      "loss/total 0.011035695672035217\n",
      "loss/total -0.36416319012641907\n",
      "loss/total -0.16763657331466675\n",
      "loss/total 0.041323523968458176\n",
      "loss/total -0.013888362795114517\n",
      "loss/total -0.3213440179824829\n",
      "loss/total 0.12197483330965042\n",
      "loss/total 0.39595746994018555\n",
      "loss/total -0.3936023712158203\n",
      "loss/total -0.4424799382686615\n",
      "mini-batch training finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 60\u001B[0m\n\u001B[1;32m     51\u001B[0m input_data \u001B[38;5;241m=\u001B[39m data_collator([\n\u001B[1;32m     52\u001B[0m     {\n\u001B[1;32m     53\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m: ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ids \u001B[38;5;129;01min\u001B[39;00m query_response_tensors\n\u001B[1;32m     57\u001B[0m ])\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# 奖励和优势\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m logprobs, rewards, values, masks \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_rewards\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscore_tensors\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m advantages, returns \u001B[38;5;241m=\u001B[39m compute_advantage(rewards, values, masks)\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# 小批次训练\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[25], line 10\u001B[0m, in \u001B[0;36mcompute_rewards\u001B[0;34m(input_data, query_tensors, response_tensors, score_tensors)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcompute_rewards\u001B[39m(\n\u001B[1;32m      2\u001B[0m     input_data,\n\u001B[1;32m      3\u001B[0m     query_tensors,\n\u001B[1;32m      4\u001B[0m     response_tensors,\n\u001B[1;32m      5\u001B[0m     score_tensors\n\u001B[1;32m      6\u001B[0m ):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;66;03m# 正在微调的模型所输出的token的logits和token的价值\u001B[39;00m\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;66;03m# 模型输出所有token的概率分布\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m         logits, values \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_data\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# b, seq, vocab\u001B[39;00m\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;66;03m# 冻结的模型的输出和价值\u001B[39;00m\n\u001B[1;32m     12\u001B[0m         ref_logits, _ \u001B[38;5;241m=\u001B[39m sft_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minput_data)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[4], line 20\u001B[0m, in \u001B[0;36mModelForCausalLMWithValueHead.forward\u001B[0;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     16\u001B[0m     input_ids,\n\u001B[1;32m     17\u001B[0m     attention_mask,\n\u001B[1;32m     18\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[torch\u001B[38;5;241m.\u001B[39mFloatTensor]:\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;66;03m# gpt2-sft模型的输出\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m     transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m# 输出的token的概率分布，维度为 `vocab_size`\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     lm_logits \u001B[38;5;241m=\u001B[39m transformer_outputs\u001B[38;5;241m.\u001B[39mlogits\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1094\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m   1091\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   1093\u001B[0m slice_indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(\u001B[38;5;241m-\u001B[39mlogits_to_keep, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(logits_to_keep, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m logits_to_keep\n\u001B[0;32m-> 1094\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlm_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslice_indices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1096\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;66;03m# Flatten the tokens\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rlhf/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
